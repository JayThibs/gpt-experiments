{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-2-alignment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2Lq-qHwDRba4",
        "dWooQtxRtJar",
        "IWi2WcqKH_cF",
        "60UIFTAY1N2l",
        "C9ZvyvZyerDT",
        "ZXvm0wpQxS10"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/gpt-experiments/blob/main/notebook/gpt_2_alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lq-qHwDRba4"
      },
      "source": [
        "# Fine-Tuning GPT-2 on Alignment Texts Dataset\n",
        "\n",
        "This notebook is meant for initial experimentation of fine-tuning on the alignment text dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjSP3oGNHyJd",
        "outputId": "89e1da56-4426-4bf5-b16f-fb815b4d2545"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jul  4 17:50:34 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap-rQ1P1Y058"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr14u8fcYR4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec59177b-a8ef-4d2b-9524-230d700cb907"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers pytorch-lightning beautifulsoup4 datasets jsonlines ftfy lm_dataformat wandb --quiet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 585 kB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 362 kB 80.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 71.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 83.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 91.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 85.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 419 kB 85.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 70.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 92.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 93.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 23.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 145 kB 89.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 87.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 86.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 86.7 MB/s \n",
            "\u001b[?25h  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84TRkXXhY2U5"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvxQKSqQY3Fa"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import seed_everything\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2Tokenizer, GPT2TokenizerFast, AutoTokenizer, TrainingArguments, Trainer, GPT2LMHeadModel\n",
        "import ftfy\n",
        "from lm_dataformat import Reader\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5tY6KPsgUFQ"
      },
      "source": [
        "# Mounting Google Drive\n",
        "\n",
        "Here we will mount our Google Drive so that we can grab data and save the HuggingFace scripts, and save the model once we've fine-tuned it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNeXrm1qgWRp",
        "outputId": "1d7d7e6c-c53b-4a59-a108-51326529e930"
      },
      "source": [
        "# For saving the data locally\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIMELLTfg76e",
        "outputId": "9bdcb15c-f408-4a32-b8c5-05729d3ee4f0"
      },
      "source": [
        "%cd drive/MyDrive/data/ai-alignment-dataset/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data/ai-alignment-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/JayThibs/gpt-experiments"
      ],
      "metadata": {
        "id": "UK0_tMPMwPOg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-AVhqkVeluk"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Sub-Datasets"
      ],
      "metadata": {
        "id": "dWooQtxRtJar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open(\"alignment_forum.jsonl\", \"w\") as writer:\n",
        "    with jsonlines.open(\"alignment_texts.jsonl\") as reader:\n",
        "        for line in reader:\n",
        "            try:\n",
        "                if line[\"source\"] == \"alignment forum\":\n",
        "                    writer.write(line)\n",
        "            except:\n",
        "                pass"
      ],
      "metadata": {
        "id": "l0bhsX0LtPkb"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clearning and Chunking Functions\n",
        "\n",
        "Functions for preparing the data into chunks that can fit into GPT."
      ],
      "metadata": {
        "id": "IWi2WcqKH_cF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python create_finetune_csv.py \"alignment_texts.jsonl\" \"gpt\" --normalize-with-ftfy --min-unique-tokens=10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2oCbGImXtEr",
        "outputId": "02feff54-f994-4833-8d3c-cbe85a792c89"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 0.99M/0.99M [00:00<00:00, 1.15MB/s]\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 506kB/s]\n",
            "Downloading: 100% 1.29M/1.29M [00:01<00:00, 1.24MB/s]\n",
            "Downloading: 100% 665/665 [00:00<00:00, 593kB/s]\n",
            "reading/tokenizing files: 100% 2138/2138 [00:36<00:00, 58.09it/s]\n",
            "enforce_min_unique_tokens: 100% 7288/7288 [00:00<00:00, 28910.39it/s]\n",
            "7288\n",
            "1000\n",
            "dropped 920 tokens of trailing data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "eFp7LJUKJ4Uh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import csv\n",
        "\n",
        "# i = 0\n",
        "# texts = []\n",
        "# with jsonlines.open(\"alignment_texts.jsonl\") as reader:\n",
        "#     for line in reader:\n",
        "#         text = line[\"text\"]\n",
        "#         texts.append(text)\n",
        "#         if i > 3:\n",
        "#             break\n",
        "#         # try:\n",
        "#         if text != \"\":\n",
        "#             print(text)\n",
        "#             print(len(text.split()))\n",
        "#             encoding = tokenizer(text)\n",
        "#             total_len = len(encoding.tokens())\n",
        "#             tokens = encoding.tokens()\n",
        "#             # print(tokens)\n",
        "#             print(tokenizer.decode(encoding.input_ids))\n",
        "#         # if total_len > 1024:\n",
        "#         #     break\n",
        "#         i += 1\n",
        "#         # except:\n",
        "#         #     pass"
      ],
      "metadata": {
        "id": "XYyFaGPBJTIm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60UIFTAY1N2l"
      },
      "source": [
        "## Training Splits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alignment_texts = pd.read_csv(\"alignment_texts_7288.csv\")"
      ],
      "metadata": {
        "id": "KAKD-owCehsb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alignment_texts = list(alignment_texts)\n",
        "alignment_texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "PGRhi7-Ee0YE",
        "outputId": "7c38e00a-31bf-4682-8101-843a071526e9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|endoftext|> I\\'ll be running an Ask Me Anything on this post from Friday (April 30) to Saturday (May 1).\\nIf you want to ask something just post a top-level comment; I\\'ll spend at least a day answering questions.\\nYou can find some background about me here.\\n<|endoftext|>**I—Meanings**\\nNow that we have some more concrete thinking under our belt, it\\'s time to circle back on Goodhart\\'s law for value learners. What sorts of bad behavior are we imagining from future value-learning AI? What makes those behaviors plausible, and what makes them bad?\\nLet\\'s start with that last point first. Judgments of goodness or badness get contextualized by models, so our framing of Goodhart\\'s law depends on what models of humans we tolerate. When I say \"I like dancing,\" this is a different use of the word \\'like,\\' backed by a different model of myself, than when I say \"I like tasting sugar.\" The model that comes to mind for dancing treats it as one of the chunks of my day, like \"playing computer games\" or \"taking the bus.\" I can know what state I\\'m in (the inference function of the model) based on seeing and hearing short scenes. Meanwhile, my model that has the taste of sugar in it has states like \"feeling sandpaper\" or \"stretching my back.\" States are more like short-term sensations, and the described world is tightly focused on my body and the things touching it.\\nOther models work too! That\\'s fine, there\\'s plenty to go around.The meta-model that talks about me having preferences in both of these models is the framing of competent preferences. If someone or something is observing humans, it looks for human preferences by seeing what the preferences are in \"agent-shaped\" models that are powerful for their size.\\n(At least, up to some finite amount of shuffling that\\'s like a choice of prior or universal Turing machine. Also note that the details of the definition of \"agent-shaped\" matter—we\\'ll come back to this under the umbrella meta-preferences.)\\nSo when we call certain behavior \"bad,\" the usage of that word might carry with it the implication of what way of thinking about the world that judgment is situated in, like how \"I like dancing\" makes sense when situated in a model of chunks of my day. There\\'s not one True Model in which the True Meaning of the word \"bad\" is expressed, though there can still be regularities among the different notions of badness.\\n**II—Mergers**\\nWhat were the patterns that stood out from my previous discussions of what humans think of as bad behavior in value learning?\\nThe most common type of failure, especially in modern day AI, is when humans are actively wrong about what\\'s going to happen. They have something specific in mind when designing an AI, like training a boat to win the race, but then they run it and don\\'t get what they wanted. The boat crashes and is on fire. We could make the boat racing game more of a value learning problem by training on human demonstrations rather than the score, and crashing and being on fire would *still *be bad behavior.\\nFor simple systems where humans are good at understanding the state space and picturing what they want, this is the only standard you need, but for more complicated systems (e.g. our galaxy) humans can only understand small parts or simple properties of the whole system, and we apply our preferences to those parts we can understand. From the inside, it can be hard to feel the distinction! We want things about tic-tac-toe or about the galaxy with the same set of emotions. What makes deciding what to do with the galaxy different is that we have these scattered preferences about different parts and patterns, and the different parts don\\'t stay neatly separate from each other. They can interact or overlap in ways that bring our preferences into conflict.\\nThis is a key point. Inter-preference conflicts aren\\'t an issue that ever comes up if you think of humans as having a utility function, but they\\'re almost *unavoidable *if you think of humans as a physical systems with different possible models. The nail in the coffin is that we humans can\\'t fit the whole galaxy into our heads, nor could evolution fit it into our genes, and so out of necessity we have to use simple heuristics that work well pragmatically but don\\'t fit together perfectly. If humans don\\'t resolve their preference conflicts well, this can lead to bad behavior like thinking the grass is always greener on the other side of the decision tree.\\nBad preference aggregation can also lead to new-ish bad behavior on the part of a value learner. This bad behavior can look like encountering a situation where humans are conflicted or inconsistent, and then resolving that conflict using a method <|endoftext|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T54RFVmS1NRv"
      },
      "source": [
        "train, val = train_test_split(musk_tweets, test_size=0.2)\n",
        "test, val = train_test_split(val, test_size=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oy37CKxfmK7",
        "outputId": "efcf5d88-1c83-4c26-f5e9-2ae996e41874"
      },
      "source": [
        "print(\"Number of Train examples: \" + str(len(train)))\n",
        "print(\"Number of Val examples: \" + str(len(val)))\n",
        "print(\"Number of Test examples: \" + str(len(test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Train examples: 27148\n",
            "Number of Val examples: 3394\n",
            "Number of Test examples: 3393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCoTxXg81quI"
      },
      "source": [
        "train_path = f'{directory}' + 'train.csv'\n",
        "val_path = f'{directory}' + 'val.csv'\n",
        "test_path = f'{directory}' + 'test.csv'\n",
        "\n",
        "train.to_csv(train_path, index=False)\n",
        "val.to_csv(val_path, index=False)\n",
        "test.to_csv(test_path, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9ZvyvZyerDT"
      },
      "source": [
        "# Fine-Tuning GPT-2\n",
        "\n",
        "If we're looking to fine-tune models which are found on the HuggingFace model hub, then it becomes much easier to fine-tune the models since HuggingFace provides us with scripts.\n",
        "\n",
        "From the `transformers` repo:\n",
        "\n",
        "> There are two sets of scripts provided. The first set leverages the Trainer API. The second set with no_trainer in the suffix uses a custom training loop and leverages the 🤗 Accelerate library. Both sets use the 🤗 Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.\n",
        "\n",
        "You can learn more about it here: https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling\n",
        "\n",
        "We will be using the script that leveraged the Trainer API. We can download the script by running:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty4g9WUhfMz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6559a1-e000-4f8f-fc50-25e2a08111a7"
      },
      "source": [
        "if not os.path.exists('/gpt-2/run_clm.py'):\n",
        "    !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/run_clm.py -P gpt-2/"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-02 21:00:52--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/run_clm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25025 (24K) [text/plain]\n",
            "Saving to: ‘gpt-2/run_clm.py.1’\n",
            "\n",
            "\rrun_clm.py.1          0%[                    ]       0  --.-KB/s               \rrun_clm.py.1        100%[===================>]  24.44K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2022-07-02 21:00:52 (7.72 MB/s) - ‘gpt-2/run_clm.py.1’ saved [25025/25025]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXvm0wpQxS10"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "RV6nEm50t2Fk",
        "outputId": "8d023eb7-5d23-4934-ecc5-5a580aeebd0e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python gpt-2/run_clm.py \\\n",
        "    --model_name_or_path \"gpt-2/tmp/alignment-texts-clm\" \\\n",
        "    --train_file alignment_texts_7288.csv \\\n",
        "    --do_train \\\n",
        "    --fp16=True \\\n",
        "    --overwrite_cache=True \\\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    --output_dir gpt-2/tmp/alignment-forum \\\n",
        "    --overwrite_output_dir=\"no\" \\\n",
        "    --save_total_limit=1 \\\n",
        "    --gradient_accumulation_steps=8 \\\n",
        "    --warmup_steps=10 \\\n",
        "    --learning_rate=3e-5 \\\n",
        "    --weight_decay=0.1 \\\n",
        "    --report_to=\"wandb\" \\\n",
        "    --run_name=\"gpt-2-alignment-forum-20220703\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7om5uFvezPsg",
        "outputId": "a9463ebb-c276-4610-c84f-0084efbcc156"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/04/2022 01:24:36 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "07/04/2022 01:24:36 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=8,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=gpt-2/tmp/alignment-forum/runs/Jul04_01-24-35_4c6c040081fd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=gpt-2/tmp/alignment-forum,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=gpt-2-alignment-forum-20220703,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=1,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=10,\n",
            "weight_decay=0.1,\n",
            "xpu_backend=None,\n",
            ")\n",
            "07/04/2022 01:24:37 - WARNING - datasets.builder - Using custom data configuration default-81baadd93982b6f6\n",
            "07/04/2022 01:24:37 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 3318.28it/s]\n",
            "07/04/2022 01:24:37 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "07/04/2022 01:24:37 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 101.98it/s]\n",
            "07/04/2022 01:24:37 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "07/04/2022 01:24:37 - INFO - datasets.builder - Generating train split\n",
            "07/04/2022 01:24:38 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 715.87it/s]\n",
            "07/04/2022 01:24:38 - WARNING - datasets.builder - Using custom data configuration default-81baadd93982b6f6\n",
            "07/04/2022 01:24:38 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "07/04/2022 01:24:38 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "07/04/2022 01:24:38 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
            "07/04/2022 01:24:38 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "07/04/2022 01:24:39 - WARNING - datasets.builder - Using custom data configuration default-81baadd93982b6f6\n",
            "07/04/2022 01:24:39 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "07/04/2022 01:24:39 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "07/04/2022 01:24:39 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
            "07/04/2022 01:24:39 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "[INFO|configuration_utils.py:662] 2022-07-04 01:24:39,760 >> loading configuration file gpt-2/tmp/alignment-texts-clm/config.json\n",
            "[INFO|configuration_utils.py:713] 2022-07-04 01:24:39,761 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt-2/tmp/alignment-texts-clm\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.21.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1721] 2022-07-04 01:24:39,765 >> Didn't find file gpt-2/tmp/alignment-texts-clm/added_tokens.json. We won't load it.\n",
            "[INFO|tokenization_utils_base.py:1799] 2022-07-04 01:24:39,767 >> loading file gpt-2/tmp/alignment-texts-clm/vocab.json\n",
            "[INFO|tokenization_utils_base.py:1799] 2022-07-04 01:24:39,767 >> loading file gpt-2/tmp/alignment-texts-clm/merges.txt\n",
            "[INFO|tokenization_utils_base.py:1799] 2022-07-04 01:24:39,767 >> loading file gpt-2/tmp/alignment-texts-clm/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:1799] 2022-07-04 01:24:39,767 >> loading file None\n",
            "[INFO|tokenization_utils_base.py:1799] 2022-07-04 01:24:39,767 >> loading file gpt-2/tmp/alignment-texts-clm/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1799] 2022-07-04 01:24:39,767 >> loading file gpt-2/tmp/alignment-texts-clm/tokenizer_config.json\n",
            "[INFO|modeling_utils.py:1997] 2022-07-04 01:24:41,752 >> loading weights file gpt-2/tmp/alignment-texts-clm/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:2389] 2022-07-04 01:24:43,610 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:2398] 2022-07-04 01:24:43,610 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt-2/tmp/alignment-texts-clm.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "07/04/2022 01:24:43 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7fd7549b4710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenizer on dataset:   0% 0/7 [00:00<?, ?ba/s]07/04/2022 01:24:44 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-1c80317fa3b1799d.arrow\n",
            "Running tokenizer on dataset: 100% 7/7 [00:07<00:00,  1.02s/ba]\n",
            "07/04/2022 01:24:50 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7fd7549b4830> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on dataset:   0% 0/1 [00:00<?, ?ba/s]07/04/2022 01:24:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-bdd640fb06671ad1.arrow\n",
            "Running tokenizer on dataset: 100% 1/1 [00:00<00:00,  2.75ba/s]\n",
            "07/04/2022 01:24:51 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.group_texts at 0x7fd7549b4950> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Grouping texts in chunks of 1024:   0% 0/7 [00:00<?, ?ba/s]07/04/2022 01:24:51 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-3eb13b9046685257.arrow\n",
            "Grouping texts in chunks of 1024: 100% 7/7 [00:06<00:00,  1.03ba/s]\n",
            "07/04/2022 01:24:58 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.group_texts at 0x7fd7549b4710> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Grouping texts in chunks of 1024:   0% 0/1 [00:00<?, ?ba/s]07/04/2022 01:24:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-81baadd93982b6f6/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-23b8c1e9392456de.arrow\n",
            "Grouping texts in chunks of 1024: 100% 1/1 [00:00<00:00,  2.83ba/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:533] 2022-07-04 01:25:02,222 >> Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1521] 2022-07-04 01:25:02,230 >> ***** Running training *****\n",
            "[INFO|trainer.py:1522] 2022-07-04 01:25:02,230 >>   Num examples = 6784\n",
            "[INFO|trainer.py:1523] 2022-07-04 01:25:02,230 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1524] 2022-07-04 01:25:02,230 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1525] 2022-07-04 01:25:02,230 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1526] 2022-07-04 01:25:02,230 >>   Gradient Accumulation steps = 8\n",
            "[INFO|trainer.py:1527] 2022-07-04 01:25:02,230 >>   Total optimization steps = 1272\n",
            "[INFO|integrations.py:580] 2022-07-04 01:25:02,231 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjacquesthibs\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/data/ai-alignment-dataset/wandb/run-20220704_012502-p0ms9nyq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgpt-2-alignment-forum-20220703\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jacquesthibs/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jacquesthibs/huggingface/runs/p0ms9nyq\u001b[0m\n",
            "{'loss': 2.9184, 'learning_rate': 1.8351822503961967e-05, 'epoch': 1.18}\n",
            " 39% 500/1272 [26:20<42:12,  3.28s/it][INFO|trainer.py:2510] 2022-07-04 01:51:24,315 >> Saving model checkpoint to gpt-2/tmp/alignment-forum/checkpoint-500\n",
            "[INFO|configuration_utils.py:451] 2022-07-04 01:51:24,321 >> Configuration saved in gpt-2/tmp/alignment-forum/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-04 01:51:25,718 >> Model weights saved in gpt-2/tmp/alignment-forum/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-04 01:51:25,722 >> tokenizer config file saved in gpt-2/tmp/alignment-forum/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-04 01:51:25,726 >> Special tokens file saved in gpt-2/tmp/alignment-forum/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 2.8483, 'learning_rate': 6.465927099841522e-06, 'epoch': 2.36}\n",
            " 79% 1000/1272 [52:45<14:50,  3.27s/it][INFO|trainer.py:2510] 2022-07-04 02:17:49,778 >> Saving model checkpoint to gpt-2/tmp/alignment-forum/checkpoint-1000\n",
            "[INFO|configuration_utils.py:451] 2022-07-04 02:17:49,783 >> Configuration saved in gpt-2/tmp/alignment-forum/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-04 02:17:51,168 >> Model weights saved in gpt-2/tmp/alignment-forum/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-04 02:17:51,172 >> tokenizer config file saved in gpt-2/tmp/alignment-forum/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-04 02:17:51,175 >> Special tokens file saved in gpt-2/tmp/alignment-forum/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|trainer.py:2588] 2022-07-04 02:17:55,989 >> Deleting older checkpoint [gpt-2/tmp/alignment-forum/checkpoint-500] due to args.save_total_limit\n",
            "100% 1272/1272 [1:07:10<00:00,  3.15s/it][INFO|trainer.py:1766] 2022-07-04 02:32:14,888 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 4032.6582, 'train_samples_per_second': 5.047, 'train_steps_per_second': 0.315, 'train_loss': 2.8758605861064024, 'epoch': 3.0}\n",
            "100% 1272/1272 [1:07:10<00:00,  3.17s/it]\n",
            "[INFO|trainer.py:2510] 2022-07-04 02:32:14,890 >> Saving model checkpoint to gpt-2/tmp/alignment-forum\n",
            "[INFO|configuration_utils.py:451] 2022-07-04 02:32:14,896 >> Configuration saved in gpt-2/tmp/alignment-forum/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-04 02:32:16,440 >> Model weights saved in gpt-2/tmp/alignment-forum/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-04 02:32:16,444 >> tokenizer config file saved in gpt-2/tmp/alignment-forum/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-04 02:32:16,448 >> Special tokens file saved in gpt-2/tmp/alignment-forum/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     2.8759\n",
            "  train_runtime            = 1:07:12.65\n",
            "  train_samples            =       6784\n",
            "  train_samples_per_second =      5.047\n",
            "  train_steps_per_second   =      0.315\n",
            "[INFO|modelcard.py:460] 2022-07-04 02:32:18,480 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▆█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 3.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 1272\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 1e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 2.8483\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.0635630870528e+16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 2.87586\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 4032.6582\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 5.047\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 0.315\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mgpt-2-alignment-forum-20220703\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/jacquesthibs/huggingface/runs/p0ms9nyq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220704_012502-p0ms9nyq/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOtpCx8KfVYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd641563-7ae1-4580-b699-41859fbd439b"
      },
      "source": [
        "# !python gpt-2/run_clm.py \\\n",
        "#     --model_name_or_path gpt2 \\\n",
        "#     --train_file alignment_texts_87606.csv \\\n",
        "#     --do_train \\\n",
        "#     --fp16=True \\\n",
        "#     --overwrite_cache=True \\\n",
        "#     --per_device_train_batch_size=2 \\\n",
        "#     --output_dir gpt-2/tmp/alignment-texts-clm \\\n",
        "#     --overwrite_output_dir=\"yes\" \\\n",
        "#     --save_total_limit=3 \\\n",
        "#     --save_steps=10000 \\\n",
        "#     --gradient_accumulation_steps=32 \\\n",
        "#     --warmup_steps=100 \\\n",
        "#     --learning_rate=3e-5 \\\n",
        "#     --weight_decay=0.1 \\\n",
        "#     --report_to=\"wandb\" \\\n",
        "#     --run_name=\"gpt-2-alignment-20220702\""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/02/2022 21:08:15 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True\n",
            "07/02/2022 21:08:15 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=32,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=gpt-2/tmp/alignment-texts-clm/runs/Jul02_21-08-14_2ef82b3c19b8,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=gpt-2/tmp/alignment-texts-clm,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['wandb'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=gpt-2-alignment-20220702,\n",
            "save_on_each_node=False,\n",
            "save_steps=10000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=3,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=100,\n",
            "weight_decay=0.1,\n",
            "xpu_backend=None,\n",
            ")\n",
            "07/02/2022 21:08:15 - WARNING - datasets.builder - Using custom data configuration default-7180ffe1010b33d0\n",
            "07/02/2022 21:08:15 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 3539.50it/s]\n",
            "07/02/2022 21:08:15 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "07/02/2022 21:08:16 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 23.19it/s]\n",
            "07/02/2022 21:08:17 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "07/02/2022 21:08:17 - INFO - datasets.builder - Generating train split\n",
            "07/02/2022 21:08:22 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 530.99it/s]\n",
            "07/02/2022 21:08:23 - WARNING - datasets.builder - Using custom data configuration default-7180ffe1010b33d0\n",
            "07/02/2022 21:08:23 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "07/02/2022 21:08:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "07/02/2022 21:08:23 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
            "07/02/2022 21:08:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "07/02/2022 21:08:23 - WARNING - datasets.builder - Using custom data configuration default-7180ffe1010b33d0\n",
            "07/02/2022 21:08:23 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "07/02/2022 21:08:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "07/02/2022 21:08:23 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
            "07/02/2022 21:08:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "[INFO|hub.py:592] 2022-07-02 21:08:23,835 >> https://huggingface.co/gpt2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpn2sbjbwt\n",
            "Downloading: 100% 665/665 [00:00<00:00, 824kB/s]\n",
            "[INFO|hub.py:596] 2022-07-02 21:08:24,129 >> storing https://huggingface.co/gpt2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|hub.py:604] 2022-07-02 21:08:24,129 >> creating metadata file for /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:664] 2022-07-02 21:08:24,130 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:713] 2022-07-02 21:08:24,131 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.21.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:396] 2022-07-02 21:08:24,404 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:664] 2022-07-02 21:08:24,679 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:713] 2022-07-02 21:08:24,680 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.21.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-02 21:08:25,231 >> https://huggingface.co/gpt2/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjkg5dn_3\n",
            "Downloading: 100% 0.99M/0.99M [00:00<00:00, 3.27MB/s]\n",
            "[INFO|hub.py:596] 2022-07-02 21:08:25,903 >> storing https://huggingface.co/gpt2/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|hub.py:604] 2022-07-02 21:08:25,903 >> creating metadata file for /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|hub.py:592] 2022-07-02 21:08:26,172 >> https://huggingface.co/gpt2/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpfl2zwp65\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 1.46MB/s]\n",
            "[INFO|hub.py:596] 2022-07-02 21:08:26,773 >> storing https://huggingface.co/gpt2/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|hub.py:604] 2022-07-02 21:08:26,773 >> creating metadata file for /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|hub.py:592] 2022-07-02 21:08:27,044 >> https://huggingface.co/gpt2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp6462b8_1\n",
            "Downloading: 100% 1.29M/1.29M [00:00<00:00, 3.54MB/s]\n",
            "[INFO|hub.py:596] 2022-07-02 21:08:27,760 >> storing https://huggingface.co/gpt2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|hub.py:604] 2022-07-02 21:08:27,760 >> creating metadata file for /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-02 21:08:28,605 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-02 21:08:28,605 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-02 21:08:28,605 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-02 21:08:28,605 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-02 21:08:28,605 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-02 21:08:28,605 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:664] 2022-07-02 21:08:28,884 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:713] 2022-07-02 21:08:28,885 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.21.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-02 21:08:29,232 >> https://huggingface.co/gpt2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp82azmds6\n",
            "Downloading: 100% 523M/523M [00:10<00:00, 52.7MB/s]\n",
            "[INFO|hub.py:596] 2022-07-02 21:08:39,735 >> storing https://huggingface.co/gpt2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|hub.py:604] 2022-07-02 21:08:39,735 >> creating metadata file for /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1999] 2022-07-02 21:08:39,736 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:2389] 2022-07-02 21:08:41,618 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:2398] 2022-07-02 21:08:41,618 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "07/02/2022 21:08:41 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7fbca2405200> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenizer on dataset:   0% 0/84 [00:00<?, ?ba/s]07/02/2022 21:08:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-1c80317fa3b1799d.arrow\n",
            "Running tokenizer on dataset: 100% 84/84 [01:44<00:00,  1.25s/ba]\n",
            "07/02/2022 21:10:26 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7fbdede284d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on dataset:   0% 0/5 [00:00<?, ?ba/s]07/02/2022 21:10:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-bdd640fb06671ad1.arrow\n",
            "Running tokenizer on dataset: 100% 5/5 [00:05<00:00,  1.13s/ba]\n",
            "07/02/2022 21:10:32 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.group_texts at 0x7fbdede285f0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Grouping texts in chunks of 1024:   0% 0/84 [00:00<?, ?ba/s]07/02/2022 21:10:32 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-3eb13b9046685257.arrow\n",
            "Grouping texts in chunks of 1024: 100% 84/84 [01:30<00:00,  1.08s/ba]\n",
            "07/02/2022 21:12:02 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.group_texts at 0x7fbdede284d0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Grouping texts in chunks of 1024:   0% 0/5 [00:00<?, ?ba/s]07/02/2022 21:12:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-7180ffe1010b33d0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-23b8c1e9392456de.arrow\n",
            "Grouping texts in chunks of 1024: 100% 5/5 [00:04<00:00,  1.04ba/s]\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[INFO|trainer.py:533] 2022-07-02 21:12:23,741 >> Using cuda_amp half precision backend\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1521] 2022-07-02 21:12:23,754 >> ***** Running training *****\n",
            "[INFO|trainer.py:1522] 2022-07-02 21:12:23,754 >>   Num examples = 81521\n",
            "[INFO|trainer.py:1523] 2022-07-02 21:12:23,754 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1524] 2022-07-02 21:12:23,754 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1525] 2022-07-02 21:12:23,754 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1526] 2022-07-02 21:12:23,754 >>   Gradient Accumulation steps = 32\n",
            "[INFO|trainer.py:1527] 2022-07-02 21:12:23,754 >>   Total optimization steps = 3819\n",
            "[INFO|integrations.py:580] 2022-07-02 21:12:23,756 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjacquesthibs\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.20\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/data/ai-alignment-dataset/wandb/run-20220702_211223-3cbz2m0o\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgpt-2-alignment-20220702\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jacquesthibs/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jacquesthibs/huggingface/runs/3cbz2m0o\u001b[0m\n",
            "{'loss': 3.0409, 'learning_rate': 2.677332616294703e-05, 'epoch': 0.39}\n",
            "{'loss': 2.9125, 'learning_rate': 2.2739983866630816e-05, 'epoch': 0.79}\n",
            "{'loss': 2.875, 'learning_rate': 1.87066415703146e-05, 'epoch': 1.18}\n",
            " 52% 2000/3819 [6:59:09<6:24:59, 12.70s/it]{'loss': 2.8497, 'learning_rate': 1.4673299273998387e-05, 'epoch': 1.57}\n",
            " 65% 2500/3819 [8:44:03<4:39:12, 12.70s/it]{'loss': 2.834, 'learning_rate': 1.0639956977682173e-05, 'epoch': 1.96}\n",
            "{'loss': 2.8204, 'learning_rate': 6.606614681365959e-06, 'epoch': 2.36}\n",
            "{'loss': 2.8061, 'learning_rate': 2.5732723850497446e-06, 'epoch': 2.75}\n",
            "100% 3819/3819 [13:21:05<00:00, 12.58s/it]{'train_runtime': 48067.5043, 'train_samples_per_second': 5.088, 'train_steps_per_second': 0.079, 'train_loss': 2.8716601671177417, 'epoch': 3.0}\n",
            "[INFO|trainer.py:1766] 2022-07-03 10:33:31,258 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "100% 3819/3819 [13:21:05<00:00, 12.59s/it]\n",
            "[INFO|trainer.py:2510] 2022-07-03 10:33:31,261 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm\n",
            "[INFO|configuration_utils.py:451] 2022-07-03 10:33:31,268 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-03 10:33:32,888 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-03 10:33:32,894 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-03 10:33:32,898 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =         3.0\n",
            "  train_loss               =      2.8717\n",
            "  train_runtime            = 13:21:07.50\n",
            "  train_samples            =       81521\n",
            "  train_samples_per_second =       5.088\n",
            "  train_steps_per_second   =       0.079\n",
            "[INFO|modelcard.py:460] 2022-07-03 10:33:33,339 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ▁▂▃▄▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ▁▂▃▄▅▆▇█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate █▇▆▄▃▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss █▄▃▂▂▁▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 3.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 3819\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 2.8061\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 1.27779119824896e+17\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 2.87166\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 48067.5043\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 5.088\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 0.079\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mgpt-2-alignment-20220702\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/jacquesthibs/huggingface/runs/3cbz2m0o\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220702_211223-3cbz2m0o/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.finish()"
      ],
      "metadata": {
        "id": "oAS6ifGEym3S"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7q0Q066UWo-"
      },
      "source": [
        "# Let's use the model!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_DIR = \"gpt-2/tmp/alignment-forum\"\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(OUTPUT_DIR)\n",
        "model = GPT2LMHeadModel.from_pretrained(OUTPUT_DIR)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "C2S7xaafWosb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdIAGG1a27xX"
      },
      "source": [
        "NUM_COMPLETIONS = 1\n",
        "\n",
        "def generate(input_str, length=50, n=NUM_COMPLETIONS):\n",
        "  cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i in range(length):\n",
        "      outputs = model(cur_ids[:, -1024:], labels=cur_ids[:, -1024:])\n",
        "      loss, logits = outputs[:2]\n",
        "      softmax_logits = torch.softmax(logits[0,-1], dim=0)\n",
        "      next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n)\n",
        "      cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim=1)\n",
        "    output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "    output_text = tokenizer.decode(output_list)\n",
        "    return output_text.replace(\"<|endoftext|>\", \"\")\n",
        "\n",
        "def choose_from_top(probs, n=NUM_COMPLETIONS):\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF05N8SmVIIr",
        "outputId": "466f0e07-b6b8-47b1-b192-f576e461d4dc"
      },
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "generated_text = generate(\"**I—Meanings**\\nNow that we have some more concrete thinking under our belt, it\\'s time to circle back on Goodhart\\'s law for value learners.\")\n",
        "end = time.time()\n",
        "print(generated_text)\n",
        "print(end - start)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**I—Meanings**\n",
            "Now that we have some more concrete thinking under our belt, it's time to circle back on Goodhart's law for value learners.\n",
            "**Definition 1:** *A learner *(A) *(A) *(A) *(A) *(A) *(A) *(A) *(A) *(A) *(A)\n",
            "9.844796895980835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kzOlqi8eMys"
      },
      "source": [
        "# Compressing the Model\n",
        "\n",
        "Let's save the model as a `tar.gz` file so that we can save it in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOmuQ4tUVei9"
      },
      "source": [
        "!tar -czf gpt-2-elon-tweets.tar.gz gpt-2/tuned-models/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
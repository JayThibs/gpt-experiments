{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tuning-GPT-2-with-HuggingFace.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNuw4lFP5Ekjmu3QqkoG4uY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/gpt-experiments/blob/main/notebooks/Fine_Tuning_GPT_2_with_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lq-qHwDRba4"
      },
      "source": [
        "# Introduction to GPT\n",
        "\n",
        "GPT stands for \"Generative Pre-Trained Transformer.\"\n",
        "\n",
        "* Generative because it is used to generate text.\n",
        "* Pre-trained because it was trained on a large corpus of unstructured text to make its weights learn things from text like structure, syntax, and general knowledge.\n",
        "* Transformer because it uses the `decoder` part of the transformer architecture. In other words, you can give it text and it will decode what it needs to output in response (by guessing the next words that follow the input text).\n",
        "\n",
        "GPT-3 is a massive model. Much too massive to fit in a puny Google Colab GPU and its RAM. Therefore, here we'll use its predecessor, GPT-2, since we can actually fit in on our Colab machine.\n",
        "\n",
        "GPT models are particular cool because they are able to be applied to many downstream NLP tasks without having to fine-tune the model. Through, few-shot learning, the model can predict what should come next. However, the only current limitation is that the model is limited by its window size. In other words, a model like GPT-J can only fit 2048 tokens as input. That means that in some cases, we might not be able to fit in enough examples to get fantastic results. And when we're in a production environment, it can often be worth it to fine-tune a GPT model to your type of data so that it can perform better.\n",
        "\n",
        "Models like GPT-3 have been show to show a lot of great results across many different tasks without fine-tuning, even when we compare them to a model like BERT that was specifically fine-tuned on the data. However, it is still often recommended to fine-tune the model to get even better performance. And, in cases like medical data, GPT-3 doesn't necessarily perform well compared to a model like BioBERT.\n",
        "\n",
        "Perhaps a good rule of thumb is to start by doing creative prompt engineering with your GPT model first to try to get great results, and then you can decide afterwards if you'd like to fine-tune the model for even better accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjSP3oGNHyJd",
        "outputId": "1e8b991c-4cf7-49a5-f973-abc80a2cba08"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 26 02:42:15 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap-rQ1P1Y058"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr14u8fcYR4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bc57b95-148d-4647-fde7-d019b734025e"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers pytorch-lightning beautifulsoup4 datasets --quiet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 523 kB 5.0 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 290 kB 72.2 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596 kB 57.2 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 59 kB 7.1 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3 MB 39.5 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 895 kB 46.4 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132 kB 68.8 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 329 kB 69.2 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 829 kB 40.1 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.1 MB 35.4 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 243 kB 54.3 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160 kB 62.7 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 271 kB 60.7 MB/s \n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 192 kB 63.8 MB/s \n",
            "\u001b[?25h  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84TRkXXhY2U5"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvxQKSqQY3Fa"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import seed_everything\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5tY6KPsgUFQ"
      },
      "source": [
        "# Mounting Google Drive\n",
        "\n",
        "Here we will mount our Google Drive so that we can grab data and save the HuggingFace scripts, and save the model once we've fine-tuned it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNeXrm1qgWRp",
        "outputId": "aab18b45-6220-48d3-c84b-0e28cc28ee67"
      },
      "source": [
        "# For saving the data locally\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIMELLTfg76e",
        "outputId": "9fa90147-8458-4f0d-b59b-4fa2f3278785"
      },
      "source": [
        "%cd drive/MyDrive/code-projects/fine-tune-gpt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/code-projects/fine-tune-gpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-AVhqkVeluk"
      },
      "source": [
        "# Getting the Data\n",
        "\n",
        "We'll be fine-tuning GPT-2 on Elon Musk tweets to see if we can start taking the first steps towards an Elon AI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3h0UWoLep_O"
      },
      "source": [
        "directory = 'data/elon-musk/tweets-2010-2021/'\n",
        "musk_tweets = pd.read_csv(f'{directory}' + '2010.csv')\n",
        "\n",
        "list_of_years = ['2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']\n",
        "\n",
        "for year in list_of_years:\n",
        "    temp_df = pd.read_csv(f'{directory}' + year + '.csv')\n",
        "    musk_tweets = musk_tweets.append(temp_df, ignore_index=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "TyskPpaMrtgA",
        "outputId": "e06ada62-c82f-4684-f881-8d79e033b3e0"
      },
      "source": [
        "musk_tweets.head(3)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>date</th>\n",
              "      <th>timezone</th>\n",
              "      <th>place</th>\n",
              "      <th>tweet</th>\n",
              "      <th>language</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>cashtags</th>\n",
              "      <th>user_id</th>\n",
              "      <th>user_id_str</th>\n",
              "      <th>username</th>\n",
              "      <th>name</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>link</th>\n",
              "      <th>urls</th>\n",
              "      <th>photos</th>\n",
              "      <th>video</th>\n",
              "      <th>thumbnail</th>\n",
              "      <th>retweet</th>\n",
              "      <th>nlikes</th>\n",
              "      <th>nreplies</th>\n",
              "      <th>nretweets</th>\n",
              "      <th>quote_url</th>\n",
              "      <th>search</th>\n",
              "      <th>near</th>\n",
              "      <th>geo</th>\n",
              "      <th>source</th>\n",
              "      <th>user_rt_id</th>\n",
              "      <th>user_rt</th>\n",
              "      <th>retweet_id</th>\n",
              "      <th>reply_to</th>\n",
              "      <th>retweet_date</th>\n",
              "      <th>translate</th>\n",
              "      <th>trans_src</th>\n",
              "      <th>trans_dest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>15434727182</td>\n",
              "      <td>15434727182</td>\n",
              "      <td>1.275676e+12</td>\n",
              "      <td>2010-06-04 18:31:57</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>44196397</td>\n",
              "      <td>44196397</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>Elon Musk</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>https://twitter.com/elonmusk/status/15434727182</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>4652</td>\n",
              "      <td>391</td>\n",
              "      <td>348</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>152153637639028736</td>\n",
              "      <td>152151847614943233</td>\n",
              "      <td>1.325111e+12</td>\n",
              "      <td>2011-12-28 22:27:08</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@TheOnion So true :)</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>44196397</td>\n",
              "      <td>44196397</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>Elon Musk</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>https://twitter.com/elonmusk/status/152153637639028736</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>151809315026636800</td>\n",
              "      <td>151809315026636800</td>\n",
              "      <td>1.325029e+12</td>\n",
              "      <td>2011-12-27 23:38:55</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>If you ever wanted to know the *real* truth about the moon landings ...(best Onion article ever)  http://t.co/pgNEJsjI</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>44196397</td>\n",
              "      <td>44196397</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>Elon Musk</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>https://twitter.com/elonmusk/status/151809315026636800</td>\n",
              "      <td>['http://j.mp/vLhhov']</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>39</td>\n",
              "      <td>13</td>\n",
              "      <td>34</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                  id  ...  trans_src  trans_dest\n",
              "0           0         15434727182  ...        NaN         NaN\n",
              "1           0  152153637639028736  ...        NaN         NaN\n",
              "2           1  151809315026636800  ...        NaN         NaN\n",
              "\n",
              "[3 rows x 39 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H79ADkFnsrcP",
        "outputId": "07d5229c-c58e-4236-fa1d-9b7bf013252c"
      },
      "source": [
        "musk_tweets.rename(columns={'tweet': 'text'}, inplace=True)\n",
        "musk_tweets = musk_tweets['text']\n",
        "musk_tweets.head(2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.\n",
              "1                                                                            @TheOnion So true :)\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7veNZgJPs0RL"
      },
      "source": [
        "import re\n",
        "musk_tweets.replace(to_replace=\"@[A-Za-z0-9]+\", value=\"\", regex=True, inplace=True)\n",
        "musk_tweets.replace(to_replace=r'http\\S+', value=\"\", regex=True, inplace=True)\n",
        "musk_tweets.replace(to_replace=r'#[A-Za-z0-9]+', value=\"\", regex=True, inplace=True)\n",
        "musk_tweets = musk_tweets[musk_tweets.str.len()>=20]\n",
        "musk_tweets = musk_tweets[musk_tweets.str.len()<=80]\n",
        "musk_tweets = \"<endoftext>\" + musk_tweets + \"<endoftext>\""
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1osSu0xot9eY",
        "outputId": "e6df8b2f-cc27-422c-91a9-ea4c0fafdc92"
      },
      "source": [
        "musk_tweets.head(20)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3       <endoftext>Walked around a neighborhood recently rebuilt with help from APJ and others  <endoftext>\n",
              "7               <endoftext>Single character Tweets are the ulitmate extension of the Twitmeme...<endoftext>\n",
              "11                                <endoftext>Kanye stopped by the SpaceX rocket factory today.  <endoftext>\n",
              "15                                         <endoftext>Yum! Even better than deep fried butter:  <endoftext>\n",
              "22      <endoftext>Am reading a great biography of Ben Franklin by Isaacson. Highly recommended.<endoftext>\n",
              "23                                                  <endoftext>That was a total non sequitur btw<endoftext>\n",
              "28                                              <endoftext> Interesting premise. I will read it.<endoftext>\n",
              "29                                                               <endoftext> V cute! Merry Xmas.<endoftext>\n",
              "30                                             <endoftext>Cowboy riding the rocket no problemo  <endoftext>\n",
              "31                                  <endoftext>Single camera view of the 40 meter rocket hover  <endoftext>\n",
              "34            <endoftext>Very much agree with this great review of Chasing Ice on Ars Technica  <endoftext>\n",
              "37                      <endoftext>Good piece on volcanoes â€œ: The USGS would seem to disagree.  <endoftext>\n",
              "38                   <endoftext> Roughly equivalent to a 747 flight from LA to London (one way).<endoftext>\n",
              "42                        <endoftext>Cool video recap of the big events of the year by Google   <endoftext>\n",
              "44                  <endoftext> _in_Space Yup or Douglas Aircraft, creator of the awesome DC-3  <endoftext>\n",
              "52                                <endoftext>Interesting possible answer to the Fermi Paradox   <endoftext>\n",
              "54    <endoftext>Uranium ore now available on Amazon! So much more convenient than Uzbekistan   <endoftext>\n",
              "60                       <endoftext>Short vid of the recent  mission to the Intl Space Station  <endoftext>\n",
              "61                     <endoftext> SS Heart of Gold powered by the Infinite Improbability Drive!<endoftext>\n",
              "63                           <endoftext>Liam Neeson's \"Life's Too Short\" sketch is super funny  <endoftext>\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2CsHkXf1Z8b",
        "outputId": "42d2ed44-53db-4a84-b138-59ce3070cfc8"
      },
      "source": [
        "len(musk_tweets)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16051"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyw9hWomAV1H",
        "outputId": "1db1ddd1-5596-4c57-f95b-4dacc3c17a8d"
      },
      "source": [
        "musk_tweets.dropna(inplace=True)\n",
        "len(musk_tweets)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16051"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60UIFTAY1N2l"
      },
      "source": [
        "## Training Splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T54RFVmS1NRv"
      },
      "source": [
        "train, val = train_test_split(musk_tweets, test_size=0.1)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCoTxXg81quI"
      },
      "source": [
        "train_path = f'{directory}' + 'train.csv'\n",
        "val_path = f'{directory}' + 'val.csv'\n",
        "\n",
        "train.to_csv(train_path, index=False)\n",
        "val.to_csv(val_path, index=False)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9ZvyvZyerDT"
      },
      "source": [
        "# Fine-Tuning GPT-2\n",
        "\n",
        "If we're looking to fine-tune models which are found on the HuggingFace model hub, then it becomes much easier to fine-tune the models since HuggingFace provides us with scripts.\n",
        "\n",
        "From the `transformers` repo:\n",
        "\n",
        "> There are two sets of scripts provided. The first set leverages the Trainer API. The second set with no_trainer in the suffix uses a custom training loop and leverages the ðŸ¤— Accelerate library. Both sets use the ðŸ¤— Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.\n",
        "\n",
        "You can learn more about it here: https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling\n",
        "\n",
        "We will be using the script that leveraged the Trainer API. We can download the script by running:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty4g9WUhfMz0"
      },
      "source": [
        "if os.path.exists('/gpt-2/run_clm.py'):\n",
        "    !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/run_clm.py -P gpt-2/"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXvm0wpQxS10"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOtpCx8KfVYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8947f24-5e1b-4477-e31a-1b18b3b34745"
      },
      "source": [
        "!python gpt-2/run_clm.py \\\n",
        "    --model_name_or_path gpt2 \\\n",
        "    --train_file data/elon-musk/tweets-2010-2021/train.csv \\\n",
        "    --validation_file data/elon-musk/tweets-2010-2021/val.csv \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --output_dir gpt-2/tmp/elon-test-clm \\\n",
        "    --overwrite_output_dir"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/26/2021 02:46:46 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/26/2021 02:46:46 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=gpt-2/tmp/elon-test-clm/runs/Nov26_02-46-46_b3979c523556,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=gpt-2/tmp/elon-test-clm,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=gpt-2/tmp/elon-test-clm,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "11/26/2021 02:46:47 - WARNING - datasets.builder - Using custom data configuration default-00c6db84d87a888a\n",
            "11/26/2021 02:46:47 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-00c6db84d87a888a/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-00c6db84d87a888a/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n",
            "100% 2/2 [00:00<00:00, 2744.96it/s]\n",
            "11/26/2021 02:46:47 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "11/26/2021 02:46:47 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "100% 2/2 [00:00<00:00, 98.64it/s]\n",
            "11/26/2021 02:46:47 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "11/26/2021 02:46:47 - INFO - datasets.builder - Generating split train\n",
            "11/26/2021 02:46:47 - INFO - datasets.builder - Generating split validation\n",
            "Traceback (most recent call last):\n",
            "  File \"gpt-2/run_clm.py\", line 526, in <module>\n",
            "    main()\n",
            "  File \"gpt-2/run_clm.py\", line 286, in main\n",
            "    raw_datasets = load_dataset(extension, data_files=data_files, cache_dir=model_args.cache_dir, **dataset_args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/load.py\", line 1637, in load_dataset\n",
            "    use_auth_token=use_auth_token,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/builder.py\", line 608, in download_and_prepare\n",
            "    dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/builder.py\", line 697, in _download_and_prepare\n",
            "    self._prepare_split(split_generator, **prepare_split_kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/builder.py\", line 1159, in _prepare_split\n",
            "    writer.write_table(table)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py\", line 442, in write_table\n",
            "    pa_table = pa.Table.from_arrays([pa_table[name] for name in self._schema.names], schema=self._schema)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/datasets/arrow_writer.py\", line 442, in <listcomp>\n",
            "    pa_table = pa.Table.from_arrays([pa_table[name] for name in self._schema.names], schema=self._schema)\n",
            "  File \"pyarrow/table.pxi\", line 1206, in pyarrow.lib.Table.__getitem__\n",
            "  File \"pyarrow/table.pxi\", line 1753, in pyarrow.lib.Table.column\n",
            "  File \"pyarrow/table.pxi\", line 1728, in pyarrow.lib.Table._ensure_integer_index\n",
            "KeyError: 'Field \"<endoftext>_Livio Hell of a wig collection too<endoftext>\" does not exist in table schema'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdIAGG1a27xX"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}
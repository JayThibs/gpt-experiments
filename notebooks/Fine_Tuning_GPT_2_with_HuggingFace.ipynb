{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tuning-GPT-2-with-HuggingFace.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNWXVdNbbnflh+V1bhV0US7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/gpt-experiments/blob/main/notebooks/Fine_Tuning_GPT_2_with_HuggingFace.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lq-qHwDRba4"
      },
      "source": [
        "# Introduction to GPT\n",
        "\n",
        "GPT stands for \"Generative Pre-Trained Transformer.\"\n",
        "\n",
        "* Generative because it is used to generate text.\n",
        "* Pre-trained because it was trained on a large corpus of unstructured text to make its weights learn things from text like structure, syntax, and general knowledge.\n",
        "* Transformer because it uses the `decoder` part of the transformer architecture. In other words, you can give it text and it will decode what it needs to output in response (by guessing the next words that follow the input text).\n",
        "\n",
        "GPT-3 is a massive model. Much too massive to fit in a puny Google Colab GPU and its RAM. Therefore, here we'll use its predecessor, GPT-2, since we can actually fit in on our Colab machine.\n",
        "\n",
        "GPT models are particular cool because they are able to be applied to many downstream NLP tasks without having to fine-tune the model. Through, few-shot learning, the model can predict what should come next. However, the only current limitation is that the model is limited by its window size. In other words, a model like GPT-J can only fit 2048 tokens as input. That means that in some cases, we might not be able to fit in enough examples to get fantastic results. And when we're in a production environment, it can often be worth it to fine-tune a GPT model to your type of data so that it can perform better.\n",
        "\n",
        "Models like GPT-3 have been show to show a lot of great results across many different tasks without fine-tuning, even when we compare them to a model like BERT that was specifically fine-tuned on the data. However, it is still often recommended to fine-tune the model to get even better performance. And, in cases like medical data, GPT-3 doesn't necessarily perform well compared to a model like BioBERT.\n",
        "\n",
        "Perhaps a good rule of thumb is to start by doing creative prompt engineering with your GPT model first to try to get great results, and then you can decide afterwards if you'd like to fine-tune the model for even better accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjSP3oGNHyJd",
        "outputId": "9dd87b01-35ba-4c65-c0e9-f26b24adcc53"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov 27 03:31:53 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap-rQ1P1Y058"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr14u8fcYR4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12fa7f1f-80f5-4dd1-88e4-95a15a9e9275"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers pytorch-lightning beautifulsoup4 datasets --quiet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 523 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 298 kB 87.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 56.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 43.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 59 kB 8.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 60.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 68.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 329 kB 68.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 67.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 53.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 243 kB 87.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 192 kB 83.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 160 kB 74.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 90.9 MB/s \n",
            "\u001b[?25h  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84TRkXXhY2U5"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvxQKSqQY3Fa"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import seed_everything\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5tY6KPsgUFQ"
      },
      "source": [
        "# Mounting Google Drive\n",
        "\n",
        "Here we will mount our Google Drive so that we can grab data and save the HuggingFace scripts, and save the model once we've fine-tuned it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNeXrm1qgWRp",
        "outputId": "f53c08fc-4834-401d-be51-067aa9e3e446"
      },
      "source": [
        "# For saving the data locally\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIMELLTfg76e",
        "outputId": "58eded61-d039-4a51-b4a1-6febd9c71bb5"
      },
      "source": [
        "%cd drive/MyDrive/code-projects/fine-tune-gpt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/code-projects/fine-tune-gpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-AVhqkVeluk"
      },
      "source": [
        "# Getting the Data\n",
        "\n",
        "We'll be fine-tuning GPT-2 on Elon Musk tweets to see if we can start taking the first steps towards an Elon AI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3h0UWoLep_O"
      },
      "source": [
        "directory = 'data/elon-musk/tweets-2010-2021/'\n",
        "musk_tweets = pd.read_csv(f'{directory}' + '2010.csv')\n",
        "\n",
        "list_of_years = ['2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']\n",
        "\n",
        "for year in list_of_years:\n",
        "    temp_df = pd.read_csv(f'{directory}' + year + '.csv')\n",
        "    musk_tweets = musk_tweets.append(temp_df, ignore_index=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "TyskPpaMrtgA",
        "outputId": "81cebe16-15e1-4dbd-9ab7-54b2ff171f37"
      },
      "source": [
        "musk_tweets.head(3)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>date</th>\n",
              "      <th>timezone</th>\n",
              "      <th>place</th>\n",
              "      <th>tweet</th>\n",
              "      <th>language</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>cashtags</th>\n",
              "      <th>user_id</th>\n",
              "      <th>user_id_str</th>\n",
              "      <th>username</th>\n",
              "      <th>name</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>link</th>\n",
              "      <th>urls</th>\n",
              "      <th>photos</th>\n",
              "      <th>video</th>\n",
              "      <th>thumbnail</th>\n",
              "      <th>retweet</th>\n",
              "      <th>nlikes</th>\n",
              "      <th>nreplies</th>\n",
              "      <th>nretweets</th>\n",
              "      <th>quote_url</th>\n",
              "      <th>search</th>\n",
              "      <th>near</th>\n",
              "      <th>geo</th>\n",
              "      <th>source</th>\n",
              "      <th>user_rt_id</th>\n",
              "      <th>user_rt</th>\n",
              "      <th>retweet_id</th>\n",
              "      <th>reply_to</th>\n",
              "      <th>retweet_date</th>\n",
              "      <th>translate</th>\n",
              "      <th>trans_src</th>\n",
              "      <th>trans_dest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>15434727182</td>\n",
              "      <td>15434727182</td>\n",
              "      <td>1.275676e+12</td>\n",
              "      <td>2010-06-04 18:31:57</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>44196397</td>\n",
              "      <td>44196397</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>Elon Musk</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>https://twitter.com/elonmusk/status/15434727182</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>4652</td>\n",
              "      <td>391</td>\n",
              "      <td>348</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>152153637639028736</td>\n",
              "      <td>152151847614943233</td>\n",
              "      <td>1.325111e+12</td>\n",
              "      <td>2011-12-28 22:27:08</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@TheOnion So true :)</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>44196397</td>\n",
              "      <td>44196397</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>Elon Musk</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>https://twitter.com/elonmusk/status/152153637639028736</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>151809315026636800</td>\n",
              "      <td>151809315026636800</td>\n",
              "      <td>1.325029e+12</td>\n",
              "      <td>2011-12-27 23:38:55</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>If you ever wanted to know the *real* truth about the moon landings ...(best Onion article ever)  http://t.co/pgNEJsjI</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>44196397</td>\n",
              "      <td>44196397</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>Elon Musk</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>https://twitter.com/elonmusk/status/151809315026636800</td>\n",
              "      <td>['http://j.mp/vLhhov']</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>39</td>\n",
              "      <td>13</td>\n",
              "      <td>34</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                  id  ...  trans_src  trans_dest\n",
              "0           0         15434727182  ...        NaN         NaN\n",
              "1           0  152153637639028736  ...        NaN         NaN\n",
              "2           1  151809315026636800  ...        NaN         NaN\n",
              "\n",
              "[3 rows x 39 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H79ADkFnsrcP",
        "outputId": "1900e59a-8d20-4584-9fe9-4a186562bef5"
      },
      "source": [
        "musk_tweets.rename(columns={'tweet': 'text'}, inplace=True)\n",
        "musk_tweets = musk_tweets['text']\n",
        "musk_tweets.head(2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.\n",
              "1                                                                            @TheOnion So true :)\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7veNZgJPs0RL"
      },
      "source": [
        "musk_tweets.replace(to_replace=\"@[A-Za-z0-9]+\", value=\"\", regex=True, inplace=True)\n",
        "musk_tweets.replace(to_replace=r'http\\S+', value=\"\", regex=True, inplace=True)\n",
        "musk_tweets.replace(to_replace=r'#[A-Za-z0-9]+', value=\"\", regex=True, inplace=True)\n",
        "musk_tweets = musk_tweets[musk_tweets.str.len()>=20]\n",
        "# musk_tweets = \"<endoftext>\" + musk_tweets + \"<endoftext>\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1osSu0xot9eY",
        "outputId": "6f702774-c920-4abc-a21d-8e49eee0acbf"
      },
      "source": [
        "musk_tweets.head(20)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                 <endoftext>Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.<endoftext>\n",
              "2                                           <endoftext>If you ever wanted to know the *real* truth about the moon landings ...(best Onion article ever)  <endoftext>\n",
              "3                                                                <endoftext>Walked around a neighborhood recently rebuilt with help from APJ and others  <endoftext>\n",
              "4                                            <endoftext>It was Xmas, so we brought presents for the kids at the orphanage. They don't usually get much.  <endoftext>\n",
              "5                  <endoftext>Met with UNICEF, Doctors Without Borders and Artists for Peace & Justice. I support them and would recommend others do too.<endoftext>\n",
              "6                          <endoftext>Just returned from a trip to Haiti. Covered a lot of ground and saw many tough situations. They need a lot of help.<endoftext>\n",
              "7                                                                        <endoftext>Single character Tweets are the ulitmate extension of the Twitmeme...<endoftext>\n",
              "9                     <endoftext>The Russians are having some challenges with their rockets. Too many of the engineers that designed them have retired:  <endoftext>\n",
              "10         <endoftext>We had a long and interesting conversation on many subjects. He has exciting ideas for extending his creative talents beyond music.<endoftext>\n",
              "11                                                                                         <endoftext>Kanye stopped by the SpaceX rocket factory today.  <endoftext>\n",
              "12                                        <endoftext>Model S options are out! Performance in red and black for me.  I will deliver my car in June/July.  <endoftext>\n",
              "13                                        <endoftext>Hi, I'm Art Garfunkel. Have you heard the sound of silence? Because, you know, it makes a sound...  <endoftext>\n",
              "14                                   <endoftext>Raul Campos invited me to do a guest DJ gig on KCRW.  Hear my random holiday season music selections at  <endoftext>\n",
              "15                                                                                                  <endoftext>Yum! Even better than deep fried butter:  <endoftext>\n",
              "16         <endoftext>Yeah, this really is me, as my Mom  will attest. Not sure I can handle just doing 140 char missives. Will put longer thoughts on G+<endoftext>\n",
              "17    <endoftext>Got called randomly by Kanye West today and received a download of his thoughts, ranging from shoes to Moses. He was polite, but opaque.<endoftext>\n",
              "18                                                    <endoftext>His singing and acting talent will be sorely missed:    South Park sequel coming soon...<endoftext>\n",
              "19                                <endoftext>Why does the crowd cry over the glorious leader Kim Il Sung's death?  Fear of being shot may play a role:   <endoftext>\n",
              "20                                                  <endoftext>Sam Harris also wrote a nice piece on the awesomeness of Hitchens:   May the good man RIP.<endoftext>\n",
              "21                               <endoftext>Read \"Lying\", the new book by my friend Sam Harris.  Excellent cover art and lots of good reasons not to lie!<endoftext>\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2CsHkXf1Z8b",
        "outputId": "f5285473-99e9-4352-cf08-0df27b516ee6"
      },
      "source": [
        "len(musk_tweets)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33935"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyw9hWomAV1H",
        "outputId": "aaceb925-b23c-4a5e-ae31-6da12d531508"
      },
      "source": [
        "musk_tweets.dropna(inplace=True)\n",
        "len(musk_tweets)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33935"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60UIFTAY1N2l"
      },
      "source": [
        "## Training Splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T54RFVmS1NRv"
      },
      "source": [
        "train, val = train_test_split(musk_tweets, test_size=0.2)\n",
        "test, val = train_test_split(val, test_size=0.5)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oy37CKxfmK7",
        "outputId": "efa42c52-2def-4683-e6b6-3c578e8494df"
      },
      "source": [
        "print(\"Number of Train examples: \" + str(len(train)))\n",
        "print(\"Number of Val examples: \" + str(len(val)))\n",
        "print(\"Number of Test examples: \" + str(len(test)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Train examples: 27148\n",
            "Number of Val examples: 3394\n",
            "Number of Test examples: 3393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCoTxXg81quI"
      },
      "source": [
        "train_path = f'{directory}' + 'train.csv'\n",
        "val_path = f'{directory}' + 'val.csv'\n",
        "test_path = f'{directory}' + 'test.csv'\n",
        "\n",
        "train.to_csv(train_path, index=False)\n",
        "val.to_csv(val_path, index=False)\n",
        "test.to_csv(test_path, index=False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9ZvyvZyerDT"
      },
      "source": [
        "# Fine-Tuning GPT-2\n",
        "\n",
        "If we're looking to fine-tune models which are found on the HuggingFace model hub, then it becomes much easier to fine-tune the models since HuggingFace provides us with scripts.\n",
        "\n",
        "From the `transformers` repo:\n",
        "\n",
        "> There are two sets of scripts provided. The first set leverages the Trainer API. The second set with no_trainer in the suffix uses a custom training loop and leverages the 🤗 Accelerate library. Both sets use the 🤗 Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.\n",
        "\n",
        "You can learn more about it here: https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling\n",
        "\n",
        "We will be using the script that leveraged the Trainer API. We can download the script by running:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty4g9WUhfMz0"
      },
      "source": [
        "if os.path.exists('/gpt-2/run_clm.py'):\n",
        "    !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/run_clm.py -P gpt-2/"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXvm0wpQxS10"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOtpCx8KfVYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3aafb3-66da-43a0-d6fe-e10d60c076db"
      },
      "source": [
        "!python gpt-2/run_clm.py \\\n",
        "    --model_name_or_path gpt2 \\\n",
        "    --train_file data/elon-musk/tweets-2010-2021/train.csv \\\n",
        "    --validation_file data/elon-musk/tweets-2010-2021/val.csv \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --per_device_eval_batch_size=2 \\\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    --output_dir gpt-2/tmp/elon-test-clm \\\n",
        "    --overwrite_output_dir"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/27/2021 03:36:55 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/27/2021 03:36:55 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=gpt-2/tmp/elon-test-clm/runs/Nov27_03-36-55_66cb7a7de0f2,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=gpt-2/tmp/elon-test-clm,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=gpt-2/tmp/elon-test-clm,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "11/27/2021 03:36:55 - WARNING - datasets.builder - Using custom data configuration default-20640b625199f28f\n",
            "11/27/2021 03:36:55 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-20640b625199f28f/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-20640b625199f28f/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n",
            "100% 2/2 [00:00<00:00, 3540.99it/s]\n",
            "11/27/2021 03:36:55 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "11/27/2021 03:36:55 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "100% 2/2 [00:00<00:00, 124.84it/s]\n",
            "11/27/2021 03:36:55 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "11/27/2021 03:36:55 - INFO - datasets.builder - Generating split train\n",
            "11/27/2021 03:36:56 - INFO - datasets.builder - Generating split validation\n",
            "11/27/2021 03:36:56 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-20640b625199f28f/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 1064.00it/s]\n",
            "[INFO|file_utils.py:1773] 2021-11-27 03:36:57,522 >> https://huggingface.co/gpt2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpt68ke7e7\n",
            "Downloading: 100% 665/665 [00:00<00:00, 669kB/s]\n",
            "[INFO|file_utils.py:1777] 2021-11-27 03:36:58,267 >> storing https://huggingface.co/gpt2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|file_utils.py:1785] 2021-11-27 03:36:58,268 >> creating metadata file for /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:602] 2021-11-27 03:36:58,268 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:639] 2021-11-27 03:36:58,269 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:344] 2021-11-27 03:36:59,010 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:602] 2021-11-27 03:37:00,504 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:639] 2021-11-27 03:37:00,504 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1773] 2021-11-27 03:37:02,005 >> https://huggingface.co/gpt2/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp8ztkinqi\n",
            "Downloading: 100% 0.99M/0.99M [00:01<00:00, 960kB/s]\n",
            "[INFO|file_utils.py:1777] 2021-11-27 03:37:03,841 >> storing https://huggingface.co/gpt2/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1785] 2021-11-27 03:37:03,841 >> creating metadata file for /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1773] 2021-11-27 03:37:04,587 >> https://huggingface.co/gpt2/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkku5maoa\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 503kB/s]\n",
            "[INFO|file_utils.py:1777] 2021-11-27 03:37:06,244 >> storing https://huggingface.co/gpt2/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1785] 2021-11-27 03:37:06,244 >> creating metadata file for /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1773] 2021-11-27 03:37:06,991 >> https://huggingface.co/gpt2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpus0l5qgh\n",
            "Downloading: 100% 1.29M/1.29M [00:01<00:00, 1.25MB/s]\n",
            "[INFO|file_utils.py:1777] 2021-11-27 03:37:08,837 >> storing https://huggingface.co/gpt2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|file_utils.py:1785] 2021-11-27 03:37:08,837 >> creating metadata file for /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-27 03:37:11,069 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-27 03:37:11,069 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-27 03:37:11,069 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-27 03:37:11,069 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-27 03:37:11,069 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-27 03:37:11,069 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:602] 2021-11-27 03:37:12,558 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:639] 2021-11-27 03:37:12,559 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1773] 2021-11-27 03:37:13,433 >> https://huggingface.co/gpt2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0fb72otf\n",
            "Downloading: 100% 523M/523M [00:08<00:00, 63.7MB/s]\n",
            "[INFO|file_utils.py:1777] 2021-11-27 03:37:22,146 >> storing https://huggingface.co/gpt2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|file_utils.py:1785] 2021-11-27 03:37:22,146 >> creating metadata file for /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1352] 2021-11-27 03:37:22,146 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1619] 2021-11-27 03:37:24,593 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1628] 2021-11-27 03:37:24,593 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Running tokenizer on dataset:   0% 0/28 [00:00<?, ?ba/s]11/27/2021 03:37:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-20640b625199f28f/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-9b1f12b043585294.arrow\n",
            "Running tokenizer on dataset: 100% 28/28 [00:01<00:00, 27.99ba/s]\n",
            "Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]11/27/2021 03:37:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-20640b625199f28f/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-01fff0ecfda5b0e1.arrow\n",
            "Running tokenizer on dataset: 100% 4/4 [00:00<00:00, 38.49ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/28 [00:00<?, ?ba/s]11/27/2021 03:37:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-20640b625199f28f/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-190a82dd3d494c29.arrow\n",
            "Grouping texts in chunks of 1024: 100% 28/28 [00:00<00:00, 39.97ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/4 [00:00<?, ?ba/s]11/27/2021 03:37:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-20640b625199f28f/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-9c1b8df3bd9de424.arrow\n",
            "Grouping texts in chunks of 1024: 100% 4/4 [00:00<00:00, 45.98ba/s]\n",
            "[INFO|trainer.py:1196] 2021-11-27 03:37:37,285 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-11-27 03:37:37,285 >>   Num examples = 813\n",
            "[INFO|trainer.py:1198] 2021-11-27 03:37:37,285 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1199] 2021-11-27 03:37:37,285 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1200] 2021-11-27 03:37:37,285 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1201] 2021-11-27 03:37:37,285 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-11-27 03:37:37,285 >>   Total optimization steps = 1221\n",
            "{'loss': 2.8247, 'learning_rate': 2.952497952497953e-05, 'epoch': 1.23}\n",
            " 41% 500/1221 [03:22<04:52,  2.46it/s][INFO|trainer.py:2003] 2021-11-27 03:41:01,390 >> Saving model checkpoint to gpt-2/tmp/elon-test-clm/checkpoint-500\n",
            "[INFO|configuration_utils.py:423] 2021-11-27 03:41:01,395 >> Configuration saved in gpt-2/tmp/elon-test-clm/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1070] 2021-11-27 03:41:03,119 >> Model weights saved in gpt-2/tmp/elon-test-clm/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2043] 2021-11-27 03:41:03,124 >> tokenizer config file saved in gpt-2/tmp/elon-test-clm/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2049] 2021-11-27 03:41:04,340 >> Special tokens file saved in gpt-2/tmp/elon-test-clm/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 2.4351, 'learning_rate': 9.049959049959051e-06, 'epoch': 2.46}\n",
            " 82% 1000/1221 [06:52<01:29,  2.46it/s][INFO|trainer.py:2003] 2021-11-27 03:44:30,764 >> Saving model checkpoint to gpt-2/tmp/elon-test-clm/checkpoint-1000\n",
            "[INFO|configuration_utils.py:423] 2021-11-27 03:44:30,769 >> Configuration saved in gpt-2/tmp/elon-test-clm/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1070] 2021-11-27 03:44:32,529 >> Model weights saved in gpt-2/tmp/elon-test-clm/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2043] 2021-11-27 03:44:32,533 >> tokenizer config file saved in gpt-2/tmp/elon-test-clm/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2049] 2021-11-27 03:44:32,536 >> Special tokens file saved in gpt-2/tmp/elon-test-clm/checkpoint-1000/special_tokens_map.json\n",
            "100% 1221/1221 [08:28<00:00,  2.86it/s][INFO|trainer.py:1417] 2021-11-27 03:46:06,934 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 509.657, 'train_samples_per_second': 4.786, 'train_steps_per_second': 2.396, 'train_loss': 2.575369220205646, 'epoch': 3.0}\n",
            "100% 1221/1221 [08:28<00:00,  2.40it/s]\n",
            "[INFO|trainer.py:2003] 2021-11-27 03:46:06,946 >> Saving model checkpoint to gpt-2/tmp/elon-test-clm\n",
            "[INFO|configuration_utils.py:423] 2021-11-27 03:46:07,411 >> Configuration saved in gpt-2/tmp/elon-test-clm/config.json\n",
            "[INFO|modeling_utils.py:1070] 2021-11-27 03:46:18,366 >> Model weights saved in gpt-2/tmp/elon-test-clm/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2043] 2021-11-27 03:46:18,810 >> tokenizer config file saved in gpt-2/tmp/elon-test-clm/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2049] 2021-11-27 03:46:19,602 >> Special tokens file saved in gpt-2/tmp/elon-test-clm/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     2.5754\n",
            "  train_runtime            = 0:08:29.65\n",
            "  train_samples            =        813\n",
            "  train_samples_per_second =      4.786\n",
            "  train_steps_per_second   =      2.396\n",
            "11/27/2021 03:46:24 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2248] 2021-11-27 03:46:24,517 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2250] 2021-11-27 03:46:24,517 >>   Num examples = 102\n",
            "[INFO|trainer.py:2253] 2021-11-27 03:46:24,517 >>   Batch size = 2\n",
            "100% 51/51 [00:06<00:00,  7.32it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_loss               =     2.3391\n",
            "  eval_runtime            = 0:00:06.97\n",
            "  eval_samples            =        102\n",
            "  eval_samples_per_second =     14.614\n",
            "  eval_steps_per_second   =      7.307\n",
            "  perplexity              =     10.372\n",
            "[INFO|modelcard.py:449] 2021-11-27 03:46:32,934 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7q0Q066UWo-"
      },
      "source": [
        "# Let's use the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdIAGG1a27xX",
        "outputId": "70a47e0b-06be-412f-f460-aaec12ca2b56"
      },
      "source": [
        "OUTPUT_DIR = \"gpt-2/tmp/elon-test-clm\"\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(OUTPUT_DIR)\n",
        "model = GPT2LMHeadModel.from_pretrained(OUTPUT_DIR)\n",
        "model = model.to(device)\n",
        "                                        \n",
        "def generate(input_str, length=250, n=5):\n",
        "  cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i in range(length):\n",
        "      outputs = model(cur_ids[:, -1024:], labels=cur_ids[:, -1024:])\n",
        "      loss, logits = outputs[:2]\n",
        "      softmax_logits = torch.softmax(logits[0,-1], dim=0)\n",
        "      next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n)\n",
        "      cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim=1)\n",
        "    output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "    output_text = tokenizer.decode(output_list)\n",
        "    return output_text\n",
        "\n",
        "def choose_from_top(probs, n=5):\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id)\n",
        "\n",
        "generated_text = generate(\"Tesla\")\n",
        "print(generated_text)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla is working on a new design for Autopilot. Looks cool.<endoftext><endoftext>      _Gardi    _Gardi Yeah, it’s pretty much like the Tesla V6.1 or V7 will be the V6.0<endoftext><endoftext>    _Station    _Station Yeah, that’s pretty much how I see it<endoftext><endoftext>     Exactly, that’s why the Model 3 is better than Model S<endoftext><endoftext>   _Padival    _Padival  Exactly. Tesla is doing great in Mexico, but we need help in Europe. Europe needs a strong Tesla in order to make up for lost sales, but we need a strong Tesla in order to compete with Tesla China.<endoftext><endoftext>    Exactly, it’s a lot of work, but we’re getting close<endoftext><endoftext>   _Station We are. The Model 3 is the most fun car ever.<endoftext>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF05N8SmVIIr",
        "outputId": "ba51e5b8-3f22-4072-a9fa-631b98da8899"
      },
      "source": [
        "generated_text = generate(\"Tesla will be\")\n",
        "print(generated_text)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tesla will be able to deliver to our customers fast.’s been great. We will be building the best Tesla service out there.<endoftext><endoftext> _Gardi   Yes, this will be the first time we’ve done so in the car. It's not perfect, but we are building a product that will make you happy.<endoftext><endoftext>      We will be adding a few more weeks<endoftext><endoftext>     _Station _Gardi      _Crew     _Padival   Yes, will be a small, light-weight version of the Raptor rocket<endoftext><endoftext>Tesla Model X unveil is set for Sept 22 in Hawthorne, California  <endoftext><endoftext>     _Station  Yes. Will include dual motor AWD &amp; dual motor AWD supercharger.<endoftext><endoftext>_Ryan   _Station We will do that. Will be a little bigger, but we are building an advanced prototype that will allow us to test the system on\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kzOlqi8eMys"
      },
      "source": [
        "# Compressing the Model\n",
        "\n",
        "Let's save the model as a `tar.gz` file so that we can save it in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOmuQ4tUVei9"
      },
      "source": [
        "!tar -czf gpt-2-elon-tweets.tar.gz gpt-2/tuned-models/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
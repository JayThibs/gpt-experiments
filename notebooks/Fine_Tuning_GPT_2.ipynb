{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-Tuning-GPT-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMskAnJB3bOyswqOrNYQNkR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/gpt-experiments/blob/main/notebooks/Fine_Tuning_GPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lq-qHwDRba4"
      },
      "source": [
        "# Introduction to GPT\n",
        "\n",
        "GPT stands for \"Generative Pre-Trained Transformer.\"\n",
        "\n",
        "* Generative because it is used to generate text.\n",
        "* Pre-trained because it was trained on a large corpus of unstructured text to make its weights learn things from text like structure, syntax, and general knowledge.\n",
        "* Transformer because it uses the `decoder` part of the transformer architecture. In other words, you can give it text and it will decode what it needs to output in response (by guessing the next words that follow the input text).\n",
        "\n",
        "GPT-3 is a massive model. Much too massive to fit in a puny Google Colab GPU and its RAM. Therefore, here we'll use its predecessor, GPT-2, since we can actually fit in on our Colab machine.\n",
        "\n",
        "GPT models are particular cool because they are able to be applied to many downstream NLP tasks without having to fine-tune the model. Through, few-shot learning, the model can predict what should come next. However, the only current limitation is that the model is limited by its window size. In other words, a model like GPT-J can only fit 2048 tokens as input. That means that in some cases, we might not be able to fit in enough examples to get fantastic results. And when we're in a production environment, it can often be worth it to fine-tune a GPT model to your type of data so that it can perform better.\n",
        "\n",
        "Models like GPT-3 have been show to show a lot of great results across many different tasks without fine-tuning, even when we compare them to a model like BERT that was specifically fine-tuned on the data. However, it is still often recommended to fine-tune the model to get even better performance. And, in cases like medical data, GPT-3 doesn't necessarily perform well compared to a model like BioBERT.\n",
        "\n",
        "Perhaps a good rule of thumb is to start by doing creative prompt engineering with your GPT model first to try to get great results, and then you can decide afterwards if you'd like to fine-tune the model for even better accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap-rQ1P1Y058"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr14u8fcYR4y"
      },
      "source": [
        "!pip install transformers pytorch-lightning --quiet"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84TRkXXhY2U5"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvxQKSqQY3Fa"
      },
      "source": [
        "import re\n",
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import seed_everything\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2Tokenizer, TrainingArguments, Trainer, GPT2LMHeadModel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOF5P6jNZrMg",
        "outputId": "fb62127a-2bc5-4b9b-fb2a-79477db5448b"
      },
      "source": [
        "RANDOM_SEED = 3407\n",
        "pl.seed_everything(RANDOM_SEED)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 3407\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3407"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y1LixvDah-k"
      },
      "source": [
        "# Dataset class\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, txt_list, label_list, tokenizer, max_length):\n",
        "        # define variables    \n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "        self.labels = []\n",
        "        map_label = {0:'negative', 4: 'positive'}\n",
        "        # iterate through the dataset\n",
        "        for txt, label in zip(txt_list, label_list):\n",
        "            # prepare the text\n",
        "            prep_txt = f'<|startoftext|>Tweet: {txt}\\nSentiment: {map_label[label]}<|endoftext|>'\n",
        "            # tokenize\n",
        "            encodings_dict = tokenizer(prep_txt, truncation=True,\n",
        "                                       max_length=max_length, padding=\"max_length\")\n",
        "            # append to list\n",
        "            self.input_ids.append(torch.tensor(encodings_dict['input_ids']))\n",
        "            self.attn_masks.append(torch.tensor(encodings_dict['attention_mask']))\n",
        "            self.labels.append(map_label[label])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attn_masks[idx], self.labels[idx]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGt8D30Xaxgz"
      },
      "source": [
        "# Data load function\n",
        "def load_sentiment_dataset(tokenizer):\n",
        "    # load dataset and sample 10k reviews.\n",
        "    file_path = \"../input/sentiment140/training.1600000.processed.noemoticon.csv\"\n",
        "    df = pd.read_csv(file_path, encoding='ISO-8859-1', header=None)\n",
        "    df = df[[0, 5]]\n",
        "    df.columns = ['label', 'text']\n",
        "    df = df.sample(10000, random_state=1)\n",
        "\n",
        "    # divide into test and train\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "              train_test_split(df['text'].tolist(), df['label'].tolist(),\n",
        "              shuffle=True, test_size=0.05, random_state=1, stratify=df['label'])\n",
        "\n",
        "    # format into SentimentDataset class\n",
        "    train_dataset = SentimentDataset(X_train, y_train, tokenizer, max_length=512)\n",
        "\n",
        "    # return\n",
        "    return train_dataset, (X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuFiKRq-bBYD"
      },
      "source": [
        "## Load model and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcORTJj5bDcY"
      },
      "source": [
        "# set model name\n",
        "model_name = \"gpt2\"\n",
        "# seed\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# load tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, bos_token='<|startoftext|>',\n",
        "                                          eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
        "model = GPT2LMHeadModel .from_pretrained(model_name).cuda()\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# prepare and load dataset\n",
        "train_dataset, test_dataset = load_sentiment_dataset(tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eos9UkHobJq5"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQX5mIMibKx6"
      },
      "source": [
        "# creating training arguments\n",
        "training_args = TrainingArguments(output_dir='results', num_train_epochs=2, logging_steps=10,\n",
        "                                 load_best_model_at_end=True, save_strategy=\"epoch\", \n",
        "                                 per_device_train_batch_size=2, per_device_eval_batch_size=2,\n",
        "                                 warmup_steps=100, weight_decay=0.01, logging_dir='logs')\n",
        "\n",
        "# start training\n",
        "Trainer(model=model, args=training_args, train_dataset=train_dataset,\n",
        "        data_collator=lambda data: {'input_ids': torch.stack([f[0] for f in data]),\n",
        "                                    'attention_mask': torch.stack([f[1] for f in data]),\n",
        "                                    'labels': torch.stack([f[0] for f in data])}).train()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l03C7yL6bNTj"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d08ir0zSbN6s"
      },
      "source": [
        "# set the model to eval mode\n",
        "_ = model.eval()\n",
        "\n",
        "# run model inference on all test data\n",
        "original_label, predicted_label, original_text, predicted_text = [], [], [], []\n",
        "map_label = {0:'negative', 4: 'positive'}\n",
        "# iter over all of the test data\n",
        "for text, label in tqdm(zip(test_dataset[0], test_dataset[1])):\n",
        "    # create prompt (in compliance with the one used during training)\n",
        "    prompt = f'<|startoftext|>Tweet: {text}\\nSentiment:'\n",
        "    # generate tokens\n",
        "    generated = tokenizer(f\"{prompt}\", return_tensors=\"pt\").input_ids.cuda()\n",
        "    # perform prediction\n",
        "    sample_outputs = model.generate(generated, do_sample=False, top_k=50, max_length=512, top_p=0.90, \n",
        "            temperature=0, num_return_sequences=0)\n",
        "    # decode the predicted tokens into texts\n",
        "    predicted_text  = tokenizer.decode(sample_outputs[0], skip_special_tokens=True)\n",
        "    # extract the predicted sentiment\n",
        "    try:\n",
        "        pred_sentiment = re.findall(\"\\nSentiment: (.*)\", predicted_text)[-1]\n",
        "    except:\n",
        "        pred_sentiment = \"None\"\n",
        "    # append results\n",
        "    original_label.append(map_label[label])\n",
        "    predicted_label.append(pred_sentiment)\n",
        "    original_text.append(text)\n",
        "    predicted_text.append(pred_text)\n",
        "\n",
        "# transform result into dataframe\n",
        "df = pd.DataFrame({'original_text': original_text, 'predicted_label': predicted_label, \n",
        "                    'original_label': original_label, 'predicted_text': predicted_text})\n",
        "\n",
        "# predict the accuracy\n",
        "print(f1_score(original_label, predicted_label, average='macro'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-2-alignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e4e0f8536cdb43a7ba458953361f2791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12feec6ad5f74761aa17a05b7bdcb5b8",
              "IPY_MODEL_3d3e64448d8c405fbce417a234334e74",
              "IPY_MODEL_e70378dce2714379bb1a41e36731a854"
            ],
            "layout": "IPY_MODEL_a8d95718a8f64ad4b1dd06d20bfc04bd"
          }
        },
        "12feec6ad5f74761aa17a05b7bdcb5b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ad3fa0709f64c92a91589a03a595178",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_06e8f663d76f45f696174c1e3287b24f",
            "value": "Downloading: 100%"
          }
        },
        "3d3e64448d8c405fbce417a234334e74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce1218021bfd4116a55d2e05599ed061",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b26371d2a91422f88aed17f4a47afdd",
            "value": 1355256
          }
        },
        "e70378dce2714379bb1a41e36731a854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a05ad1a7cdd4755b720e42e0469602c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_765b1f4fb8a44a3b922638ac97ad4498",
            "value": " 1.29M/1.29M [00:01&lt;00:00, 982kB/s]"
          }
        },
        "a8d95718a8f64ad4b1dd06d20bfc04bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ad3fa0709f64c92a91589a03a595178": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06e8f663d76f45f696174c1e3287b24f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ce1218021bfd4116a55d2e05599ed061": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b26371d2a91422f88aed17f4a47afdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5a05ad1a7cdd4755b720e42e0469602c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "765b1f4fb8a44a3b922638ac97ad4498": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/gpt-experiments/blob/main/notebooks/gpt_2_alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lq-qHwDRba4"
      },
      "source": [
        "# Fine-Tuning GPT-2 on Alignment Texts Dataset\n",
        "\n",
        "This notebook is meant for initial experimentation of fine-tuning on the alignment text dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjSP3oGNHyJd",
        "outputId": "3d9b002e-ffb6-4a91-fc28-281b11b890ff"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jun 26 21:54:14 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   34C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap-rQ1P1Y058"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr14u8fcYR4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed319365-3660-45f3-d362-4dfb5d00675b"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers pytorch-lightning beautifulsoup4 datasets jsonlines --quiet"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84TRkXXhY2U5"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvxQKSqQY3Fa"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import seed_everything\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2Tokenizer, AutoTokenizer, TrainingArguments, Trainer, GPT2LMHeadModel\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5tY6KPsgUFQ"
      },
      "source": [
        "# Mounting Google Drive\n",
        "\n",
        "Here we will mount our Google Drive so that we can grab data and save the HuggingFace scripts, and save the model once we've fine-tuned it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNeXrm1qgWRp",
        "outputId": "d0d089ff-77fd-419a-ddd4-e41797312804"
      },
      "source": [
        "# For saving the data locally\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIMELLTfg76e",
        "outputId": "bb2bd5fd-77bc-401d-8859-e20b577d5ee9"
      },
      "source": [
        "%cd drive/MyDrive/data/ai-alignment-dataset/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data/ai-alignment-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-AVhqkVeluk"
      },
      "source": [
        "# Getting the Data\n",
        "\n",
        "We'll be fine-tuning GPT-2 on Elon Musk tweets to see if we can start taking the first steps towards an Elon AI."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e4e0f8536cdb43a7ba458953361f2791",
            "12feec6ad5f74761aa17a05b7bdcb5b8",
            "3d3e64448d8c405fbce417a234334e74",
            "e70378dce2714379bb1a41e36731a854",
            "a8d95718a8f64ad4b1dd06d20bfc04bd",
            "6ad3fa0709f64c92a91589a03a595178",
            "06e8f663d76f45f696174c1e3287b24f",
            "ce1218021bfd4116a55d2e05599ed061",
            "2b26371d2a91422f88aed17f4a47afdd",
            "5a05ad1a7cdd4755b720e42e0469602c",
            "765b1f4fb8a44a3b922638ac97ad4498"
          ]
        },
        "id": "eFp7LJUKJ4Uh",
        "outputId": "7490ad53-90d5-4248-a9ba-dc51467f8877"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4e0f8536cdb43a7ba458953361f2791"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i = 0 \n",
        "with jsonlines.open(\"alignment_texts.jsonl\") as reader:\n",
        "    for line in reader:\n",
        "        if i > 0:\n",
        "            break\n",
        "        text = line[\"text\"]\n",
        "        try:\n",
        "            if text != \"\":\n",
        "                print(len(text.split()))\n",
        "                print(text)\n",
        "                encoding = tokenizer(text)\n",
        "                total_len = len(encoding.tokens())\n",
        "                print(encoding.tokens)\n",
        "            i += 1\n",
        "        except:\n",
        "            pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYyFaGPBJTIm",
        "outputId": "85968063-a4dc-41ad-8457-1ee1aa0d0e62"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "306\n",
            "Webinar with Congressman Ro Khanna: Challenges in IT Law and Governance\n",
            "\n",
            "\n",
            " Download as PDF\n",
            "\n",
            "On Friday, February 19, the AI Pulse project hosted a web conversation on current issues in IT Governance with Congressman Ro Khanna, a leading progressive thinker on a wide range of law and technology issues in the United States Congress. Rep. Khanna represents Californiaâ€™s 17th district in the House of Representatives, where he chairs the Environment Subcommittee of the House Committee on Oversight and Reform, and serves as Deputy Whip of the Congressional Progressive Caucus. He is a passionate advocate of using technology to bring economic opportunity to rural and small-town America. In 2018, at the request of Speaker Pelosi, he authored a widely praised set of principles for an Internet Bill of Rights. Prior to serving in Congress, Rep. Khanna worked as an intellectual-property lawyer and served in the Obama Administration as Deputy Assistant Secretary of Commerce. He holds an undergraduate degree in Economics from the University of Chicago and a J.D. from Yale.\n",
            "Joining Rep. Khanna in conversation were Professors Eugene Volokh and Ted Parson of the UCLA School of Law. The conversation ranged over a wide set of law and technology issues, including market concentration and antitrust in the IT sector, first-amendment issues associated with online speech and content moderation, and online privacy regulation and related consumer online rights.\n",
            "The recording of the event is available here.\n",
            "We plan a continuing series of informal web conversations on various interesting and fun questions related to the societal impacts, governance, and ethics of AI/ML and related data and computational technologies. Stay tuned for these â€“ or to be added to our mailing list for announcements of future events, please send an email to aipulse@law.ucla.edu.The post Webinar with Congressman Ro Khanna: Challenges in IT Law and Governance first appeared on AI Pulse.\n",
            "<bound method BatchEncoding.tokens of {'input_ids': [1135, 8800, 283, 351, 30700, 5564, 5311, 7697, 25, 44495, 287, 7283, 3854, 290, 3948, 590, 628, 198, 10472, 355, 12960, 198, 198, 2202, 3217, 11, 3945, 678, 11, 262, 9552, 25062, 1628, 12007, 257, 3992, 5273, 319, 1459, 2428, 287, 7283, 3948, 590, 351, 30700, 5564, 5311, 7697, 11, 257, 3756, 10393, 45206, 319, 257, 3094, 2837, 286, 1099, 290, 3037, 2428, 287, 262, 1578, 1829, 3162, 13, 1432, 13, 5311, 7697, 6870, 3442, 447, 247, 82, 1596, 400, 4783, 287, 262, 2097, 286, 17132, 11, 810, 339, 18791, 262, 9344, 42187, 286, 262, 2097, 4606, 319, 35968, 290, 17893, 11, 290, 9179, 355, 15110, 40930, 286, 262, 15757, 25852, 31787, 13, 679, 318, 257, 15347, 12811, 286, 1262, 3037, 284, 2222, 3034, 3663, 284, 10016, 290, 1402, 12, 12735, 2253, 13, 554, 2864, 11, 379, 262, 2581, 286, 14931, 29611, 11, 339, 33941, 257, 6768, 15342, 900, 286, 7811, 329, 281, 4455, 3941, 286, 6923, 13, 14481, 284, 7351, 287, 3162, 11, 1432, 13, 5311, 7697, 3111, 355, 281, 9028, 12, 26745, 6853, 290, 4983, 287, 262, 2486, 8694, 355, 15110, 15286, 4986, 286, 16127, 13, 679, 6622, 281, 22952, 4922, 287, 18963, 422, 262, 2059, 286, 4842, 290, 257, 449, 13, 35, 13, 422, 19681, 13, 198, 9908, 3191, 1432, 13, 5311, 7697, 287, 5273, 547, 4415, 23295, 24532, 4709, 482, 71, 290, 11396, 350, 12613, 286, 262, 21750, 3961, 286, 3854, 13, 383, 5273, 17929, 625, 257, 3094, 900, 286, 1099, 290, 3037, 2428, 11, 1390, 1910, 10368, 290, 42766, 287, 262, 7283, 6567, 11, 717, 12, 321, 5904, 2428, 3917, 351, 2691, 4046, 290, 2695, 34401, 11, 290, 2691, 6782, 9001, 290, 3519, 7172, 2691, 2489, 13, 198, 464, 8296, 286, 262, 1785, 318, 1695, 994, 13, 198, 1135, 1410, 257, 8282, 2168, 286, 22176, 3992, 10275, 319, 2972, 3499, 290, 1257, 2683, 3519, 284, 262, 26877, 12751, 11, 18848, 11, 290, 14458, 286, 9552, 14, 5805, 290, 3519, 1366, 290, 31350, 8514, 13, 16160, 16524, 329, 777, 784, 393, 284, 307, 2087, 284, 674, 21898, 1351, 329, 24009, 286, 2003, 2995, 11, 3387, 3758, 281, 3053, 284, 257, 541, 9615, 31, 6270, 13, 36616, 64, 13, 15532, 13, 464, 1281, 5313, 22050, 351, 30700, 5564, 5311, 7697, 25, 44495, 287, 7283, 3854, 290, 3948, 590, 717, 4120, 319, 9552, 25062, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(encoding.tokens()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFngOlySNd5x",
        "outputId": "66afc1d9-1ab6-45a3-8794-9a54e05679ec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3h0UWoLep_O"
      },
      "source": [
        "directory = 'data/elon-musk/tweets-2010-2021/'\n",
        "musk_tweets = pd.read_csv(f'{directory}' + '2010.csv')\n",
        "\n",
        "list_of_years = ['2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']\n",
        "\n",
        "for year in list_of_years:\n",
        "    temp_df = pd.read_csv(f'{directory}' + year + '.csv')\n",
        "    musk_tweets = musk_tweets.append(temp_df, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "TyskPpaMrtgA",
        "outputId": "9bfae0b2-3d18-436f-e5ba-84898e6d1f92"
      },
      "source": [
        "musk_tweets.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>date</th>\n",
              "      <th>timezone</th>\n",
              "      <th>place</th>\n",
              "      <th>tweet</th>\n",
              "      <th>language</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>cashtags</th>\n",
              "      <th>user_id</th>\n",
              "      <th>user_id_str</th>\n",
              "      <th>username</th>\n",
              "      <th>name</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>link</th>\n",
              "      <th>urls</th>\n",
              "      <th>photos</th>\n",
              "      <th>video</th>\n",
              "      <th>thumbnail</th>\n",
              "      <th>retweet</th>\n",
              "      <th>nlikes</th>\n",
              "      <th>nreplies</th>\n",
              "      <th>nretweets</th>\n",
              "      <th>quote_url</th>\n",
              "      <th>search</th>\n",
              "      <th>near</th>\n",
              "      <th>geo</th>\n",
              "      <th>source</th>\n",
              "      <th>user_rt_id</th>\n",
              "      <th>user_rt</th>\n",
              "      <th>retweet_id</th>\n",
              "      <th>reply_to</th>\n",
              "      <th>retweet_date</th>\n",
              "      <th>translate</th>\n",
              "      <th>trans_src</th>\n",
              "      <th>trans_dest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>15434727182</td>\n",
              "      <td>15434727182</td>\n",
              "      <td>1.275676e+12</td>\n",
              "      <td>2010-06-04 18:31:57</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>44196397</td>\n",
              "      <td>44196397</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>Elon Musk</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>https://twitter.com/elonmusk/status/15434727182</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>4652</td>\n",
              "      <td>391</td>\n",
              "      <td>348</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>152153637639028736</td>\n",
              "      <td>152151847614943233</td>\n",
              "      <td>1.325111e+12</td>\n",
              "      <td>2011-12-28 22:27:08</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@TheOnion So true :)</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>44196397</td>\n",
              "      <td>44196397</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>Elon Musk</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>https://twitter.com/elonmusk/status/152153637639028736</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>151809315026636800</td>\n",
              "      <td>151809315026636800</td>\n",
              "      <td>1.325029e+12</td>\n",
              "      <td>2011-12-27 23:38:55</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>If you ever wanted to know the *real* truth about the moon landings ...(best Onion article ever)  http://t.co/pgNEJsjI</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>44196397</td>\n",
              "      <td>44196397</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>Elon Musk</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>https://twitter.com/elonmusk/status/151809315026636800</td>\n",
              "      <td>['http://j.mp/vLhhov']</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>39</td>\n",
              "      <td>13</td>\n",
              "      <td>34</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                  id  ...  trans_src  trans_dest\n",
              "0           0         15434727182  ...        NaN         NaN\n",
              "1           0  152153637639028736  ...        NaN         NaN\n",
              "2           1  151809315026636800  ...        NaN         NaN\n",
              "\n",
              "[3 rows x 39 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H79ADkFnsrcP",
        "outputId": "e216f4ff-e90b-4786-c4f2-452538391ce2"
      },
      "source": [
        "musk_tweets.rename(columns={'tweet': 'text'}, inplace=True)\n",
        "musk_tweets = musk_tweets['text']\n",
        "musk_tweets.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.\n",
              "1                                                                            @TheOnion So true :)\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7veNZgJPs0RL"
      },
      "source": [
        "musk_tweets.replace(to_replace=\"@[A-Za-z0-9]+\", value=\"\", regex=True, inplace=True)\n",
        "musk_tweets.replace(to_replace=r'http\\S+', value=\"\", regex=True, inplace=True)\n",
        "musk_tweets.replace(to_replace=r'#[A-Za-z0-9]+', value=\"\", regex=True, inplace=True)\n",
        "musk_tweets = musk_tweets[musk_tweets.str.len()>=20]\n",
        "# musk_tweets = \"<endoftext>\" + musk_tweets + \"<endoftext>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1osSu0xot9eY",
        "outputId": "e231087c-5172-4e4b-9e47-d19ecf4d0a7c"
      },
      "source": [
        "musk_tweets.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                 Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.\n",
              "2                                           If you ever wanted to know the *real* truth about the moon landings ...(best Onion article ever)  \n",
              "3                                                                Walked around a neighborhood recently rebuilt with help from APJ and others  \n",
              "4                                            It was Xmas, so we brought presents for the kids at the orphanage. They don't usually get much.  \n",
              "5                  Met with UNICEF, Doctors Without Borders and Artists for Peace & Justice. I support them and would recommend others do too.\n",
              "6                          Just returned from a trip to Haiti. Covered a lot of ground and saw many tough situations. They need a lot of help.\n",
              "7                                                                        Single character Tweets are the ulitmate extension of the Twitmeme...\n",
              "9                     The Russians are having some challenges with their rockets. Too many of the engineers that designed them have retired:  \n",
              "10         We had a long and interesting conversation on many subjects. He has exciting ideas for extending his creative talents beyond music.\n",
              "11                                                                                         Kanye stopped by the SpaceX rocket factory today.  \n",
              "12                                        Model S options are out! Performance in red and black for me.  I will deliver my car in June/July.  \n",
              "13                                        Hi, I'm Art Garfunkel. Have you heard the sound of silence? Because, you know, it makes a sound...  \n",
              "14                                   Raul Campos invited me to do a guest DJ gig on KCRW.  Hear my random holiday season music selections at  \n",
              "15                                                                                                  Yum! Even better than deep fried butter:  \n",
              "16         Yeah, this really is me, as my Mom  will attest. Not sure I can handle just doing 140 char missives. Will put longer thoughts on G+\n",
              "17    Got called randomly by Kanye West today and received a download of his thoughts, ranging from shoes to Moses. He was polite, but opaque.\n",
              "18                                                    His singing and acting talent will be sorely missed:    South Park sequel coming soon...\n",
              "19                                Why does the crowd cry over the glorious leader Kim Il Sung's death?  Fear of being shot may play a role:   \n",
              "20                                                  Sam Harris also wrote a nice piece on the awesomeness of Hitchens:   May the good man RIP.\n",
              "21                               Read \"Lying\", the new book by my friend Sam Harris.  Excellent cover art and lots of good reasons not to lie!\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2CsHkXf1Z8b",
        "outputId": "e5f4801e-99c5-456a-8bef-486e41924126"
      },
      "source": [
        "len(musk_tweets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33935"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyw9hWomAV1H",
        "outputId": "2b14cb5a-97e8-47ed-db39-7bb892705726"
      },
      "source": [
        "musk_tweets.dropna(inplace=True)\n",
        "len(musk_tweets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33935"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60UIFTAY1N2l"
      },
      "source": [
        "## Training Splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T54RFVmS1NRv"
      },
      "source": [
        "train, val = train_test_split(musk_tweets, test_size=0.2)\n",
        "test, val = train_test_split(val, test_size=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oy37CKxfmK7",
        "outputId": "efcf5d88-1c83-4c26-f5e9-2ae996e41874"
      },
      "source": [
        "print(\"Number of Train examples: \" + str(len(train)))\n",
        "print(\"Number of Val examples: \" + str(len(val)))\n",
        "print(\"Number of Test examples: \" + str(len(test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Train examples: 27148\n",
            "Number of Val examples: 3394\n",
            "Number of Test examples: 3393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCoTxXg81quI"
      },
      "source": [
        "train_path = f'{directory}' + 'train.csv'\n",
        "val_path = f'{directory}' + 'val.csv'\n",
        "test_path = f'{directory}' + 'test.csv'\n",
        "\n",
        "train.to_csv(train_path, index=False)\n",
        "val.to_csv(val_path, index=False)\n",
        "test.to_csv(test_path, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9ZvyvZyerDT"
      },
      "source": [
        "# Fine-Tuning GPT-2\n",
        "\n",
        "If we're looking to fine-tune models which are found on the HuggingFace model hub, then it becomes much easier to fine-tune the models since HuggingFace provides us with scripts.\n",
        "\n",
        "From the `transformers` repo:\n",
        "\n",
        "> There are two sets of scripts provided. The first set leverages the Trainer API. The second set with no_trainer in the suffix uses a custom training loop and leverages the ðŸ¤— Accelerate library. Both sets use the ðŸ¤— Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.\n",
        "\n",
        "You can learn more about it here: https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling\n",
        "\n",
        "We will be using the script that leveraged the Trainer API. We can download the script by running:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty4g9WUhfMz0"
      },
      "source": [
        "if os.path.exists('/gpt-2/run_clm.py'):\n",
        "    !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/run_clm.py -P gpt-2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXvm0wpQxS10"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOtpCx8KfVYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e1542db-53bb-4b8b-c10c-638490044244"
      },
      "source": [
        "!python gpt-2/run_clm.py \\\n",
        "    --model_name_or_path gpt2 \\\n",
        "    --train_file data/elon-musk/tweets-2010-2021/train.csv \\\n",
        "    --validation_file data/elon-musk/tweets-2010-2021/val.csv \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --per_device_eval_batch_size=2 \\\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    --output_dir gpt-2/tmp/elon-test-clm \\\n",
        "    --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/30/2021 00:14:04 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/30/2021 00:14:04 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=gpt-2/tmp/elon-test-clm/runs/Nov30_00-14-04_551d802533ba,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=gpt-2/tmp/elon-test-clm,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=gpt-2/tmp/elon-test-clm,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "11/30/2021 00:14:04 - WARNING - datasets.builder - Using custom data configuration default-c3208190fec074ad\n",
            "11/30/2021 00:14:04 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-c3208190fec074ad/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-c3208190fec074ad/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n",
            "100% 2/2 [00:00<00:00, 2439.97it/s]\n",
            "11/30/2021 00:14:04 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "11/30/2021 00:14:04 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "100% 2/2 [00:00<00:00, 69.03it/s]\n",
            "11/30/2021 00:14:04 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "11/30/2021 00:14:04 - INFO - datasets.builder - Generating split train\n",
            "11/30/2021 00:14:05 - INFO - datasets.builder - Generating split validation\n",
            "11/30/2021 00:14:05 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-c3208190fec074ad/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 902.49it/s]\n",
            "[INFO|file_utils.py:1773] 2021-11-30 00:14:05,345 >> https://huggingface.co/gpt2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpyu1fuisu\n",
            "Downloading: 100% 665/665 [00:00<00:00, 562kB/s]\n",
            "[INFO|file_utils.py:1777] 2021-11-30 00:14:05,444 >> storing https://huggingface.co/gpt2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|file_utils.py:1785] 2021-11-30 00:14:05,444 >> creating metadata file for /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:602] 2021-11-30 00:14:05,444 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:639] 2021-11-30 00:14:05,445 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:344] 2021-11-30 00:14:05,543 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:602] 2021-11-30 00:14:05,759 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:639] 2021-11-30 00:14:05,760 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1773] 2021-11-30 00:14:05,973 >> https://huggingface.co/gpt2/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4k3asp0i\n",
            "Downloading: 100% 0.99M/0.99M [00:00<00:00, 8.87MB/s]\n",
            "[INFO|file_utils.py:1777] 2021-11-30 00:14:06,199 >> storing https://huggingface.co/gpt2/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1785] 2021-11-30 00:14:06,199 >> creating metadata file for /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1773] 2021-11-30 00:14:06,304 >> https://huggingface.co/gpt2/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjr7y9sg7\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 4.58MB/s]\n",
            "[INFO|file_utils.py:1777] 2021-11-30 00:14:06,508 >> storing https://huggingface.co/gpt2/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1785] 2021-11-30 00:14:06,508 >> creating metadata file for /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1773] 2021-11-30 00:14:06,610 >> https://huggingface.co/gpt2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7p4j0ofa\n",
            "Downloading: 100% 1.29M/1.29M [00:00<00:00, 10.6MB/s]\n",
            "[INFO|file_utils.py:1777] 2021-11-30 00:14:06,850 >> storing https://huggingface.co/gpt2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|file_utils.py:1785] 2021-11-30 00:14:06,850 >> creating metadata file for /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-30 00:14:07,150 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-30 00:14:07,150 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-30 00:14:07,150 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-30 00:14:07,150 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-30 00:14:07,150 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-30 00:14:07,150 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:602] 2021-11-30 00:14:07,368 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:639] 2021-11-30 00:14:07,369 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1773] 2021-11-30 00:14:07,569 >> https://huggingface.co/gpt2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpnvpnwd0a\n",
            "Downloading: 100% 523M/523M [00:14<00:00, 38.4MB/s]\n",
            "[INFO|file_utils.py:1777] 2021-11-30 00:14:21,963 >> storing https://huggingface.co/gpt2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|file_utils.py:1785] 2021-11-30 00:14:21,963 >> creating metadata file for /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1352] 2021-11-30 00:14:21,963 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1619] 2021-11-30 00:14:24,113 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1628] 2021-11-30 00:14:24,113 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Running tokenizer on dataset:   0% 0/28 [00:00<?, ?ba/s]11/30/2021 00:14:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-c3208190fec074ad/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-fefea0224f9e1bd3.arrow\n",
            "Running tokenizer on dataset: 100% 28/28 [00:01<00:00, 22.59ba/s]\n",
            "Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]11/30/2021 00:14:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-c3208190fec074ad/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-14f394ef7a1bd2bf.arrow\n",
            "Running tokenizer on dataset: 100% 4/4 [00:00<00:00, 30.65ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/28 [00:00<?, ?ba/s]11/30/2021 00:14:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-c3208190fec074ad/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-0e98e27fef2ce3c8.arrow\n",
            "Grouping texts in chunks of 1024: 100% 28/28 [00:00<00:00, 42.12ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/4 [00:00<?, ?ba/s]11/30/2021 00:14:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-c3208190fec074ad/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-1127eebd5c59c92c.arrow\n",
            "Grouping texts in chunks of 1024: 100% 4/4 [00:00<00:00, 47.17ba/s]\n",
            "[INFO|trainer.py:1196] 2021-11-30 00:14:39,025 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-11-30 00:14:39,026 >>   Num examples = 564\n",
            "[INFO|trainer.py:1198] 2021-11-30 00:14:39,026 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1199] 2021-11-30 00:14:39,026 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1200] 2021-11-30 00:14:39,026 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1201] 2021-11-30 00:14:39,026 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-11-30 00:14:39,026 >>   Total optimization steps = 846\n",
            "{'loss': 3.9963, 'learning_rate': 2.0449172576832152e-05, 'epoch': 1.77}\n",
            " 59% 500/846 [03:26<02:23,  2.41it/s][INFO|trainer.py:2003] 2021-11-30 00:18:06,543 >> Saving model checkpoint to gpt-2/tmp/elon-test-clm/checkpoint-500\n",
            "[INFO|configuration_utils.py:423] 2021-11-30 00:18:06,720 >> Configuration saved in gpt-2/tmp/elon-test-clm/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1070] 2021-11-30 00:18:16,537 >> Model weights saved in gpt-2/tmp/elon-test-clm/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2043] 2021-11-30 00:18:17,175 >> tokenizer config file saved in gpt-2/tmp/elon-test-clm/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2049] 2021-11-30 00:18:17,368 >> Special tokens file saved in gpt-2/tmp/elon-test-clm/checkpoint-500/special_tokens_map.json\n",
            "100% 846/846 [06:16<00:00,  2.42it/s][INFO|trainer.py:1417] 2021-11-30 00:20:55,729 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 376.7065, 'train_samples_per_second': 4.492, 'train_steps_per_second': 2.246, 'train_loss': 3.816978652990174, 'epoch': 3.0}\n",
            "100% 846/846 [06:16<00:00,  2.25it/s]\n",
            "[INFO|trainer.py:2003] 2021-11-30 00:20:55,738 >> Saving model checkpoint to gpt-2/tmp/elon-test-clm\n",
            "[INFO|configuration_utils.py:423] 2021-11-30 00:20:55,873 >> Configuration saved in gpt-2/tmp/elon-test-clm/config.json\n",
            "[INFO|modeling_utils.py:1070] 2021-11-30 00:21:05,537 >> Model weights saved in gpt-2/tmp/elon-test-clm/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2043] 2021-11-30 00:21:05,780 >> tokenizer config file saved in gpt-2/tmp/elon-test-clm/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2049] 2021-11-30 00:21:05,935 >> Special tokens file saved in gpt-2/tmp/elon-test-clm/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =      3.817\n",
            "  train_runtime            = 0:06:16.70\n",
            "  train_samples            =        564\n",
            "  train_samples_per_second =      4.492\n",
            "  train_steps_per_second   =      2.246\n",
            "11/30/2021 00:21:07 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2248] 2021-11-30 00:21:07,787 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2250] 2021-11-30 00:21:07,788 >>   Num examples = 71\n",
            "[INFO|trainer.py:2253] 2021-11-30 00:21:07,788 >>   Batch size = 2\n",
            "100% 36/36 [00:04<00:00,  7.40it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_loss               =     3.5285\n",
            "  eval_runtime            = 0:00:04.87\n",
            "  eval_samples            =         71\n",
            "  eval_samples_per_second =     14.552\n",
            "  eval_steps_per_second   =      7.379\n",
            "  perplexity              =    34.0726\n",
            "[INFO|modelcard.py:449] 2021-11-30 00:21:13,072 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7q0Q066UWo-"
      },
      "source": [
        "# Let's use the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdIAGG1a27xX",
        "outputId": "f73c6a29-f0c9-4519-ccc5-f6057408b3dd"
      },
      "source": [
        "OUTPUT_DIR = \"gpt-2/tmp/elon-test-clm\"\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(OUTPUT_DIR)\n",
        "model = GPT2LMHeadModel.from_pretrained(OUTPUT_DIR)\n",
        "model = model.to(device)\n",
        "                                        \n",
        "def generate(input_str, length=250, n=5):\n",
        "  cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i in range(length):\n",
        "      outputs = model(cur_ids[:, -1024:], labels=cur_ids[:, -1024:])\n",
        "      loss, logits = outputs[:2]\n",
        "      softmax_logits = torch.softmax(logits[0,-1], dim=0)\n",
        "      next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n)\n",
        "      cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim=1)\n",
        "    output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "    output_text = tokenizer.decode(output_list)\n",
        "    return output_text\n",
        "\n",
        "def choose_from_top(probs, n=5):\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id)\n",
        "\n",
        "generated_text = generate(\"Just dropping some\")\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Just dropping some of my old stuff in the trunk. Itâ€™s been a while since I last used a car.      _AA_Carmack   Yeah, I think we should do something about it.  We have a long way to go. Will be interesting to see what happens to those who donâ€™t support this.   I think it will be great. It will be a lot more than a mere cameo. _AA_Carmack  Yes, itâ€™s a great game. I think we should do something about it.   _Station _Ryan _AA_Carmack       _Station _AA_Carmack  Iâ€™m not a big fan of the Tesla Autopilot software, but I do like the idea of having a car capable of recognizing pedestrians and cyclists. Itâ€™s a great idea, especially with the high speed autotracing. _Station I love you         _Padival         _Station  Yes, it will have a lot of new features coming to the Tesla Model S, including the ability to drive from the garage to the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF05N8SmVIIr",
        "outputId": "ae797da8-79c9-43ef-aa77-feca1aec9452"
      },
      "source": [
        "generated_text = generate(\"Just dropping some\")\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Just dropping some of our own resources into the ocean                   Yeah, we have to make sure we have a good product. That will be a priority.  We will make a new version of Falcon Heavy for free.  _AA_Carmack      _Sword _Sword _Sword    _AA_Carmack _Sword          Weâ€™ll try to get that done, but I think weâ€™ll be better off with a more advanced, reusable, reusable rocket booster.        Yeah, I love it :)       I love the idea of having a Tesla in the car. Itâ€™s awesome.  _Ryan    Yeah, that's what we should do   Yeah, we will make the Model S a lot faster, but we have a lot of work to do to get it right, as we did in the beginning. Weâ€™ve had a lot of setbacks._Gardi  _Ryan Yeah, that's exactly right _AA_Carmack Iâ€™m just trying to be as polite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kzOlqi8eMys"
      },
      "source": [
        "# Compressing the Model\n",
        "\n",
        "Let's save the model as a `tar.gz` file so that we can save it in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOmuQ4tUVei9"
      },
      "source": [
        "!tar -czf gpt-2-elon-tweets.tar.gz gpt-2/tuned-models/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-2-alignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c902a6114ac84ac3acae62bead10ac21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ab7dfcc053a43feb7e5a90e31a42a40",
              "IPY_MODEL_f415a9341b314d148a87eaed980f4bff",
              "IPY_MODEL_8a2fe27a515f48fe86bb266a1b7de2b1"
            ],
            "layout": "IPY_MODEL_6c3e285de3e640f9aaea14f9232f5e44"
          }
        },
        "8ab7dfcc053a43feb7e5a90e31a42a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f33e2f8414944e1a9788e4b4d4adae76",
            "placeholder": "​",
            "style": "IPY_MODEL_f2a9ad3c09b94ddf8f4c90caee21bd05",
            "value": "Downloading: 100%"
          }
        },
        "f415a9341b314d148a87eaed980f4bff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4eeda9df97584a9799c0be3f9f7f9a51",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd7b9f49a9164706ab004e9be422989a",
            "value": 665
          }
        },
        "8a2fe27a515f48fe86bb266a1b7de2b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e092e535b49a486ba847d1dbce145388",
            "placeholder": "​",
            "style": "IPY_MODEL_b22a6682fc784cb0b57d2ffdba5d8f1b",
            "value": " 665/665 [00:00&lt;00:00, 19.7kB/s]"
          }
        },
        "6c3e285de3e640f9aaea14f9232f5e44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f33e2f8414944e1a9788e4b4d4adae76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2a9ad3c09b94ddf8f4c90caee21bd05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4eeda9df97584a9799c0be3f9f7f9a51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd7b9f49a9164706ab004e9be422989a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e092e535b49a486ba847d1dbce145388": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b22a6682fc784cb0b57d2ffdba5d8f1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bed7c8232e984ac58f6031a64a57dd9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44b4f22fffe64e7aac49622077fb89bd",
              "IPY_MODEL_18730a0eaf6241ffa283886b9849f841",
              "IPY_MODEL_a806bec1b80d4149abb9d79dd36a6703"
            ],
            "layout": "IPY_MODEL_8908eb60dd704b5fbd57901dcc676673"
          }
        },
        "44b4f22fffe64e7aac49622077fb89bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d6c92c2c54844c9aad80b5403b3669b",
            "placeholder": "​",
            "style": "IPY_MODEL_2032b906962c4c7eb0761969a93fc7af",
            "value": "Downloading: 100%"
          }
        },
        "18730a0eaf6241ffa283886b9849f841": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d5be19442dc45bb903cc0ad9b4a05a0",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c21944427ad2472885c2298beec26ab3",
            "value": 1042301
          }
        },
        "a806bec1b80d4149abb9d79dd36a6703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ca6d4744b1d47f48445bcd447a57aa4",
            "placeholder": "​",
            "style": "IPY_MODEL_5e30058d65a74d04b2b8f1e599083c3a",
            "value": " 0.99M/0.99M [00:01&lt;00:00, 1.14MB/s]"
          }
        },
        "8908eb60dd704b5fbd57901dcc676673": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d6c92c2c54844c9aad80b5403b3669b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2032b906962c4c7eb0761969a93fc7af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d5be19442dc45bb903cc0ad9b4a05a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c21944427ad2472885c2298beec26ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ca6d4744b1d47f48445bcd447a57aa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e30058d65a74d04b2b8f1e599083c3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ad24663655045daa30618041e85b38f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50e450ab7d3a4159adf67c7ea3bb043f",
              "IPY_MODEL_b58333f6d71d42df9ea2864bb56f2aef",
              "IPY_MODEL_6bb0934e4cc54d89959a84f26c9c329a"
            ],
            "layout": "IPY_MODEL_6bc12e03aeb44fb9a0ec38508e70e8d6"
          }
        },
        "50e450ab7d3a4159adf67c7ea3bb043f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2300bce9606448939117dd4e6d791ec5",
            "placeholder": "​",
            "style": "IPY_MODEL_4ba4a9cf625942bfb6de23a9adae1bd2",
            "value": "Downloading: 100%"
          }
        },
        "b58333f6d71d42df9ea2864bb56f2aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1525a48fa76a4ce587152789a51115ff",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_afc2bf4e9bca44e0839644b71d3ffa05",
            "value": 456318
          }
        },
        "6bb0934e4cc54d89959a84f26c9c329a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5fbd6e58d4f47dca45ec3f97f8d8932",
            "placeholder": "​",
            "style": "IPY_MODEL_68eb8413730b40a288d7d1992ba7b5db",
            "value": " 446k/446k [00:00&lt;00:00, 629kB/s]"
          }
        },
        "6bc12e03aeb44fb9a0ec38508e70e8d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2300bce9606448939117dd4e6d791ec5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ba4a9cf625942bfb6de23a9adae1bd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1525a48fa76a4ce587152789a51115ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afc2bf4e9bca44e0839644b71d3ffa05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5fbd6e58d4f47dca45ec3f97f8d8932": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68eb8413730b40a288d7d1992ba7b5db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f231109b428b49108967d88006f5f938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88bed19337c749edb8fff6c44f4ab018",
              "IPY_MODEL_8dc91db2a69f4624bf0c3c852ebc6602",
              "IPY_MODEL_4400db59bb9b420686ab7bc28468cd30"
            ],
            "layout": "IPY_MODEL_1c2859dcaae24b6b8663040dd5573f9e"
          }
        },
        "88bed19337c749edb8fff6c44f4ab018": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42f2ab8b95d346b186305fad93c6d2c9",
            "placeholder": "​",
            "style": "IPY_MODEL_b030b8b7f0514ee8b457f34d1e93c8d8",
            "value": "Downloading: 100%"
          }
        },
        "8dc91db2a69f4624bf0c3c852ebc6602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7ea89d4d4864bb28ea1e5ae0a3ea030",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_768002bb30b544bea406630988bda6d2",
            "value": 1355256
          }
        },
        "4400db59bb9b420686ab7bc28468cd30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b5f16e4439b43ae85d88212e4781011",
            "placeholder": "​",
            "style": "IPY_MODEL_72b8121096454dc792644cbb34b46e7d",
            "value": " 1.29M/1.29M [00:00&lt;00:00, 1.21MB/s]"
          }
        },
        "1c2859dcaae24b6b8663040dd5573f9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42f2ab8b95d346b186305fad93c6d2c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b030b8b7f0514ee8b457f34d1e93c8d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7ea89d4d4864bb28ea1e5ae0a3ea030": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "768002bb30b544bea406630988bda6d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b5f16e4439b43ae85d88212e4781011": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72b8121096454dc792644cbb34b46e7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/gpt-experiments/blob/main/notebooks/gpt_2_alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lq-qHwDRba4"
      },
      "source": [
        "# Fine-Tuning GPT-2 on Alignment Texts Dataset\n",
        "\n",
        "This notebook is meant for initial experimentation of fine-tuning on the alignment text dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjSP3oGNHyJd",
        "outputId": "94ca5388-3933-4740-d6d7-fdc16cfc5740"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jun 27 16:33:47 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap-rQ1P1Y058"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr14u8fcYR4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36373238-e23f-4c03-f695-eafa01124195"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers pytorch-lightning beautifulsoup4 datasets jsonlines ftfy --quiet"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84TRkXXhY2U5"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvxQKSqQY3Fa"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import seed_everything\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2Tokenizer, AutoTokenizer, TrainingArguments, Trainer, GPT2LMHeadModel\n",
        "import ftfy\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5tY6KPsgUFQ"
      },
      "source": [
        "# Mounting Google Drive\n",
        "\n",
        "Here we will mount our Google Drive so that we can grab data and save the HuggingFace scripts, and save the model once we've fine-tuned it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNeXrm1qgWRp",
        "outputId": "b1d65697-c4cf-423f-a2bd-ce221cb53e02"
      },
      "source": [
        "# For saving the data locally\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIMELLTfg76e",
        "outputId": "51561081-9bf7-432d-a20d-26e58bbff1ef"
      },
      "source": [
        "%cd drive/MyDrive/data/ai-alignment-dataset/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data/ai-alignment-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-AVhqkVeluk"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clearning and Chunking Functions\n",
        "\n",
        "Functions for preparing the data into chunks that can fit into GPT."
      ],
      "metadata": {
        "id": "IWi2WcqKH_cF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def enforce_min_unique(seqs, min_unique_tokens, enc, verbose=False):\n",
        "    \"\"\"Sets a minimum about of unique tokens so that it skips chunks that just repeat the same characters.\"\"\"\n",
        "    for seq in tqdm(seqs, mininterval=1, smoothing=0, desc=\"enforce_min_unique_tokens\"):\n",
        "        if len(set(seq)) >= min_unique_tokens:\n",
        "            yield seq\n",
        "        elif verbose:\n",
        "            text = enc.decode(seq)\n",
        "            print(f\"excluding with {len(set(seq))} unique tokens:\\n\\n{repr(text)}\\n\\n\")\n",
        "\n",
        "\n",
        "def eot_splitting_generator(string_iterable, encoder):\n",
        "    \"\"\"\n",
        "    Given strings, splits them internally on <|endoftext|> and yields (generally more) strings\n",
        "    \"\"\"\n",
        "    for doc in string_iterable:\n",
        "        for d in doc.split(encoder.eos_token):\n",
        "            if len(d) > 0:\n",
        "                yield d\n",
        "\n",
        "def prep_and_tokenize_generator(string_iterable, encoder, normalize_with_ftfy, normalize_with_wikitext_detokenize):\n",
        "    \"\"\"\n",
        "    Given strings, does data cleaning / tokenization and yields arrays of tokens\n",
        "    \"\"\"\n",
        "    for doc in string_iterable:\n",
        "        doc = ftfy.fix_text(doc, normalization='NFKC')\n",
        "        tokens = encoder.encode(doc) + [encoder.eos_token_id]\n",
        "        yield tokens"
      ],
      "metadata": {
        "id": "gVrkckvvIduU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "veb5erAYIzYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_list(l, n):\n",
        "    # splits list/string into n size chunks\n",
        "    return [l[i:i + n] for i in range(0, len(l), n)]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "63IqNo6xEqgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "c902a6114ac84ac3acae62bead10ac21",
            "8ab7dfcc053a43feb7e5a90e31a42a40",
            "f415a9341b314d148a87eaed980f4bff",
            "8a2fe27a515f48fe86bb266a1b7de2b1",
            "6c3e285de3e640f9aaea14f9232f5e44",
            "f33e2f8414944e1a9788e4b4d4adae76",
            "f2a9ad3c09b94ddf8f4c90caee21bd05",
            "4eeda9df97584a9799c0be3f9f7f9a51",
            "cd7b9f49a9164706ab004e9be422989a",
            "e092e535b49a486ba847d1dbce145388",
            "b22a6682fc784cb0b57d2ffdba5d8f1b",
            "bed7c8232e984ac58f6031a64a57dd9c",
            "44b4f22fffe64e7aac49622077fb89bd",
            "18730a0eaf6241ffa283886b9849f841",
            "a806bec1b80d4149abb9d79dd36a6703",
            "8908eb60dd704b5fbd57901dcc676673",
            "2d6c92c2c54844c9aad80b5403b3669b",
            "2032b906962c4c7eb0761969a93fc7af",
            "0d5be19442dc45bb903cc0ad9b4a05a0",
            "c21944427ad2472885c2298beec26ab3",
            "7ca6d4744b1d47f48445bcd447a57aa4",
            "5e30058d65a74d04b2b8f1e599083c3a",
            "7ad24663655045daa30618041e85b38f",
            "50e450ab7d3a4159adf67c7ea3bb043f",
            "b58333f6d71d42df9ea2864bb56f2aef",
            "6bb0934e4cc54d89959a84f26c9c329a",
            "6bc12e03aeb44fb9a0ec38508e70e8d6",
            "2300bce9606448939117dd4e6d791ec5",
            "4ba4a9cf625942bfb6de23a9adae1bd2",
            "1525a48fa76a4ce587152789a51115ff",
            "afc2bf4e9bca44e0839644b71d3ffa05",
            "e5fbd6e58d4f47dca45ec3f97f8d8932",
            "68eb8413730b40a288d7d1992ba7b5db",
            "f231109b428b49108967d88006f5f938",
            "88bed19337c749edb8fff6c44f4ab018",
            "8dc91db2a69f4624bf0c3c852ebc6602",
            "4400db59bb9b420686ab7bc28468cd30",
            "1c2859dcaae24b6b8663040dd5573f9e",
            "42f2ab8b95d346b186305fad93c6d2c9",
            "b030b8b7f0514ee8b457f34d1e93c8d8",
            "e7ea89d4d4864bb28ea1e5ae0a3ea030",
            "768002bb30b544bea406630988bda6d2",
            "8b5f16e4439b43ae85d88212e4781011",
            "72b8121096454dc792644cbb34b46e7d"
          ]
        },
        "id": "eFp7LJUKJ4Uh",
        "outputId": "43551514-7aad-4343-89c4-f608944334a4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c902a6114ac84ac3acae62bead10ac21"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bed7c8232e984ac58f6031a64a57dd9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ad24663655045daa30618041e85b38f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f231109b428b49108967d88006f5f938"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with jsonlines.open(\"alignment_texts.jsonl\") as reader:\n",
        "    for line in reader:\n",
        "        text = line[\"text\"]\n",
        "        try:\n",
        "            if text != \"\":\n",
        "                print(len(text.split()))\n",
        "                print(text)\n",
        "                encoding = tokenizer(text)\n",
        "                total_len = len(encoding.tokens())\n",
        "                print(encoding.tokens)\n",
        "            if total_len > 1024:\n",
        "                break\n",
        "        except:\n",
        "            pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYyFaGPBJTIm",
        "outputId": "57163c36-411a-4d82-f4bb-6010dd48b889"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (25279 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "306\n",
            "Webinar with Congressman Ro Khanna: Challenges in IT Law and Governance\n",
            "\n",
            "\n",
            " Download as PDF\n",
            "\n",
            "On Friday, February 19, the AI Pulse project hosted a web conversation on current issues in IT Governance with Congressman Ro Khanna, a leading progressive thinker on a wide range of law and technology issues in the United States Congress. Rep. Khanna represents California’s 17th district in the House of Representatives, where he chairs the Environment Subcommittee of the House Committee on Oversight and Reform, and serves as Deputy Whip of the Congressional Progressive Caucus. He is a passionate advocate of using technology to bring economic opportunity to rural and small-town America. In 2018, at the request of Speaker Pelosi, he authored a widely praised set of principles for an Internet Bill of Rights. Prior to serving in Congress, Rep. Khanna worked as an intellectual-property lawyer and served in the Obama Administration as Deputy Assistant Secretary of Commerce. He holds an undergraduate degree in Economics from the University of Chicago and a J.D. from Yale.\n",
            "Joining Rep. Khanna in conversation were Professors Eugene Volokh and Ted Parson of the UCLA School of Law. The conversation ranged over a wide set of law and technology issues, including market concentration and antitrust in the IT sector, first-amendment issues associated with online speech and content moderation, and online privacy regulation and related consumer online rights.\n",
            "The recording of the event is available here.\n",
            "We plan a continuing series of informal web conversations on various interesting and fun questions related to the societal impacts, governance, and ethics of AI/ML and related data and computational technologies. Stay tuned for these – or to be added to our mailing list for announcements of future events, please send an email to aipulse@law.ucla.edu.The post Webinar with Congressman Ro Khanna: Challenges in IT Law and Governance first appeared on AI Pulse.\n",
            "<bound method BatchEncoding.tokens of {'input_ids': [1135, 8800, 283, 351, 30700, 5564, 5311, 7697, 25, 44495, 287, 7283, 3854, 290, 3948, 590, 628, 198, 10472, 355, 12960, 198, 198, 2202, 3217, 11, 3945, 678, 11, 262, 9552, 25062, 1628, 12007, 257, 3992, 5273, 319, 1459, 2428, 287, 7283, 3948, 590, 351, 30700, 5564, 5311, 7697, 11, 257, 3756, 10393, 45206, 319, 257, 3094, 2837, 286, 1099, 290, 3037, 2428, 287, 262, 1578, 1829, 3162, 13, 1432, 13, 5311, 7697, 6870, 3442, 447, 247, 82, 1596, 400, 4783, 287, 262, 2097, 286, 17132, 11, 810, 339, 18791, 262, 9344, 42187, 286, 262, 2097, 4606, 319, 35968, 290, 17893, 11, 290, 9179, 355, 15110, 40930, 286, 262, 15757, 25852, 31787, 13, 679, 318, 257, 15347, 12811, 286, 1262, 3037, 284, 2222, 3034, 3663, 284, 10016, 290, 1402, 12, 12735, 2253, 13, 554, 2864, 11, 379, 262, 2581, 286, 14931, 29611, 11, 339, 33941, 257, 6768, 15342, 900, 286, 7811, 329, 281, 4455, 3941, 286, 6923, 13, 14481, 284, 7351, 287, 3162, 11, 1432, 13, 5311, 7697, 3111, 355, 281, 9028, 12, 26745, 6853, 290, 4983, 287, 262, 2486, 8694, 355, 15110, 15286, 4986, 286, 16127, 13, 679, 6622, 281, 22952, 4922, 287, 18963, 422, 262, 2059, 286, 4842, 290, 257, 449, 13, 35, 13, 422, 19681, 13, 198, 9908, 3191, 1432, 13, 5311, 7697, 287, 5273, 547, 4415, 23295, 24532, 4709, 482, 71, 290, 11396, 350, 12613, 286, 262, 21750, 3961, 286, 3854, 13, 383, 5273, 17929, 625, 257, 3094, 900, 286, 1099, 290, 3037, 2428, 11, 1390, 1910, 10368, 290, 42766, 287, 262, 7283, 6567, 11, 717, 12, 321, 5904, 2428, 3917, 351, 2691, 4046, 290, 2695, 34401, 11, 290, 2691, 6782, 9001, 290, 3519, 7172, 2691, 2489, 13, 198, 464, 8296, 286, 262, 1785, 318, 1695, 994, 13, 198, 1135, 1410, 257, 8282, 2168, 286, 22176, 3992, 10275, 319, 2972, 3499, 290, 1257, 2683, 3519, 284, 262, 26877, 12751, 11, 18848, 11, 290, 14458, 286, 9552, 14, 5805, 290, 3519, 1366, 290, 31350, 8514, 13, 16160, 16524, 329, 777, 784, 393, 284, 307, 2087, 284, 674, 21898, 1351, 329, 24009, 286, 2003, 2995, 11, 3387, 3758, 281, 3053, 284, 257, 541, 9615, 31, 6270, 13, 36616, 64, 13, 15532, 13, 464, 1281, 5313, 22050, 351, 30700, 5564, 5311, 7697, 25, 44495, 287, 7283, 3854, 290, 3948, 590, 717, 4120, 319, 9552, 25062, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}>\n",
            "332\n",
            "Webinar: The Filter Bubble\n",
            "\n",
            "\n",
            " Download as PDF\n",
            "\n",
            "On Friday November 6, the AI Pulse project hosted a web conversation on The Filter Bubble – or maybe it’s The Echo Chamber: the notion that we are living in social worlds that are increasingly narrow, tightly connected, and homogeneous – in the people we interact with, the ideas and values we engage, even what we believe, and the facts we experience about the world. In various accounts, this might be caused by the mere fact of interacting online, or by specific design and algorithmic choices of the social media platforms we interact with. And crucially, this might occur without our active choice, or without or even being aware that it is happening. We considered a few big questions about the Filter Bubble. First, is it a real thin, and is it meaningfully different from familiar processes of social interaction? To the extent it is real and different, what is causing it? What effects is it having, for our social, economic, and political lives – AND if it’s doing bad things, what can be done about it?\n",
            "The conversation was kicked off by Jane Bambauer, Professor of Law at the University of Arizona. Joining the conversation with Jane were David Brin, astrophysicist and celebrated author of science fiction and non-fiction futurist speculation; Mark Lemley, Professor of Law at Stanford University and director of Stanford’s Program in Law, Science and Technology; Eugene Volokh, Professor of Law at UCLA; and Ted Parson, Professor of Law at UCLA and Director of the AI Pulse Project.\n",
            "The recording of the event is available here.\n",
            "We plan a continuing series of informal web conversations on various interesting and fun questions related to the societal impacts, governance, and ethics of AI/ML and related data and computational technologies. Stay tuned for these – or to be added to our mailing list for announcements of future events, please send an email to pulse@law.ucla.edu.The post Webinar: The Filter Bubble first appeared on AI Pulse.\n",
            "<bound method BatchEncoding.tokens of {'input_ids': [1135, 8800, 283, 25, 383, 25853, 33691, 628, 198, 10472, 355, 12960, 198, 198, 2202, 3217, 3389, 718, 11, 262, 9552, 25062, 1628, 12007, 257, 3992, 5273, 319, 383, 25853, 33691, 784, 393, 3863, 340, 447, 247, 82, 383, 21455, 15840, 25, 262, 9495, 326, 356, 389, 2877, 287, 1919, 11621, 326, 389, 6481, 7135, 11, 17707, 5884, 11, 290, 3488, 32269, 784, 287, 262, 661, 356, 9427, 351, 11, 262, 4213, 290, 3815, 356, 8209, 11, 772, 644, 356, 1975, 11, 290, 262, 6419, 356, 1998, 546, 262, 995, 13, 554, 2972, 5504, 11, 428, 1244, 307, 4073, 416, 262, 5019, 1109, 286, 24986, 2691, 11, 393, 416, 2176, 1486, 290, 8385, 9383, 7747, 286, 262, 1919, 2056, 9554, 356, 9427, 351, 13, 843, 4630, 2131, 11, 428, 1244, 3051, 1231, 674, 4075, 3572, 11, 393, 1231, 393, 772, 852, 3910, 326, 340, 318, 5836, 13, 775, 3177, 257, 1178, 1263, 2683, 546, 262, 25853, 33691, 13, 3274, 11, 318, 340, 257, 1103, 7888, 11, 290, 318, 340, 3616, 2759, 1180, 422, 5385, 7767, 286, 1919, 10375, 30, 1675, 262, 6287, 340, 318, 1103, 290, 1180, 11, 644, 318, 6666, 340, 30, 1867, 3048, 318, 340, 1719, 11, 329, 674, 1919, 11, 3034, 11, 290, 1964, 3160, 784, 5357, 611, 340, 447, 247, 82, 1804, 2089, 1243, 11, 644, 460, 307, 1760, 546, 340, 30, 198, 464, 5273, 373, 12165, 572, 416, 12091, 347, 4131, 16261, 11, 8129, 286, 3854, 379, 262, 2059, 286, 7943, 13, 5302, 3191, 262, 5273, 351, 12091, 547, 3271, 1709, 259, 11, 48782, 893, 48187, 290, 13943, 1772, 286, 3783, 10165, 290, 1729, 12, 24046, 13294, 333, 396, 13367, 26, 2940, 20607, 1636, 11, 8129, 286, 3854, 379, 13863, 2059, 290, 3437, 286, 13863, 447, 247, 82, 6118, 287, 3854, 11, 5800, 290, 8987, 26, 24532, 4709, 482, 71, 11, 8129, 286, 3854, 379, 21750, 26, 290, 11396, 350, 12613, 11, 8129, 286, 3854, 379, 21750, 290, 5890, 286, 262, 9552, 25062, 4935, 13, 198, 464, 8296, 286, 262, 1785, 318, 1695, 994, 13, 198, 1135, 1410, 257, 8282, 2168, 286, 22176, 3992, 10275, 319, 2972, 3499, 290, 1257, 2683, 3519, 284, 262, 26877, 12751, 11, 18848, 11, 290, 14458, 286, 9552, 14, 5805, 290, 3519, 1366, 290, 31350, 8514, 13, 16160, 16524, 329, 777, 784, 393, 284, 307, 2087, 284, 674, 21898, 1351, 329, 24009, 286, 2003, 2995, 11, 3387, 3758, 281, 3053, 284, 19445, 31, 6270, 13, 36616, 64, 13, 15532, 13, 464, 1281, 5313, 22050, 25, 383, 25853, 33691, 717, 4120, 319, 9552, 25062, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}>\n",
            "189\n",
            "Webinar: Alternative Payment and Business Models for the Internet\n",
            "\n",
            "\n",
            " Download as PDF\n",
            "\n",
            "The AI Pulse project hosted a web conversation on Friday, September 18, on the topic of alternative payment and business models for the Internet. Is the now-dominant advertising model fundamentally flawed or doomed? And if so, what alternative approaches are most promising? In particular, does the model of small (mini, micro, nano) payments for information content have promise, despite the widespread criticism it has attracted and the failure of several early attempts to implement it?\n",
            "The conversation was kicked off by David Brin, astrophysicist and celebrated author of science fiction and non-fiction futurist speculation. Joining the conversation with David were Jane Bambauer, Professor of Law at the University of Arizona; Mark Lemley, Professor of Law at Stanford University and director of Stanford’s Program in Law, Science and Technology; and Eugene Volokh, Professor of Law at UCLA. UCLA Law Professor and AI Pulse Project Director Ted Parson moderated.\n",
            "The recording of the event is available here.\n",
            "Stay tuned for continuations of the conversation.The post Webinar: Alternative Payment and Business Models for the Internet first appeared on AI Pulse.\n",
            "<bound method BatchEncoding.tokens of {'input_ids': [1135, 8800, 283, 25, 27182, 28784, 290, 7320, 32329, 329, 262, 4455, 628, 198, 10472, 355, 12960, 198, 198, 464, 9552, 25062, 1628, 12007, 257, 3992, 5273, 319, 3217, 11, 2693, 1248, 11, 319, 262, 7243, 286, 5559, 6074, 290, 1597, 4981, 329, 262, 4455, 13, 1148, 262, 783, 12, 3438, 42483, 8560, 2746, 17640, 19556, 393, 24312, 30, 843, 611, 523, 11, 644, 5559, 10581, 389, 749, 11781, 30, 554, 1948, 11, 857, 262, 2746, 286, 1402, 357, 45313, 11, 4580, 11, 38706, 8, 7524, 329, 1321, 2695, 423, 6991, 11, 3805, 262, 10095, 7734, 340, 468, 12725, 290, 262, 5287, 286, 1811, 1903, 6370, 284, 3494, 340, 30, 198, 464, 5273, 373, 12165, 572, 416, 3271, 1709, 259, 11, 48782, 893, 48187, 290, 13943, 1772, 286, 3783, 10165, 290, 1729, 12, 24046, 13294, 333, 396, 13367, 13, 5302, 3191, 262, 5273, 351, 3271, 547, 12091, 347, 4131, 16261, 11, 8129, 286, 3854, 379, 262, 2059, 286, 7943, 26, 2940, 20607, 1636, 11, 8129, 286, 3854, 379, 13863, 2059, 290, 3437, 286, 13863, 447, 247, 82, 6118, 287, 3854, 11, 5800, 290, 8987, 26, 290, 24532, 4709, 482, 71, 11, 8129, 286, 3854, 379, 21750, 13, 21750, 3854, 8129, 290, 9552, 25062, 4935, 5890, 11396, 350, 12613, 7019, 515, 13, 198, 464, 8296, 286, 262, 1785, 318, 1695, 994, 13, 198, 25681, 16524, 329, 1261, 6055, 286, 262, 5273, 13, 464, 1281, 5313, 22050, 25, 27182, 28784, 290, 7320, 32329, 329, 262, 4455, 717, 4120, 319, 9552, 25062, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}>\n",
            "20389\n",
            "Max – A Thought Experiment: Could AI Run the Economy Better Than Markets?\n",
            "\n",
            "\n",
            " Download as PDF\n",
            "\n",
            "Edward A. (Ted) Parson1\n",
            "Abstract\n",
            "One of the fundamental critiques against twentieth century experiments in central economic planning, and the main reason for their failures, was the inability of human-directed planning systems to manage the data gathering, analysis, computation, and control necessary to direct the vast complexity of production, allocation, and exchange decisions that make up a modern economy. Rapid recent advances in AI, data, and related technological capabilities have re-opened that old question, and provoked vigorous speculation about the feasibility, benefits, and threats of an AI-directed economy. This paper presents a thought experiment about how this might work, based on assuming a powerful AI agent (whimsically named “Max”) with no binding computational or algorithmic limits on its (his) ability to do the task. The paper’s novel contribution is to make this hitherto under-specified question more concrete and specific. It reasons concretely through how such a system might work under explicit assumptions about contextual conditions; what benefits it might offer relative to present market and mixed-market arrangements; what novel requirements or constraints it would present; what threats and challenges it would pose, and how it inflects long-standing understandings of foundational questions about state, society, and human liberty.\n",
            "As with smaller-scale regulatory interventions, the concrete implementation of comprehensive central planning can be abstracted as intervening via controlling either quantities or prices. The paper argues that quantity-based approaches would be fundamentally impaired by problems of principal-agent relations and incentives, which hobbled historical planning systems and would persist under arbitrary computational advances. Price-based approaches, as proposed by Oskar Lange, do not necessarily suffer from the same disabilities. More promising than either, however, would be a variant in which Max manages a comprehensive system of price modifications added to emergent market outcomes, equivalent to a comprehensive economy-wide system of Pigovian taxes and subsidies. Such a system, “Pigovian Max,” could in principle realize the information efficiency benefits and liberty interests of decentralized market outcomes, while also comprehensively correcting externalities and controlling inefficient concentration of market power and associated rent-seeking behavior. It could also, under certain additional assumptions, offer the prospect of taxation without deadweight loss, by taking all taxes from inframarginal rents.\n",
            "Having outlined the basic approach and these potential benefits, the paper discusses several challenges and potential risks presented by such a system. These include Max’s need for data and the potential costs of providing it; the granularity or aggregation of Max’s determinations; the problem of maintaining variety and innovation in an economy directed by Max; the implications of Max for the welfare of human workers, the meaning and extent of property rights, and associated liberty interests; the definition of social welfare that determines Max’s objective function, its compatibility with democratic control, and the resultant stability of the boundary between the state and the economy; and finally, the relationship of Max to AI-enabled trends already underway, with implications for the feasibility of Max being developed and adopted, and the associated risks. In view of the depth and difficulty of these questions, the discussion of each is necessarily preliminary and speculative.\n",
            "Introduction\n",
            "Artificial Intelligence: Advances, Impacts, and Governance Concerns\n",
            "Artificial intelligence (AI)—particularly various methods of machine learning (ML)—have made landmark advances in the past few years in applications as diverse as playing complex games, purchase recommendations, language processing, speech recognition and synthesis, image identification, and facial recognition. These advances have brought a surge of popular, journalistic, and policy attention to the field, including both excitement about anticipated benefits and concern about societal impacts and risks. Risks could arise through some combination of accidental, malicious or reckless use, as well as through the expected social and political disruption from the speed and scale of changes.\n",
            "Potential impacts of AI range from the immediate and particular to the vast and transformative. While most current scholarly and policy commentary on AI impacts addresses near-term advances and concerns, popular accounts are dominated by vivid scenarios of existential threats to human survival or autonomy, often inspired by fictional accounts in which AI has advanced to general super-intelligence, independent volition, or some other landmark of capabilities equivalent to exceeding those of humans. Expert opinions about the likelihood and timing of such extreme advances vary widely.2 Yet it is also increasingly clear that such extreme advances in capability are not necessary for AI to have transformative societal impacts—for good or ill, or more likely for both—including the prospect of severe disruptions.\n",
            "Efforts to manage societal impacts of technology always face deep uncertainties, both about trends in technical capabilities and about how they will be used in social context. These perennial challenges are even greater for AI than for other recent areas of technological concern, due to its diffuse, labile character, strong linkages with multiple areas of technological advance, and breadth and diversity of potential application areas.3 In its foundational and potentially transformative character, AI has been credibly compared to the drivers of previous industrial revolutions, electricity and fossil fuels.4\n",
            "In view of these challenges, analysis and criticism of AI’s social impacts and its governance have tended to cluster at two endpoints in terms of the immediacy and scale of the concerns they consider. Most current work targets present or immediately anticipated applications, such as autonomous vehicles and algorithmic decision-support systems in criminal justice, health-care, employment, and education, addressing already present concerns about safety, liability, privacy, bias, and due process.5 A bolder minority of current work goes to the opposite extreme, aiming to characterize the implications of some future endpoint of capability—super-intelligent AI, or artificial general intelligence (AGI), for example—with attendant risks to human survival or autonomy. This latter work includes efforts to identify and develop technical characteristics that would make AI robustly safe, benign, or “friendly” for humans, no matter how powerful it becomes: in effect, seeking practical (and contradiction-free) analogues to Asimov’s Three Laws of Robotics.6\n",
            "The broad range that lies between these two clusters, however—the impacts, risks, and governance challenges of AI that are intermediate in time-scale and magnitude between the immediate and the existential—also carries the potential for transformative societal impacts and disruptions, for good and ill. Yet despite admitting some degree of informed and disciplined speculation, this intermediate range has received less attention.7 This intermediate range of AI applications and impacts is unavoidably somewhat diffuse in its boundaries, but can be coherently distinguished, at least conceptually, from both the ultimate and the immediate. The distinction from ultimate, singularity-related concerns is relatively simple: in this mid-range, AI applications are still under human control.8\n",
            "The distinction of mid-range from immediate concerns is subtler, yet can be meaningfully drawn in terms of scope of control. In current and projected near-term uses, AI applications advise, augment, or replace existing actors (a person, role, or organization) in existing decisions. They are embedded in products and services marketed by existing firms to identified customers. They support or replace human expertise in decisions now taken by individual humans, or by larger groups or organizations (corporations, courts, boards, etc.) that are recognized and held accountable like individuals. But this correspondence between AI applications and pre-existing actors and decisions is historically contingent, and need not persist as AI capabilities expand. In the medium term, AI could be deployed to do things that somewhat resemble present actors’ decisions, but at such expanded scale or scope that their impacts are qualitatively changed, by, for example, expanding actors’ power, transforming their relationships, or enabling new goals. Alternatively, AI could be deployed to do things not now done by any single actor, but by larger-scale social processes or networks, such as markets, normative systems, diffuse non-localized institutions, or the international system. We can envision future AI systems comprehensively integrating—and presumably aiming to optimize—all decisions made by and within large complex organizations. For example, we might envision AI “running” UCLA, the UK National Health Service, the State of California, or as I explore in this paper, the entire economy. Deployed at such scales, AI would take outcomes that are now viewed as emergent properties, equilibria, or other phenomena beyond the reach of any individual decision or centralized control, and subject them to unified control, intentionality, and (possibly) explication and accountability. Assessment and governance of AI impacts in this intermediate range would, more clearly than for either immediate or singularity-related concerns, require consideration of both the technical characteristics of AI systems and the social, economic, and political context in which they are developed and used.\n",
            "A Thought Experiment: AI-Powered Central Economic Planning\n",
            "To explore these possibilities, this paper develops a thought experiment that sits squarely in this middle range: Could AI run the economy, replacing decentralized decisions by market actors? Could some plausible extrapolation of rapidly advancing AI and data capabilities perform the resource allocation and coordination functions of markets—the functions that twentieth century central planning systems attempted and so notably failed at—and do it better than either past planning systems or markets?\n",
            "Although this exercise is speculative, there are at least three reasons that it is worthwhile, both as an intellectual exploration with deep historical relevance and surprising current saliency and for its practical implications. First, it provides a vivid illustration of the potentially transformative impact of AI capabilities that sit in this middle range, not requiring general or super-intelligent AI systems. Indeed, far from being implausibly audacious, its ambition is comparable to many other expansive projections, for good or ill, of potentially transformative AI applications.9 Second, it offers new perspectives on deep, enduring questions of social, political, and legal theory, such as the definition of social welfare, the relationship between economic and personal liberty, civil pluralism, the relationship between the market economy and the state, and the boundaries between individual liberties and state or other collective authority. The inquiry informs sharp current political controversies, as rapid progress in AI shifts the ground under seemingly settled questions such as the distribution of economic surplus between labor and capital, the impacts of economic concentration, and the distribution of power in society.10 Third, this is a potential AI application whose moral valence is not obvious a priori but rather ambiguous and contingent, not clearly pointing to either Utopian or Dystopian extremes but potentially capable of turning in either direction. It thus provides rich ground for inquiry into its consequences and the conditions that would tilt toward either societal benefits or harms, of specific forms or in aggregate, and hence may suggest guidance for near-term policy and legal responses.\n",
            "Before getting into details, I briefly address the issue of what name to give the AI who wields this great power. I propose “Max.” Among its other virtues, “Max” is helpfully gender-ambiguous —but it being 2019, Max also needs pronouns. Here, I look back before recent portrayals of uber-powerful AIs as female (for example, Her, Ex Machina), to two landmarks from a prior period of social upheaval: Kubrick and Clarke’s HAL 2000 and even further back, to Roy Orbison.11 Many of us will be working for Max, if we are working at all, so Max is clearly “The Man”—and gets masculine pronouns.\n",
            "Max will have two big advantages over markets in promoting human welfare, both consequences of the fact that his pursuit of human welfare would be intentional and explicit, rather than indirect and emergent. Rather than performing a set of parallel, decentralized, private optimizations from which one must invoke “invisible hand” logic to assert good aggregate outcomes, Max would perform a global social optimization. This would enable him to correct market failures. This means, first, that Max can internalize all externalities, incorporating both market and non-market information to identify and assess external effects and respond appropriately—if not for all, then at least for the most serious and uncontested externalities, such as environmental harms, resource depletion, over-use of commons, and the under-compensated social benefits of health, education, knowledge, the arts, and civic institutions. Max could correct the pricing of fossil fuels, agricultural products, and water, and the salaries of teachers, nurses, and social workers.\n",
            "Second, Max could reduce or eliminate market power and the associated rent-seeking behavior. Unlike human-managed firms, Max would not waste effort trying to create socially sub-optimal market power, or to shift rents or costs under conditions of existing, widespread market power – except insofar as these shifts somehow bring aggregate benefits. These advantages distinguish Max both from pure market arrangements and from historical attempts at central planning, which had their hands more than full simply trying to manage production and get markets to clear. My focus on these advantages also distinguishes Max from other proposals for central planning based on computational advances, which have invoked broad social aims such as equality, sustainability, and democratic participation but have not worked through the practicalities of how the proposed systems would improve on market outcomes in advancing these aims.12\n",
            "The paper proceeds as follows. Part I provides a brief historical background on the question of central planning, the main arguments for and against it, and the reasons that coming advances in AI and related technologies may transform the issue. Section II elaborates the task of “running the economy,” asking what it might mean concretely and what background assumptions must be specified to make sense of it, then proposes three alternative models of how Max might operate. Section III then gives a preliminary sketch of several issues and challenges raised by Max, including Max’s data needs, implications for social diversity and innovation, the problem of defining Max’s objective function, and the dynamics of how Max might come about, as well as what to do about them.\n",
            "This inquiry presents the clear risk of sprawling over a vast landscape and thus ending up both speculative and superficial. To bound the inquiry and help limit this risk, and to distinguish this from an exercise in technological forecasting, I rely on several explicit simplifying assumptions. The first and most important of these is an assumption of computational capability. For any computational task relevant to the scale of the problem, “running the economy”— millions to billions of people, and a similar or somewhat larger order of potential goods, inputs, and production and distribution decisions13 —Max can do it. There is no binding constraint in computational capacity, bandwidth, or algorithmic ability to optimize a well-specified objective function: these are assumed to be in unlimited, effectively free supply. This assumption, adopted for heuristic purposes, also distinguishes this exercise from the many efforts to characterize the computational complexity of the economy relative to presented or projected computing power, either to demonstrate or reject the feasibility of control.14 I simply assume the necessary capacity, require only that the assumption pass some minimal threshold of plausibility, then work through its implications. No such simplifying assumption can be made, however, for the data Max needs to do his job, which is central to the inquiry and cannot be similarly hand-waved away. Relative to other computation-related resources, generation and distribution of relevant data is more difficult, more contingent on social and economic conditions, more dependent on Max’s precise job description, and interacts more strongly with other, non-economic values that are (at least in its initial specification) outside Max’s job description. Needed data, and the constraints and implications of getting it, are among the issues discussed in Section III. The paper closes with brief conclusions and questions for further investigation.\n",
            "I. Historical Context: The Socialist Calculation Debate\n",
            "In the twentieth-century intellectual struggle between the centrally planned, ostensibly socialist states and the liberal capitalist democracies, two basic arguments were advanced against socialism. The first was based on liberty and related normative claims about the proper scope of state authority relative to citizens, most sharply focused on the relationship between property rights and civil and political rights. The state cannot control the means of production without impermissible encroachment on the liberties of citizens. This critique is normative and foundational, independent of the state of technology or other contingent material conditions.15 The second argument was based on competency—the ability of state planning systems to efficiently produce the goods and services that people want. Critics of central planning argued that no matter how capable the officials running the system or the resources at their disposal, central planning could not match the performance of decentralized decisions in markets, but would be perennially afflicted with shortages, misallocations, and wasteful surpluses. Unlike the first critique, this one is contingent on specific conditions and capabilities. Even if it was true for all real efforts at central economic planning—as it almost always was—you can imagine alternative conditions under which it might not be true. My focus here is on this second argument.\n",
            "Although it has earlier roots, this argument grew prominent in the early twentieth century following the Russian revolution. The most prominent anti-planning statements were by Von Mises (1922), responding to a planning system advocated and partly implemented in early post-war Bavaria by Otto Neurath (1919).16 Hayek (1945) later sharpened and extended Von Mises’s critique,17 while the most prominent rebuttal was by Oskar Lange. Von Mises and Hayek both argued, in different ways, that the equilibrium conditions necessary for competitive markets to clear and achieve their claimed social benefits could not be achieved by central planning because the information needed to do so is only available encoded in the prices that emerge from decentralized market interactions in competitive equilibrium (or more imperfectly through rougher competitive interactions, even absent perfect competitive equilibrium).\n",
            "Against Von Mises’s initial statement of this thesis, Lange showed that there is no barrier in principle to the same optimality conditions produced by competitive interactions being attained by central direction, guided by a set of shadow prices playing a role parallel to that of market prices. Lange even proposed a practical process of incremental, trial-and-error adjustment by which planners could find market-clearing prices, analogous to the private-market adjustment process proposed by Walras.18\n",
            "Hayek then sharpened the critique, arguing that even if planners could in theory replicate markets’ socially optimal allocation, the scale of the required data and computation made the task impossible in practice—particularly considering the vast, fine-grained diversity of conditions under which people transact (Day-old muffins, half price!), and the dynamism of market conditions with resultant need for rapid adjustments. Lange’s response, published posthumously in 1967, merely stated that advances in computing rendered the problem feasible, even easy.19\n",
            "Although the early rounds of this “socialist calculation” debate occurred before the development of modern computers, rapid advances in computation and in optimization algorithms—first using analog devices that built on wartime advances in cybernetic control, then with digital devices after the mid-1950s—repeatedly changed the context for subsequent rounds, albeit more in theory than in practice. The conflict between opposing conclusory assertions—Hayek’s assertion of impossibility, Lange’s of possibility—was unresolvable, as it depended upon contending speculations about future developments in technological capability. And while rapid continuing advances in both computers and algorithms since the 1950s stimulated periodic suggestions that the terms of the debate had fundamentally changed,20 there was no concrete evidence that a major threshold of capability had been crossed. Indeed, the planning problem is sufficiently under-specified that it is not clear precisely what level or type of computing resources would count as the relevant threshold. Meanwhile, the concrete economic and strategic victory of the liberal democracies over the Soviet bloc, and the obvious failure of actual attempts at central planning,21 made the question seem uninteresting.\n",
            "The debate thus sat unresolved—and arguably unresolvable —for decades. Lange’s was the strongest argument for socialist planning, but his shift to directing prices rather than quantities, and his leaving final goods and labor markets outside his planning system, left his proposal an odd, under-specified hybrid. His proposal was criticized both from the left for not being socialist enough and failing to guarantee social equality and democratic participation,22 and from the right for assuming perfect, unified firm response to planners’ directives and for failing to account for the incentives of managers and entrepreneurs.23 Depending on implementation details that Lange did not specify, either critique—or both—may have been valid. Moreover, the arguments over computational feasibility between Lange and critics such as Hayek and Lavoie turned on competing unverifiable assumptions about future technical progress and its social context,24 which were not subject to empirical resolution.\n",
            "Three far-reaching recent changes in conditions, however, make it a useful time to seriously revisit the question. First, advances in AI and machine learning, in parallel with rapid expansion in hardware-based computational capacity. Second, the explosion in volume, ubiquity, and usability of data, particularly the widespread and powerful use of proxy data as skilled predictors for things that cannot be observed directly: for example, consumer preferences, attitudes and dispositions, and receptivity to political messages. And third, the growth of sub-systems of the economy—mainly within large integrated firms and cross-firm networks—that operate by central direction under algorithmic control, rather than human decisions responding to market conditions.25 These represent large islands of planning that aim to optimize private, rather than social, objective functions. Under these trends, there has been some revival of the planning debate, although with an unfortunate tendency to re-contest old questions without specific connections to recent progress. Although the most expansive exploration of these issues has been in speculative fiction,26 there is also active debate on the left about the feasibility and desirability of revived central planning based on modern computing.27\n",
            "II. How Would MAX Work?\n",
            "A. Mechanics of Max: Background Assumptions\n",
            "How much does Max control? What does “run the economy” mean? Let’s assume Max won’t be supplanting human agency, telling everyone what to do all the time: that does not seem aligned with the goal of advancing human welfare. Then over what actual decisions is he given authority? We begin to approach this question by taking Max’s job description seriously: Max “runs the economy,” a description that presumes the economy is not all of society, but is distinguished both from the state, and from some extensive set of non-economic social interactions and arrangements. Let’s stipulate that the economy is the set of processes, institutions, and practices that control how goods and services are produced, exchanged, and consumed.28\n",
            "As I sharpen the thought experiment to make Max more concrete and specific, at several points in the argument additional assumptions will be needed, either about the definition and boundaries of Max’s job or about the social and political context in which Max operates. My aims in making these assumptions—to keep the exercise interesting and potentially relevant for near-term decisions—will suggest a few points of heuristic guidance in what assumptions are most useful. First, having already assumed no computational constraints I will try not to sneak in additional assumptions about Max’s capability that shatter the (admittedly loose) bounds of plausibility I am trying to maintain. Second, since the purpose of Max is to advance human welfare, in specifying how Max works I will avoid choices that run strongly against evident human preferences and values—with the two caveats, of course, that preferences and values may change, and that future political conditions may favor deploying actual AI-based planning systems in ways that do not enhance human welfare. Finally, this thought experiment is intended to serve as a scenario exercise—a description and analysis of uncertain future conditions whose purpose is to inform near-term choices.29 At some points, this purpose tends to favor assuming less profound societal transformations, in order to maintain relevance and continuity with near-term decisions and research priorities. Throughout, I endeavor to make these assumptions explicit, and to note where other choices might be similarly plausible. For the most part, I choose just one path through the dense tree of possibilities, with brief observations on potential alternative paths but mostly leaving these to further development in future work.\n",
            "The first of these required assumptions concerns the scope of Max’s authority: in particular what authority he would have over consumption. Would Max tell people what to eat, wear, do, where to go for dinner or vacation? I assume that he does not, but rather that people still make their own consumption decisions. I make this choice partly as a generalization from my own preferences. I don’t like being told what to consume, both out of an intrinsic preference for autonomy and because others who try often get my preferences wrong. This is also partly a moral choice—the overlap of consumption choices with basic liberty interests is too strong to give up, and I worry that letting people give up this autonomy, even if sometimes convenient, may be incompatible with human flourishing.30 And it is partly about Max’s information needs—consumer choice provides continually updated information about preferences, which Max needs and may only be able to get by observing freely exercised choices. Rather than specifying consumption, Max will do what the economy already does—determine the options available to me, with contextual conditions of time and place—and provide relevant information and suggestions.31\n",
            "A second needed simplifying assumption concerns scarcity versus abundance. To keep the thought experiment relevant to current decisions and distinct from Utopian fiction—this is not Iain Banks’s Culture32 —I assume that technical progress has not eliminated scarcity. So while consumption is not specified or compelled, neither does it operate as “it’s all free, take whatever you want.”33 Consumption choices remain constrained, and any constraint on total consumption that does not dictate specific choices will resemble a familiar budget constraint. This implies that even with Max running the economy, absent conditions of post-scarcity plenty there must still be money. I have a finite amount of it, although we have not yet considered how I get it. And things have prices—or at least, final consumer goods have prices. We haven’t yet considered input factors or intermediate goods.\n",
            "This condition of continuing scarcity distinguishes the thought experiment here from the most expansive technological-communist reflections, which broadly assume technology (omnipresent data, 3-D printing) will generate conditions of limitless abundance, under which marginal costs—and hence prices—converge toward zero.34 In contrast to these visions, I assume that production still requires material inputs, many of which will be in constrained supply even with optimized production technology, perhaps increasingly tightly constrained, if Max’s deployment comes before human civilization expands beyond the limits of the Earth. Perhaps the most decisive constraint on limitless abundance, however, comes from social limits to growth.35 To the extent that many things people desire remain ordinal or positional—markers of relative social status that are intrinsically constrained—even perfectly optimized production technology will not overcome scarcity: the goalposts will simply move. With many things people want still in limited supply, due to any combination of material, environmental, and social-structure constraints, the economy will still need an allocation mechanism to determine who gets what. Although it may take different forms, this will look to consumers like prices and a budget constraint.\n",
            "With Max’s authority limited to production, another assumption is needed immediately: Do people still work? To pull the exercise toward relevance for near-term decisions, I assume that Max, other AI systems, and robots have not replaced all human productive activity. People still work, including instrumental or productive work (working to make things other people want) as well as intrinsically motivated work independent of any demand for the output. This might be because AI and robots cannot satisfactorily do every job and people are still needed,36 or because people want to work. The number of people working may be far fewer than today but is not a tiny number. Enough people are working that allocating and managing them, and their motivation and welfare, must be considered in how the economy runs.\n",
            "With Max running the economy and people still working, the next assumption needed is the nature of the boundary and interactions between Max and human workers; in particular, are there still firms? In theory, it is possible to have an economy without firms.37 Every human worker could be a sole proprietor, interacting with others through contractual market transactions.38 Firms are artifacts of information, principal-agent relations, and economies of scale, which make it more efficient to gather workers and resources inside organizations with internal operations controlled by collegial, normative, and (mostly) authority relationships rather than market transactions.\n",
            "For the three assumptions discussed thus far, only one option appears to keep the imagined world potentially desirable and the thought experiment relevant and bounded. Max controls production, not consumption; there is still scarcity and thus a need for some way to allocate output among people; and people still work. On whether firms still exist, however, and the related question of how human workers interact with Max, at least two cases appear plausible. First, we can assume there are still firms, within which managers contract with human employees and exercise authority over their work. Firms may employ AI or robots alongside human workers, but human managers run the show internally. Under this assumption, Max’s authority operates only in the external environment of the firm. Alternatively, we can assume that firms are gone. Every human worker is then accountable directly to Max, rather than to human managers. Workers may still sit together in shared offices, collaborate with each other, and hang out by the coffee machine, but their work is directed by Max via a set of contractual arrangements.\n",
            "Intermediate cases are possible, although they probably don’t all require separate consideration. For example, the economy might be mixed. Some firms still operate, in parallel with a large economy of individual contractors working directly for Max. One intermediate case that might require separate consideration would be if some firms are managed by non-Max AI’s. For this case to be distinct, firm-manager AIs must not be fully integrated into Max, but rather are separate decision-makers in an agency relationship with Max. Max’s ability to see inside the firm must be limited, and interests must not be perfectly aligned. The firm AIs may have private interests in their firm’s enrichment or status, perhaps making their own workers happy or satisfying their shareholders (if they still have them), or they may disagree with Max on the aggregate social welfare function. Bargaining between Max and the firm would be AI-to-AI, and so on more equal footing than Max’s interactions with human managers. And of course, workers’ experience within the firm would be different; they would be under the authority of their firm’s AI manager, rather than either human managers or Max.\n",
            "On this point, I begin by assuming that firms do still exist, managed by either humans or AIs. Max’s main area of operation thus lies outside the boundary of the firm, in dealings among firms and between firms and consumers.\n",
            "B. How Would MAX Work II: Quantities or Prices and Applied to What?\n",
            "What does Max actually do? The simplest possibility is that Max operates just like an old-fashioned central planner, specifying input and output quantities to every firm. I call this variant “Quantity Max”. Max provides your allocation of all inputs—your capital, workers, and material inputs. They will arrive on your loading dock, on the following schedule. If you have a problem with the inputs delivered, you are free to take it up with the supplier, but you’d probably rather deal directly with Max, who has an excellent record of resolving disputes rapidly and fairly.39 And here is your output quota: how much of each product, with delivery timing and locations specified. With Max’s unlimited computational capability, the inputs and outputs all match up perfectly (subject to stochastic optimization, to the extent there are still equipment breakdowns, snowstorms, or other uncertainties outside Max’s control).\n",
            "The most basic challenge for this arrangement concerns the incentives of firm managers. Do managers have discretion in how they run things inside their firms? Presumably they do, and presumably they are not pure altruists. We thus expect them to use their discretion to advance their own interests, not to act as perfectly faithful agents for Max’s social welfare function. And to the extent they do not have discretion, why have people doing these jobs and why would anyone want them?40 Max may get the flows of inputs and outputs among firms perfectly. But just controlling quantities (plus whatever structure of contracts Max gives managers in case of variation from these) leaves a serious agency problem. Managers can use their discretion to advance their divergent interests, through various forms of rent-seeking, cutting quality, skimming off inputs, abusing their workers, and creating negative externalities—anything that is within their scope of authority and concealable from Max. Moreover, the problem is not solved by having Max specify more precisely what the firm does, including technology choice and other internal decisions. As long as there are—by need or choice—firms managed by humans with discretion, and private information to make the discretion meaningful, there will be agency problems of this sort. These can be reduced by more tightly specifying firm behavior, at the cost of whatever values motivated having human managers; they can be reduced to individual-level agency problems if there are no firms and every human worker reports directly to Max; and they are changed in character if firms are managed by AI’s separate from Max. But all these reductions carry costs and tradeoffs, and none fully eliminates agency problems.\n",
            "The cause of this problem is obvious; like old-time central planning, this system has no prices. Oddly, we had to assume prices at the point of final consumer sale to have meaningful consumer budget constraints. But under Quantity Max, all input and production decisions up to that point are made by diktat. For Max to tack on prices at final retail sale, without tracking and using them through the production process up to that point, fails to take advantage of available, high-value information and communication devices. Socialist planners were hostile to prices for ideological reasons, but Max doesn’t have to be. Max is not an ideologue,41 he’s an instrumentalist and an empiricist. He’s looking for ways to advance aggregate human welfare and willing to adopt new approaches in pursuit of that end.\n",
            "We thus consider a second variant of Max, “Price Max”. Instead of specifying quantities, Price Max specifies prices of all goods in commerce, including all firm inputs and outputs. Although Price Max is still imposing different transaction conditions than parties would adopt based on private interests alone—and thus requires effective suppression of black markets to enforce his exclusive authority—the change from specifying quantities to prices reproduces several major features of markets. Firms are free to organize their operations as they choose, subject to the given prices they face. Managers can use this discretion to increase profits, which remain within the firm. The things managers do within the market system to increase profits—for example, shopping around for more suitable or lower-priced inputs,42 tuning and improving production processes, motivating workers, improving and differentiating their outputs to command a higher price—remain feasible, potentially effective at increasing profits, and socially desirable. The change from setting quantities to setting prices reduces many—not all—of the agency problems present under Quantity Max, assuming firms can retain a large enough fraction of their earnings to be motivating.43 Max setting prices instead of quantities also mitigates liberty concerns related to Max’s direction of labor markets. Max setting wages, perhaps also running a clearinghouse to suggest matches of people to jobs, better preserves the voluntary nature of work decisions that, like consumption decisions, are too strongly linked to individual liberty to consider compelled assignments.\n",
            "Our assumptions about Max’s optimizing ability imply that Max gets all prices right—all markets clear, with no shortages or surpluses. But for Price Max to set these prices, he must either independently calculate or observe the same data as is revealed or generated in market interactions: the abundance and characteristics of resources, their alternative uses, the production technologies available to transform them, and consumer preferences. If he cannot garner exactly the same data, he must identify good enough proxies to closely approach the same competitive equilibrium solutions. Although I have assumed no effective constraints on Max’s computational ability, similarly expansive assumptions about Max’s access to all needed data are more suspect. Data is the weakest and most troublesome link in the chain of capabilities this thought experiment requires. Max might be able to independently calculate these competitive equilibrium prices. But to the extent the data needed to reproduce these are not available, are costly, or cause harms or violate valued principles in their acquisition—or, for that matter, to the extent there are other social values beyond information-generation attributed to market processes of search, bargaining, and contracting—we might prefer not to have Max re-estimate these market-clearing prices. Instead, Max could use the prices that emerge from independent production and consumption decisions, transactional offers and requests (bids and asks), in competitive interactions—in effect, let Max free-ride on market processes to generate price information.\n",
            "Great: We’ve come this far, and the best Max can do amounts to reproducing market prices—like the character in the Borges story who independently “wrote” Don Quixote?44 In one sense, we have simply reproduced Hayek’s argument about the information economy of decentralized market decisions. But we’re not done. Market prices provide high-value information, but only as a starting point for Max’s job. Max is charged with improving on market outcomes when these diverge from social optimality. The prices Max calculates to achieve this will often be equal or very close to those emerging from market exchange, but not always; and the differences are important. To illustrate this most clearly, it is helpful to consider yet a third variant of Max.\n",
            "This form of Max would use market interactions to generate initial prices that serve as the starting point for every transaction, but would then impose price adjustments on each transaction as needed to correct market failures. Insofar as many of the market imperfections Max must correct can be understood as externalities (both negative and positive), we have now re-defined Max’s job as administering a complete system of Pigovian taxes and subsidies,45 so I call this variant “Pigovian Max.” Pigovian Max would evaluate all externalities and other market imperfections (not just as single points, but as they vary over some relevant range of output), announce taxes or subsidies, then manage whatever adjustment process is needed to ensure that markets still clear.\n",
            "How would Pigovian Max be implemented? At the level of individual transactions, Pigovian Max might look quite unobtrusive and familiar. Sellers could post fixed prices or buyers and sellers could negotiate, as they do under market systems, up to the point of transaction. Max would then calculate and add the appropriate tax or subsidy at the point of sale. The process would be similar to the imposition of a sales tax, but with two differences. First, the adjustments would vary over transactions, so buyers and sellers would need to be informed of the adjustment before they commit to each transaction, presumably via mobile devices, information on sales displays, or point-of-sale systems. Second, adjustments could be of either sign, and could be large for goods with large externalities.\n",
            "At larger scale, how disruptive Pigovian Max would be will depend on details of implementation, and on uncertainties about the size of the adjustments that require analysis beyond my scope here. Max might be relatively unobtrusive, to the extent that relatively few goods carry most of the external effects that need correction—for example negative externalities from fossil fuels, water extractions,46 heavy metals, toxic chemicals, agricultural fertilizer and chemical inputs; and positive externalities from provision and dissemination of knowledge, physical and mental health, social services, etc. The system could be implemented at various points in supply chains, depending on how external effects are distributed across these. Implementing it like a Value-Added Tax (VAT),47 with Max’s adjustment based on incremental external costs or benefits at each stage from primary inputs to final consumer goods, would be a plausible approach. For goods carrying the largest negative externalities—such as fossil fuels in the world of severe climate change—the preferred social outcome may involve large reductions in the total quantity in commerce or complete elimination. If the responsibility of making such large-scale social transformations falls entirely to Pigovian Max’s price adjustments, these might have to phase in slowly, as Max balances the continuing harm caused by the products with the social cost of disruption from rapid squeezing out of existing products and stranding capital investments. Alternatively, the state might use other regulatory tools, which will still be available to it even with Max operating, to pursue these changes. When social goals are pursued partly or wholly through such other regulatory tools, the share of responsibility for these issues falling to Max, and the size of Pigovian Max’s price adjustments, would be reduced or eliminated accordingly.\n",
            "III. Designing and Implementing Max: Issues and Challenges\n",
            "In discussions of AI, seemingly prosaic matters of design and implementation lead, surprisingly directly and quickly, to deep questions of political, legal, and moral foundations of social institutions. As a thought experiment, Max’s job in part is to provoke these discussions. Max is intended to be taken seriously as an exploration of a potential transformative application of AI. Simply positing Max as a serious possibility and reasoning concretely through how it would work clarifies various conditions, requirements, and potential impacts and risks. But Max also aims to provoke questions about the societal conditions that define his context: how they operate, what they require, their impacts, what they are, their operations, requirements, impacts, unrecognized assumptions, and inter-relationships.\n",
            "This section addresses this second class of questions. It considers Max’s needs, implications, and potential impacts—both promising and troublesome—to probe both how feasible or desirable Max (or similarly vast AI uses) might be and what new perspectives Max provides on old questions. Even more than prior sections, the discussion roams over a vast territory, and is thus necessarily speculative and preliminary.\n",
            "A. Data: What Does Max Need and How Does He Get It?\n",
            "The central element of the old socialist calculation debate, and the one most profoundly changed by recent advances, is data. Any form of Max, like any central planning system, will require a vast amount of data to support its calculations. I rejected Quantity Max on grounds of agency problems and managerial incentives, not data limits. The data needs of managing via prices or quantities may differ based on the technical structure of the optimization problem—the relative computational efficiency of optimizing on primal versus dual variables—but that question is moot given the rejection of Quantity Max for other reasons. The two remaining variants, Price Max and Pigovian Max, have similar data needs, but differ in how they fulfill them.\n",
            "Consider first the data Max needs to replicate market outcomes insofar as these are socially beneficial, such as to generate market-clearing outcomes that are allocatively efficient in the limited, Pareto sense. Max needs data about all supply and demand conditions internal to any potential transaction, including inputs, production technologies, and consumer preferences. This is the same information as old socialist planning needed and failed for the lack of, with the small qualification that Max has a somewhat larger job than Lange’s planner, which did not set prices for final consumer goods or labor. Both Price and Pigovian Max need these data, but Pigovian Max relies on decentralized market interactions to generate them, subject to his subsequent adjustments to correct market failures. Price Max enjoys no such short-cut, but must gather, integrate, and analyze all these data and synthesize the results to contribute to his price setting for each transaction.\n",
            "In contrast to the old socialist calculation debate, it is plausible, perhaps even likely, that the data needed to construct these independent estimates of market prices are now available. This is particularly clear on the supply side, for firms. Relevant information is available from multiple sensors doing real-time monitoring of multiple attributes of production, distribution, and sales; internal accounting and management information systems; technical characteristics and performance data from machines and equipment, greatly extended by the proliferation of internet-connected devices; and complete records of the training, skills and behavior of workers, together with relevant outcome measurements. The sufficiency of these firm-level data is barely even a matter of speculation, given the high reliance on algorithmically directed planning, within large enterprises and in supply chains and multi-enterprise networks organized by a single hegemonic firm (Amazon, the Apple and Android app stores). Decisions to coordinate these large-scale operations by data-guided direction rather than internal markets strongly imply that the data needed for efficient production, cross-enterprise cost minimization, and identification and pursuit of new opportunities is available, at least to optimize the objective function of the firm directing the system.48\n",
            "Max needs these production-related data not just at the level of single firms, however, but for the whole economy. In addition to the computational challenges that I am ignoring, this shift to an aggregate perspective raises questions about incentives for full and accurate disclosure. Max would presumably be authorized to compel data disclosure, which may be effective for data from direct observations (equipment sensors, surveillance cameras), or other sources not readily subject to misrepresentation or gaming (internal managerial accounting data). Obtaining reliable disclosure may be harder for data dependent on human observation and reporting—most acutely for “tacit knowledge,” skill-like knowledge that people hold without being able to articulate, which played a major role in Hayek’s critique of planning. While I assume that this problem can be kept manageable through advances in sensors and data management, together with incentive-compatible disclosure systems and penalties for outright falsification, this is a contestable assumption.\n",
            "On the consumption side, human preferences and welfare are not directly observable, although advances in neuroscience suggest this may be changing. A host of related behavioral data is observable, however, from which machine-learning-based predictive analytics systems are advancing rapidly in their ability to predict purchase decisions and related behavior. Firms collect a huge amount of such data, and rapid progress in systems, including recommendation engines and personal assistants, suggests they may be adequate for Max to do his job. These data probably do not present serious problems related to disclosure incentives because they originate outside firms (even if firms then collect them), and so they are less likely to be deeply embedded in internal tacit knowledge.\n",
            "The data challenges involved with shifting from firm-based to societal optimization will be more serious for consumption-related than production-related data. Market systems presume correspondence between consumers’ voluntary choices and their welfare. This identification relies at two points on the axiom of revealed preference: first, if you chose it you must have preferred it given the available alternatives; and second, your preferences thus expressed are better indicators of your welfare than any outside agent can provide. To the extent this proposition is not treated purely as an axiom, it is obviously sometimes false: people make some choices that clearly harm them. No comprehensively better way to measure welfare is clear, however, and opening the door to letting others tell you what you need poses clear threats to liberty, via paternalism or worse. I mainly address this issue in discussing the problem of defining Max’s objective function in Section III.F. But I flag it here to raise the possibility that optimizing for welfare rather than for consumption behavior may require different data, which may be less readily available, less observable, or less well proxied. To the extent this is the case, even brilliant success advising and predicting consumption choices may not be sufficient to demonstrate the availability of data needed for welfare optimization.\n",
            "The collection and use of consumer data by firms is already raising serious concerns related to privacy and citizen control over their information, for which various policy and legal responses are proposed. I do not address these issues, except to note that the relevant question for my purposes is how these concerns differ depending whether the actor gathering your data is a private firm or Max. This could go either way. You might initially object more strongly to data gathering by a quasi-state actor like Max, although this difference may fade or reverse as the scale and data-integration capabilities of private firms grow to resemble, or exceed, those of states. There may, indeed, be better reasons to trust Max with our data than Facebook, Google, or Amazon. Max might, for example, be more able and willing than private firms to implement strong privacy-protective measures, such as privacy defaults, strong consent requirements, or prohibitions on redistributing, re-using, or re-purposing data. On the other hand, privacy-protecting restrictions on data use might be more disabling for Max than for private firms, who can obtain information about consumer preferences from their own interactions as market players. In any case, privacy concerns are distinct from my main focus on feasibility, unless they prompt an outraged reaction that makes needed data unavailable or unusable.\n",
            "Relative to Price Max, Pigovian Max has less need for transaction-internal production and consumption-related data, because he relies on market interactions to generate initial prices based on these. In addition to assuming that these emergent prices accurately reflect underlying producer and consumer information, Pigovian Max must also assume that using market outcomes in this way does not impair their validity.49 Transactions under Pigovian Max would occur in two stages, because transacting parties would see both the initially determined, market-based price, and Max’s adjustment to yield the final price. This two-stage process might change behavior and outcomes, depending on the strength and form of decision heuristics operating. For example, parties might fail to make transactions that are advantageous due to strong positive externalities, if they do not anticipate Max’s contribution making it privately more attractive to them. Alternatively, if buyers exhibit strong anchoring on the posted pre-adjustment price, we would expect the two-stage disclosure process of Pigovian Max to generate stronger responses to Max’s adjustments than those by parties interacting with Price Max, who would only see the final price.50 Pigovian Max might also face gaming of initial transactions, or reduced vigor in seeking advantageous transactions by parties who know Max will come in after the fact to control their transactions. Such possibilities might require Max to re-check the validity of initial prices by replicating Price Max’s estimates in some cases, thereby reducing his information advantage over Price Max.\n",
            "Both Price and Pigovian Max also need information related to any effects external to transacting parties or other market failures. Relevant market failures are of three types: (1) limited or asymmetric information, especially given heterogeneous goods and fine-grained variation of transaction conditions over space and time; (2) conventional externalities such as environment, health, and safety harms; and (3) market power. I discuss the first two here and consider market power and its consequences in the next section.\n",
            "Broadly, Max’s assumed capabilities imply that there are no information-related market failures, but there is a little more to say on this for Pigovian Max. His reliance on transacting parties’ bargaining as a proxy for all relevant transaction-internal information will be invalid if these outcomes reflect limited or asymmetric information. Pigovian Max thus cannot avoid looking under the hood for transaction-internal information; although he does not need to do this to set an initial, pre-adjustment price, he still must do it to identify and correct any information limits. This need may only apply to certain types of transaction, or may be less burdensome than Price Max’s construction of prices de novo, but still reduces Pigovian Max’s computational advantage over Price Max.\n",
            "To correct environmental and other externalities, most data Max needs will be external to the transaction, related to public or externally imposed benefits and harms. This will include both scientific and consumer-preference data—information about the physical and biological consequences of economic decisions, and about how people value these consequences. Estimates of citizen’s valuation of environmental and related outcomes are presently conducted for benefit-cost analysis of regulatory decisions, relying on a combination of behavioral proxy data and explicit value-elicitation surveys. These methods are quite crude; indeed, there are controversies over the epistemic validity of such preference estimates separate from realized market transactions; although, absent clearly better alternatives, these are extensively relied on in regulatory decisions.51\n",
            "Whether or not Max can approach some valid stable representation of such preferences, I am confident Max can construct estimates of these values better than those produced by present methods. He could equal them by precisely replicating present crude data and estimation techniques; and he would almost certainly be able to deploy his vast data and computational resources to develop better surveys, proxies, and validity-checking procedures. Max’s advantages would be even greater in integrating scientific information about causal mechanisms that link economic choices to valued impacts. Max could integrate expert scientific and technical knowledge about production processes and their external material and energy flows, as well as evolving state-of-the-art understanding of dynamics of environmental systems that link these flows to changes in valued environmental attributes. Under Max, beliefs about climate change or vaccine effects that were known with high confidence to be false would play no role in pricing the adjustments for associated transactions.\n",
            "Given uncertainty in knowledge of environmental processes, Max would also have the option of taking a precautionary approach. Such an approach would start with a stipulated constraint on some specified environmental burden, defined over the relevant spatial scale and the associated producers and consumers. Such a constraint could come from a political process or could be generated by Max based on analysis of the same preference and environmental data incorporating some specified degree of risk-aversion. With that constraint specified, Max would then set optimal price adjustments to achieve that constraint, in effect, taking a cost-effectiveness rather than a benefit-cost approach.\n",
            "All the data required for Max’s calculations will change over time and so require monitoring and adjustment. Indeed, the explosion of complexity associated with product characteristics varying over time and location was the main basis of Hayek’s revised argument for the impossibility of central planning. This was clearly correct for human planners, who could not do continuous updating and so had to specify uniform conditions over extended periods, but Max will be much more capable of location-specific and real-time adjustments. As a result, ironically, Max will have less need for accurate predictions of future conditions than human planners did. Max may also be able to identify cases where conditions change slowly or interactions are weak, and so decide when he can simplify his calculations at small social cost – if his computation is not quite costless, so such short-cuts are worthwhile. Changes over time will occur in both transaction-internal conditions and externalities, but the latter may present particular challenges of abrupt change. Scientific knowledge of mechanisms of environmental or health harm is occasionally subject to large revisions from new discoveries, which might imply sudden changes in Max’s price adjustments. As noted above for Max’s initial phase-in, his adjustments would then have to incorporate both the new scientific knowledge of harms and the costs of rapid adjustments, given the current state of the economy and capital stock. He must balance the costs of responding too slowly to the environmental harm against the disruption of steering the economy too fast in a new direction—or too confidently, given uncertainty.52\n",
            "B. Max Does Antitrust and IP: Market Power, Rent-Seeking, and Innovation\n",
            "In addition to accounting for externalities, Max will be able to manage market power and related behavior and impacts for maximal social benefit. For purposes of analyzing how Max might do so, market power can usefully be categorized in three types, with different causes. First, most jurisdictions create monopolies by intentional policy choice through intellectual property law, with the aim that the resultant rents will generate incentives for creativity and innovation. Second, some industries are natural monopolies due to cost structures involving economies of scale or scope, which give large firms decisive advantages in terms of lower cost or ability to offer more attractive goods or services. Third, market power can be created through firms’ efforts to erect barriers to entry against new competitors, using a wide variety of technological, strategic, marketing, policy, or legal means that subsume but are more extensive than the prior two mechanisms.\n",
            "In all these cases, market power – and firms’ resultant ability to raise prices or otherwise gather rents – is socially harmful. The third type, market power through artificially produced barriers to entry, represents a pure social harm with no offsetting benefit. Moreover, such advantages are often secured through explicit rent-seeking efforts, which present additional social costs with no net benefit: Those pursuing the rents benefit if their efforts succeed, of course, but at the cost of larger losses elsewhere. The second type, market power due to economies of scale or scope, also represents a net societal harm, not due to contrived efforts to seek rents but to the cost structure of the industry. Either large fixed costs create economies of scale, as in utilities with costly distribution networks or other traditional natural monopolies. Or strong network effects create economies of scope, enabling larger producers to provide some combination of better products or services, or lower costs. Economies of scale and scope create real advantages to being large, which tend toward market domination and resultant inefficiencies, even without the additional harm of rent-seeking behavior.\n",
            "Both these types of market power produce social losses as firms raise prices or restrict supply to secure rents. For both types, the core of Max’s response is to adjust prices to reduce or eliminate the rents. In the first type Max should target the rents, not the rent-seeking behavior, because the ways to erect barriers to entry are too varied and numerous to control them all, and the rents—given Max’s assumed computational capability and data access—are relatively easy to observe. Even if the boundary between normal capital returns and rents is contested and imperfectly observable (since it depends, among other things, on the riskiness of the enterprise), even approximately eliminating the rents will greatly reduce or eliminate incentives for rent-seeking, so this response – with adjustment and correction over time – is a complete solution. In the third type, where market power was artificially created through rent-seeking behavior, extracting the rents will promote a return toward competitive conditions as rent-seeking behavior declines.\n",
            "In the second type, however, the tendency toward market power is inherent in the market’s cost structure and will not be eliminated by extracting the rents. Moreover, having one or a few firms dominate such markets is socially advantageous. The problem is not the market domination per se, but the resultant opportunity to raise prices and accrue rents. The solution again is for Max to set prices to capture the rents. Using Max in this way effectively reproduces rate-of-return regulation for natural monopolies, except that this response is applied not just to a few pre-identified natural monopolies but to any firm accruing significant rents. Modern monopolies, however—internet platforms and others whose market power comes from network externalities—present one additional complexity for Max. Many such firms exploit their market power partly through transactions that are unpriced, based on the exchange of attractive free services for personal data, often under terms of service that obscure the terms of exchange. While there may be close analogies to conventional market power in firms’ ability to impose these terms, it is not clear that these relationships are fully analyzable in terms of market power. To the extent these firms act like monopolists, this will be clearer in the pricing of other related transactions, such as selling targeted advertising based on aggregation of user-provided data. The correct policy response is unclear, and may depend on regulations related to data ownership and use that would be separate from Max. Assuming such policies are in place and effective, the remaining job for Max is once again identifying and extracting the rents—a job for which the data needs are similar to what Max is already using: firms’ technological possibilities and internal accounting data, plus consumers’ preferences provide a good basis to characterize economies of scale and scope and the rents derived from them.\n",
            "The third type of market power raises more significant policy challenges. Society benefits from creation and innovation, and IP law confers market power in order to create incentives for these activities. Past economic planning efforts did not perform well on this score, and were criticized for being dull, rigid, stodgy, and lacking in innovation. Effectively promoting variety, innovation, and creativity, will represent a challenge for Max distinct from those discussed thus far. How could Max effectively promote these values—at least as well as, or hopefully better than, the present system of markets plus IP law?\n",
            "To consider this question, it is useful to separately consider different degrees of scale and novelty in innovation. At the smallest scale, innovation blends into variety in markets, as diverse products and designs are offered to cater to heterogeneous tastes and preferences for novelty. Markets do this pretty well, typically providing a mix of high-volume goods for mainstream tastes and differentiated or unique items for minority tastes. For Max to match or beat this performance is largely a data problem; if he has sufficiently fine-grained data, he should be able to identify both consumer preferences and production opportunities for a wide variety of goods. Neither Price Max nor Pigovian Max decides what is offered in commerce, of course: they only set prices or price adjustments for products that market actors are already offering. Max can use his price-setting authority to promote variety by being alert to variation and change in consumer tastes and rewarding producers who offer novel or non-standard products that some people want. He might further increase the rewards to novelty, by treating consumers’ preferences for a variety of items being offered even if they do not presently consume them as an option value that represents a positive externality. Moreover, with a small broadening of his job description, Max could prompt producers about potentially attractive opportunities when he detects a preference for variety that is not being met. In addition, Max’s job of discouraging non-beneficial market concentration will tend to promote variety of products, as a side-effect of promoting diversity of firms.\n",
            "As we consider innovation that extends beyond present product variation, Max may not be able to observe preferences for novel goods that are not presently offered. He could explore tastes or production opportunities beyond the present margin by inflecting prices to actively promote small variation, then prompt producers about opportunities and promote their exploration through small variation in prices. In effect, Max would then be conducting small experiments, encouraging producers to offer new things for sale (by a combination of suggestions to firms and favorable pricing), then tracking results and adjusting offerings in response (again by combination of providing information, offering suggestions, and favorable pricing). These small changes to Max’s operations could give modest boosts to innovation—at least small, incremental innovations, more akin to fashion and design innovation than technological innovation—via what I call a “William Gibson” mechanism.53\n",
            "If such small exploratory innovation on the margin of current offerings is judged insufficient, Max could promote larger innovation by conducting technological R&D, or even scientific research. This would represent a substantial expansion of Max’s job description. It would also present a large-scale policy choice, regarding whether to favor (in either direction) innovation and creativity by people, or by Max and other AIs.54Max could search over existing and proposed technologies and related patents and scientific and technical literature, to identify promising margins for advance. There are already signs of AI systems exhibiting such capabilities; for example, an AI system’s recent victory in a scientific contest to predict the folded structure of proteins from their amino-acid sequences,55 not to mention AI’s growing success in writing genre fiction (an AI was a runner-up in a recent novel-writing contest),56 and composing derivative but likeable music in specified styles.57\n",
            "There may be subtle risks in relying on Max for innovation and creation. The products of human creativity may differ from Max’s output or may be valued more highly for intrinsic reasons even if not observably different. Alternatively, creative outlets and activities might be judged necessary for human agency or flourishing. Moreover, innovation and creation—even technological innovation, but especially artistic, social, and political innovation—sometimes bring disruption and conflict. The creative impulses may originate in specific dissatisfactions or frustrations, in aspirations for self-definition and expression, or in novel political or social visions; and they may both be provoked by, and provoke, some degree of irritation, disagreement, or outrage. Any of these may provide reasons to limit Max’s role in innovation or creation—for example, if Max’s prolific output discourages human creators, or if the ease and reliability of innovation by Max undercuts important processes of social innovation by reducing friction and dissatisfaction, and so subtly impairs individual or societal agency.\n",
            "If it is judged important to motivate creation and innovation by humans, either in parallel with or instead of Max, Max could design and implement policies to motivate these, probably better than current IP policy. He could provide incentives using the same bundle of policies occasionally proposed as alternatives to IP, either ex ante by creator’s wages or cost reimbursement, or ex post by lump-sum prizes or price premiums added to uses of your creative work. He might even be able to assess the social value of innovations, and on that basis set optimal incentives to promote socially advantageous innovation without conferring large windfall rents.\n",
            "C. Max’s Granularity: Individually Tailored or Aggregated Determinations?\n",
            "A key question in defining Max’s responsibilities will be at what scale of aggregation he determines prices or price adders. Will groups of sufficiently similar transactions be aggregated, in effect treating them like one market with one price or price-adder? Or will Max make separate calculations for every transaction, unique to each combination of buyer, seller, and item transacted?\n",
            "This question cuts surprisingly deep in how Max is designed and what aims he is able to pursue. If Max is conceived as an externality-fixing and rent-extracting machine, the answer will depend on much these vary across transactions, and thus at what level of aggregation differences among transactions matter for social optimization. You might expect that for large numbers of similar products, made in the same or similar factories, differences in externalities across transactions might be very small. Similarly, rents might accrue to firms at a similar rate across large number of transactions. Under these conditions, there might be small losses from social optimality in aggregating across large numbers of transactions, with large reductions in computational and data burden (once again, if computation is not really costless, so we care about these burdens).\n",
            "At the same time, assessing each transaction individually would open up a powerful range of additional policy goals for Max, presenting both the potential for large benefits, and substantial risks. Assessing each transaction individually, Max could consider multiple attributes of both the product exchanged and the parties to the transaction, including not just transaction-specific externalities but also determinants of individual supply and demand characteristics, or even additional party attributes beyond these. Considering supply and demand characteristics alone, Max could know buyer’s and seller’s reservation prices for every transaction, and so replicate perfect price discrimination, with the difference that, in contrast to either price discrimination by a monopolist or bilateral bargaining, Max can divide the available surplus from every transaction in line with his social welfare function. This division would presumably reflect some reward to low-cost producers and some benefit-sharing to buyers with high willingness to pay, partly replicating the differential distribution of surplus that would occur if transactions are aggregated into quasi-markets.\n",
            "But Max could also deploy this capability in other ways. He could, for example, operate as a powerful engine to reduce social inequality by shading each transaction incrementally in that direction: in contrast to typical outcomes in present market-based systems, Max could charge poor buyers less and pay poor sellers more, so each transaction contributes a small reduction to inequality. Perfect price discrimination for individual transactions would also enable Max to take some share of every transaction’s surplus at a tax. This would represent perfect taxation with no allocative inefficiency (or deadweight loss) because all tax revenues would come from infra-marginal rents and thus have no allocative effect.58\n",
            "Individual adjustment of every transaction also raises clear concerns. At a minimum, individualized transaction assessment loses the liberating anonymity of market transactions—a loss of privacy, although I suspect privacy is gone in Max-world in any case. People have scarcely more privacy from Max than they do from an omniscient deity, although Max could still protect people’s private information from other people and organizations.\n",
            "But there are other concerns presented by individualized transaction assessments, related to the bases on which Max makes these decisions. I have described Max’s principal role as correcting market failures and have highlighted examples of traditionally recognized externalities that are large and mostly uncontroversial, such as environmental harms plus knowledge, health, and cultural spillovers. But individualized transaction assessments, in addition to letting Max conduct fine-grained calculation and correction of externalities, would also create temptations to broaden the conception of externalities in ways that begin to resemble comprehensive social engineering, raising potentially serious concerns about liberty and autonomy. As technological progress so often does, the possibility of Max opens news margins of individual and collective choice that never previously had to be considered, for which decisions are now required whether, and how, to use them.\n",
            "For example, consider the prospect of treating employee welfare—a phenomenon that is important, highly variable, and largely unpriced—as an externality of production. Firms and managers sometimes make their workers miserable, and labor markets are not so perfect that unhappy workers reliably move to alternative employment that increases their welfare. Max could treat this as a compensable externality, penalizing producers and sellers by imposing what would amount to an “unhappy worker tax.” But if Max is authorized to treat abusive managers as a correctable negative externality of production, what is to stop him from doing the same for people who act badly in other ways, or in other roles? Much human behavior harms other people even if it takes place outside the workplace. With Max in place, there would be obvious temptations to intervene more expansively, making individualized judgments of social merit based on observed or inferred behavior or attitudes. Some earnest social planner might want Max to tax people with secret vices outside their work lives, grumpy people, people with dis-favored religious beliefs, strange-looking people, and so on. Markets already do this, of course, rewarding or penalizing people for things that are irrelevant to their participation in economic production—or should be—but Max would create the ability to either reduce such differentiated treatment or increase it, potentially without bound.\n",
            "Such capabilities would present the worrisome prospect of drifting toward meddlesome and invidious discrimination to support whatever values, preferences, and prejudices are presently dominant—among the majority, or among whoever gets to influence Max’s objective function—and a broader descent to a profoundly illiberal state. The same individualized determinations that enable Max to perfect the pursuit of social optimality also enable him to exercise unassailable, individualized tyranny through complete control of individuals, even over matters well within the zone of presumptive individual liberty, by pricing their labor and defining the terms of all their consumption opportunities. Max could operate like a Twitter mob, except deploying more powerful, authoritative sanctions. These concerns provide strong reason to worry about the definition of Max’s objective function, discussed in Section III.F. below.\n",
            "D. Work Life and Worker Welfare Under Max\n",
            "I am describing Max in terms that are a blend of old-fashioned technocratic and playful, but we must not under-estimate the gravity of political transformation that Max could represent, or the intensity of associated political conflicts. The most salient dimensions of potential conflict over Max are likely to be between workers and employers (the managers or owners of enterprises) and between those at the top, middle, and bottom of the socio-economic status hierarchy. These dimensions of division evoke Marxism, and appropriately so. Max raises questions of the ownership and control of the means of production in a comprehensive and fundamental way, and so directly raises intense, long-standing political struggles.\n",
            "So is Max socialism59 —and if he is, is that a bad thing or a good thing? Or to focus on real effects rather than political labels, what would Max mean for the life and welfare of workers and for the magnitude and determinants of social inequality? My assumptions for the exercise put some constraints on these questions. People still work, but far fewer than today. And they do so not just as vocations or in pursuit of intrinsic aims, but also to contribute to the production of desired goods and services in the economy, to some degree in response to extrinsic motivations.\n",
            "The large-scale displacement of labor thus assumed is as transformative a shift as is having Max run the economy. Yet it is still also a limited assumption, because the displacement of labor is not complete. There are large numbers of people both working and no longer working. The thought experiment thus raises two deep questions, both long central to the ideological conflict between socialism and capitalism—the nature of working life and welfare of workers, and social equality.\n",
            "Firms and other large organizations, even those that participate in markets externally, mostly operate internally not by market transactions but by authority-backed planning. They are thus simultaneously islands of planning within market systems, providing a powerful rebuttal to simplistic ideologies of how capitalist economies operate;60 and islands of authoritarian control of workers by management, not organized along democratic principles.61 Workers submit to these relationships for multiple reasons, but a predominant one has been that they need the income.62\n",
            "The assumed scale of Max’s authority raises both questions in new forms. If far fewer people are working, it is no longer either feasible or morally acceptable to use wages from employment as the main basis to distribute income and other social rewards. But if these are not determined by outcomes of labor markets, then who gets what and how is it decided? Are all equal, as per simple proposals that the policy response to AI is a universal basic income (UBI)? Or if they are still differentiated, then on what basis? Is Max involved in these determinations? These supremely important questions about how to respond to AI-driven displacement of employment, and the inadequacy of UBI as a response, are topics of intense current debate, but I do not engage them here.\n",
            "But even if Max is not involved in the overall determination of rewards and the degree and basis of social inequality, I cannot fully avoid the question of how Max engages with the terms and conditions of employment for those who are working, because these questions are tightly connected with Max’s job of running the productive economy. Recall that Lange’s planning system excluded labor markets and final consumption goods from its scope, oddly leaving these areas to market interactions. That represents one a possible answer in my thought experiment here, but it is still necessary to work through the question and the implications of this along with other possible answers.\n",
            "The question of the conditions and terms of those working is tightly connected to the questions of who is working, who decides, and on what basis. Who still has jobs in the presence of Max? This will be determined by some combination of who wants to work, and what skills are still needed. This determination will have to consider the intensely heterogeneous character of work and jobs, both in their desirability and in the skills required to do them.\n",
            "Assuming there is some acceptable system in place to distribute societal resources among people—as there must be under any manner of profound AI-driven disruption of labor markets and the broader economy, whether controlled by Max, market forces, or other means—it can no longer be intolerable to be unemployed. As a result, the threat of such intolerable life conditions will no longer be available as an incentive to induce people to work (independent of the question whether it will be, or ever was, morally acceptable). Some people will want to work, for intrinsic reasons. This might be few people or many, so it is not clear in general whether human labor is likely to be in shortage or surplus. Moreover, whatever the supply-demand balance for general human labor overall, the economy will continue to require labor from people with specific skills that cannot yet be automated.\n",
            "Working will still mean some degree of relinquishing control and submitting to direction. That will be the case under any system of large-scale production coordination, by any combination of markets, central planning by Max, or authority relations within firms. For people working directly for Max outside firms, that control will be implicit, operating through the set of price opportunities or adjustments that Max offers for working on particular tasks. Within firms, additional control will be exercised by managers, whether these are people or AI. Absent some magical harmonization of collective consciousness, the terms of work life can be neither fully voluntary for individual workers nor fully democratic at the collective level, given the need for some larger-scale coordination mechanism.\n",
            "Firms operating under Max will still have to organize production effectively and control costs. Moreover, subject to Max’s vigilant policing of the magnitude of rents allowed, they will—and must for their internal decision-making to reliably align to large-scale societal needs—have incentives to earn profits. Utopian visions aside, this implies that firms must still sometimes direct employees to do things they would rather not do and must sometimes dismiss workers who are not contributing or whose skills are no longer needed. But at the same time, the human stakes of labor markets will be greatly reduced under Max, reducing or eliminating coercion to take employment. This will represent a fundamental transformation in the conditions of workers’ lives.\n",
            "The complete experience of employment—meaning the wages or other compensation, the character of tasks and the environment in which they are performed, the interactions with co-workers and managers, and the compatibility of employment with other life aims and responsibilities—must in total be attractive enough to induce people to choose to do it, under the conditions of greater voluntarism that follow from the overall reduced need for workers. How attractive these conditions must be will depend on the conditions of shortage or surplus that prevail for workers with particular skills. The greater the shortage, the more attractive the inducements for employment must be. We might generally expect the likelihood of shortage to be greater for specialized skills, although this need not necessarily be the case. When there is shortage, employers will offer higher incremental wages (incremental relative to what the workers they need can receive for not working) or other attractive inducements. Under conditions of worker surplus for particular job types, this will not be the case. Indeed, we might even imagine some areas where there is little or no need to pay incremental wages above what non-workers receive, still assuming that those life conditions available for non-workers are broadly perceived as acceptable. Even with more people wanting to work than firms need, the changed conditions of unemployment will put a floor on how miserable workers can be—a floor that is not present in current labor markets. Employers’ market power over terms of employment will still vary with the shortage or surplus of particular skills but will never be as extreme as when loss of employment is catastrophic.\n",
            "Should Pigovian Max be involved in setting wages and terms of employment? (Price Max obviously will be.) I propose provisionally that he should not, under assumptions of full information in worker-employer bargaining and no externalities directly caused by employment decisions. Externalities from other related decisions can be corrected in the transactions where they arise. If you work on a destructive product, Max will correct that externality elsewhere in production inputs or final product sale, with no need to intervene in your wages. Under those conditions, Max can leave negotiation of employment, wages, and other working conditions to market bargaining between workers (perhaps advised by their AI assistants) and their prospective (human or AI) employers.63\n",
            "E. Same Old Communist Tyranny? Property Rights and Liberty Under Max\n",
            "Where the prior discussion of worker life under Max partly addresses potential objections to Max from the left, this section aims to address some objections from the right. Even if Max doesn’t amount to state seizure of private property, isn’t Max close enough to raise all the same objections—seizure of control if not formal ownership without compensation, and threats to the associated liberty interests of both firms and citizens? In early discussions of this project, the sharpest forms of this criticism—appropriately, in view of their experience—have been raised by colleagues with personal or family experience living under the Soviet Union or other ostensibly socialist authoritarian states. These critiques suggest that a serious proposal to adopt Max is at best naïve about foreseeable ways Max would amount to, or foreseeably lead to, tyrannical state power.\n",
            "It is clear that Max is an instrument of centralized coercion on market transactions, and hence on the use and control of private property, at least for private property involved in production. But the degree of control, and thus the extent of intrusion on liberty, will vary strongly under different forms of Max.\n",
            "I rejected Quantity Max for reasons of agency problems and incentives, but that form of Max would also represent the most extreme seizure of state control, compelled production and exchange. Depending on how he is implemented, Quantity Max might also entail compelled labor. His unacceptability thus appears to be overdetermined, based on both ineffectiveness and impermissibly extreme violations of liberty.\n",
            "Price Max and Pigovian Max would still represent coercive state intervention, but to lesser degrees. Production and exchange transactions would not be compelled, but would be subject to centrally imposed conditions. For Pigovian Max, these conditions are imposed as price adjustments to transactions that are otherwise voluntary. In form, they would thus resemble a system of comprehensive sales or value-added taxes, suggesting by analogy that this degree of intrusion is not a categorically impermissible restriction of liberty, and may be justifiable in view of the public aims being advanced. This may be sufficient to establish the permissibility of Max, but this will depend on the details.\n",
            "In contrast to familiar sales-tax systems, whose purpose is to raise government revenue, Max’s purpose is mainly to steer economic production in socially favored directions and correct market failures, while perhaps also raising revenue as a secondary aim. Given this purpose, Max’s price adjustments will be more variable across transactions than those of sales taxes, including some of both signs, and in some cases will be much larger. Under Max’s direction, some products with extremely high negative externalities may be driven out of commerce, and some enterprises whose business model is mostly or entirely based on creating or shifting rents may be driven out of business.\n",
            "These aims in principle lie within the legitimate purview of democratic states. Indeed, mixed market-regulatory systems often pursue the same aims, although by various forms of explicit regulation less integrated with market transactions than Max would be. At this level of speculative generality, it is clear that Max, at least in his Pigovian form, is not fundamentally impermissible in liberal democratic states.\n",
            "But the details matter. Max would raise political controversy, as conventional regulation does, including the possibility of claims that strong interventions amount to impermissible uncompensated takings of private property. And any form of Max will be a powerful tool, making authoritative determinations on behalf of the state whose consequences are sometimes severe for particular enterprises or the value of particular assets, even if not matters of life and death. He will thus require vigilance that he only be deployed to advance broadly defensible, widely shared societal interests, not as an instrument to impose, explicitly or subtly, one faction’s vision of the good life, or their interests, on others. The conditions that determine whether Max is compatible with a liberal state and society will be fuzzy and context-specific. They will depend on Max’s objective function and the process by which it is established, as discussed in the next section. They will depend on some criteria of proportionality of costs imposed relative to benefits pursued—partly a matter of accurate and trustworthy estimation of social harms, partly a matter of limiting disruptions by phasing in large changes gradually, for Max as for conventional regulation. And they will depend on procedural recourse as protection against error and corruption, including provisions for explanation of decisions, independent review, and correction or compensation as judged warranted.\n",
            "F. What’s the Goal? Max’s Objective Function and How It Gets Decided\n",
            "We now come to the two hardest clusters of questions that Max presents. First, what goal does Max pursue in guiding his interventions, and how—and by whom—is this decided? And second, how might we get to Max: what pathways from present conditions to a society with Max in place might be feasible, likely, or desirable; how do these relate to present capabilities and trends; and what pitfalls and risks do these pathways present? I deal with the first set of questions in this section, the second set in the next.\n",
            "What goal, what conception of social welfare, does Max pursue? In technical terms, what is Max’s objective function, and how is it determined? I have presented Max as an alternative—or in the case of Pigovian Max, an augmentation and corrective—to markets. Market systems have a claimed normative foundation, originating in the “invisible hand” metaphor in Smith’s Wealth of Nations64 and later formalized in the two fundamental theorems of Welfare Economics.65\n",
            "This normative claim depends on a few strong assumptions. The widely recognized and often-violated assumptions required for conditions of perfectly competitive markets—full information, no market power, no externalities—define most of Max’s job as discussed thus far, so I do not address them further here.\n",
            "But there are two other, more foundational assumptions on which the claimed social optimality of market outcomes depend. These assumptions allow markets—or more precisely, defenders of markets’ optimality—to avoid certain hard problems that most forms of Max cannot. First, market optimality claims presume that people’s market choices reliably reveal their preferences and their well-being. Second, these claims rely on a definition of social welfare, Pareto optimality, which excludes consideration of interpersonal welfare comparisons and distribution. These assumptions together allow a thin conception of social welfare, which avoids the need to define an explicit social welfare function but at the price of being silent on many points of clear importance for total societal welfare, notably, but not only, distribution and inequality.\n",
            "Could Max get away with a similarly thin conception of social welfare, and thus avoid an explicit welfare function? This will depend on how broadly or narrowly his job is drawn. In its narrowest conception—Max only modifies each transaction to correct for information disparities, market power, and externalities—it is conceivable that Max could do this job, or approximate it, without an explicit social welfare function. Max could correct information limits or disparities between transacting parties. He could assess rents using internal accounting information from producers, perhaps augmented by comparative information from other firms in similar businesses. He could assess and correct externalities based on scientific knowledge about biophysical mechanisms of harm and estimates of people’s valuation of the resultant end-states. To the extent external harms and benefits operate as public goods that affect multiple people, assessing their aggregate effect requires adding up individual effects and thus that these be expressed in commensurate terms, but does not require explicit interpersonal comparisons.\n",
            "But Max also has the opportunity—or the duty—to allocate the available surplus from every transaction after he has taken account of externalities and rents. In doing this, he could take various simple approaches that can be defined from parties’ relative valuations within the transaction, and thus do not require an explicit social welfare function. He could, for example, divide surplus in some given proportion between buyer and seller—equally, or in the same shares as the parties would have realized if Max had not intervened—applying such proportional division either to the entire available surplus, or to that portion that remains after Max takes some share as tax revenue.66\n",
            "But any more ambitious approach that Max might take—including any approach that does not treat all transactions the same after accounting for externalities and market-concentration rents—must rely on characteristics of the parties external to the transaction, such as their wealth or other characteristics. Providing guidance for such choices requires an explicit social welfare function to define what count as better or worse social outcomes. As in many other applications, the shift to AI-directed decisions requires explication and codification of values and tradeoffs that may be left ambiguous or implicit absent such central direction.\n",
            "Assuming Max is ambitious, and thus does require an explicit social welfare function, the task of defining it can be separated into two parts: defining individual welfare and aggregating across individuals to define overall social value. These two parts present different difficulties, and challenge different parts of the edifice of assumptions and arguments underlying normative claims for market outcomes.\n",
            "First, how does Max define and measure individual people’s well-being? In doing this, Max has a harder job than present AI systems, which only aim to predict commercially relevant behaviors: purchases, engagement, click-throughs, and the like. As noted above, normative claims for optimality of markets depend on assuming all these behaviors are aligned with your well-being, via one or another form of the axiom of revealed preference: if you do it, you must want it (relative to available choices); and if you want it, it must make you better off. This axiom provides a powerful foundation for liberal states: assuming you know what you value and act to pursue it is generally preferable to assuming I know what is good for you. On the other hand, the assumption is obviously false in many cases. People often make choices that are bad for them in a reasonably objective sense, e.g., in self-harming activities and use of recreational and performance-enhancing drugs that are addictive or harmful. And people often do, or fail to do, things that they later regret: not exercising enough, not saving for retirement, or spending too little time cultivating meaningful activities and relationships. Indeed, many business models depend on exploiting these misalignments, by taking advantage of impulsive behavior, distraction, or weakness of will.\n",
            "We would want Max to avoid these clear pitfalls, ideally to do comprehensively better. But this ambition raises serious risks, including paternalism, loss of autonomy, or imposing one group’s values on others, which require proceeding with great care. These risks are mitigated for Max in his Price or Pigovian forms, because he only has power to modify prices, not to tell you what to do. Max will discourage you from drinking or smoking by raising the price you face for alcohol or cigarettes67 —perhaps encouraging moderation rather than abstinence by dynamically changing prices (I want another drink; Wait, it costs how much?) —but not saying you can’t have them. He might even recycle the revenues realized from these high-priced transactions for your benefit, by directing them to your future health-care or retirement expenses rather than sending them to either the distillery or the treasury. But while this price-based approach reduces Max’s coercive power over you, that power can still be substantial. Max must only wield it in service of your considered interests and values, not slide over to me (or anyone else) specifying how you should live or what you should want.\n",
            "To achieve this balance, Max needs a model of your welfare that avoids pathologies of choice but that still represents your vision of your welfare. It must represent a considered view of your interests and values that is not distorted by unconsidered habit or impulse; is not manipulated by other parties for their own advantage; that takes account of how you want to be, even when your present behavior diverges from that vision; and that appropriately reflects intertemporal tradeoffs,68 uncertainty, and the welfare of other people and values outside yourself – but that is still yours. Or at least, since Max’s authority is limited to the economy, he needs a model of these things for you insofar as they are implicated in your economic transactions.69\n",
            "To form this model, Max can draw on the same behavioral data firms already use and are developing, both data that pertains uniquely to you and generalizations inferred from other people. If Max is sufficiently trustworthy that we consent, he may also be able to draw on data not necessarily available to firms, such as medical data, or internal physiological and neurological observations, present and past. But Max’s biggest advantage in forming this model of your welfare is that he does not have to do it alone. Like present proposals for AI-enabled personal assistants, Max can work with you, observing you and asking you about your preferences, aspirations, and feelings about your past choices and hypothetical future ones, to refine and update his model of your welfare. Operating in this way, Max looks more like a life coach or counsellor than an economic planner: indeed, this vision of Max is very similar to the approach proposed by Stuart Russell as a safety measure against AI assistants making serious errors when they act on your behalf.70 Such a personal AI assistant would be concerned with many other choices in addition to your participation in economic transactions, however, raising the question of whether this assistant should be some other AI-enabled agent, distinct from Max, whose information and concerns are limited to you. Such a personal AI agent—let’s call him Mini-Max—would closely resemble Russell’s faithful personal AI assistant, except that, as the guardian of your personal welfare, he would be responsible for passing on to economy-wide Max (“Big Max”) a subset of the information he holds about you, which is relevant to your preferences and welfare as they are connected to your participation in economic transactions, and the effects on you of externalities from others’ transactions. This is the information about you that Max needs to incorporate your welfare into his price-adjustment decisions. The rest of your interactions with Mini-Max, and the rest of his knowledge about you, are not needed by Big Max and can stay private between the two of you.\n",
            "Even with a valid assessment of everyone’s welfare as affected by economic transactions, Max will still need to aggregate to a collective measure of social welfare. Because Max is serving in a liberal state—not a theocratic one, not one that tries to implement a universal Kantian approach to ethics (except, perhaps, in criminal law, which remains the state’s business, not Max’s)—that measure of social welfare must be some form of utilitarian summation of individual welfare measures as they pertain to economic activities. Any such aggregation requires weights attached to each person’s welfare. While giving equal weight to everyone’s welfare is an obvious default choice, there may also be legitimate bases to give some people’s welfare stronger weights than others’. In particular, under conditions of social inequality, it may be permissible, or even morally required, to give larger weights to the welfare of those worst off. Moreover, any aggregate welfare measure must consider the relative weights to give to economic versus non-economic contributors to welfare;71 conditions at different times; and conditions that apply under different realizations of uncertainties. Except under the assumption that all these dimensions are correctly embedded in the individual welfare measures passed to Max, the social welfare function must represent collective judgments on these matters.\n",
            "Although fully specifying Max’s objective function is beyond my scope here, this discussion suggests the problem can be approximated by specifying a few parameters. If we assume that Max’s social welfare function is some basically utilitarian aggregation of individual welfare measures, which takes appropriate account of inequality, time, uncertainty, and economic versus non-economic determinants of welfare, this suggests that specifying the function might be closely approximated by setting values for four parameters: (1) a measure of aversion to inequality to be used in setting relative weights for better and worse-off individuals; (2) a discount rate or other parameter to set the relative weighting of outcomes at different times;72 (3) a measure of risk-aversion to weight outcomes under more or less favorable resolutions of uncertainties; and (4) a relative weighting of material consumption and non-economic contributors to welfare such as environmental conditions.\n",
            "This last parameter, the relative weighting of economic and non-economic contributions to welfare, is likely to be the main instrument controlling the aggregate size of economic output under Max. If the material and energy flows associated with production, which determine environmental impacts, cannot be arbitrarily reduced toward zero, then environmental conditions will define the limits on the aggregate scale of the human productive enterprise. In a world of greatly reduced need for human labor in production, such environmental constraints are likely to be more tightly binding than any limit on production that arises from people choosing leisure time over employment.\n",
            "In addition to asking what Max’s objective function is, we must also consider the process by which it is chosen. Although Max mostly represents a technocratic vision, this is a point where democracy must come in. Defining a collective conception of social welfare is an intrinsically political process, which must have people in charge working through some democratically legitimate mechanism. In considering how to do this, the assumptions already made have simplified matters considerably. Measures of individual welfare emerge from the interactions between people and their AI-enabled personal assistants, while the aggregation to social welfare has been reduced (for purposes of argument) to setting values for a few powerful, readily understandable parameters. Without denying the advantages of expert-driven, even technocratic, decisions for complex, largely instrumental decisions in pursuit of broadly agreed political ends,73 this decision agenda is sufficiently clear and simple to place it within the capabilities of many different democratically legitimate processes. For example, you can imagine this as a legislative task, by which values for the major parameters of Max’s objective function are explicitly enacted and periodically revised in statute. You can also readily imagine these as being matters of explicit debate in electoral politics, or being delegated to novel democratic processes such as juries of randomly selected citizens. You could even imagine the task being delegated to some expert administrative agency under legislative articulation of some higher-order aims to be advanced by the choice, assuming (in U.S. law) this decision survives the resultant constitutional challenge on non-delegation grounds.\n",
            "The biggest risk associated with Max’s objective function is the risk of capture. One irony that Max presents is that while one of his major jobs is reducing market power and associated rent-seeking in particular markets, the centralized political process of defining Max’s objective function represents a concentrated opportunity for rent-seeking that overwhelms all others. Anyone able to inflect Max’s decisions to serve their aims, even slightly, would be in a position of unprecedented power—to gain rapid wealth even beyond the dreams of tech-startup founders, or to shape society to their vision. Worse, the exercise of such power might be concealed by Max’s status as a seemingly objective, neutral artifact.74 Restricting the political agenda to setting a few highly aggregated parameters partly addresses these concerns.75 These parameters do not allow the manipulation of small-scale details that would be needed to distort Max’s decisions to a few actors’ material advantage, and they aim to promote a democratic dialog on basic political values. But it is a long way from these high-level decisions to Max’s actual operations, with many intervening steps that are more technical and opaque, over which many actors would love to exercise quiet influence. At the level of generality of this discussion, there is no more to say here beyond exhortations to vigilance about such manipulation, as much transparency as is feasible in the process of designing, training, and implementing Max, and procedures for recourse for those harmed by Max’s decisions.\n",
            "G. Getting to Max (And Avoiding Dangers Along the Way)\n",
            "Max is a thought experiment, intended to be speculative and provocative. Yet part of the purpose of the exercise is to argue that Max is not crazily remote from present capabilities and trends. Many elements that could make up Max-like capabilities—rapid expansions in computational capacity, algorithms, data, and data integration and analysis tools—are already present or in development. These are mostly developing under private control to pursue commercial interests, or under state control to pursue military and geopolitical advantage, but not exclusively. There is also substantial research underway in universities and publicly supported research institutions, some of it loosely organized as a pursuit of “AI for good.”\n",
            "In this section I shift from how Max would work as an endpoint to considering possible transition pathways by which Max, or similar capabilities, might come about. Any such pathway will involve a combination of technical and socio-political developments. I sketch three transition pathways that are sufficiently distinct and (to varying degrees) plausible to merit examination.\n",
            "The first, and seemingly simplest, pathway would involve some jurisdiction deciding at some future point to adopt Max wholesale by political choice. Such a choice would lie within the authority of states, but would raise several immediate questions and challenges. Even assuming the needed capabilities existed, were ready to deploy, and confidently judged to work, the administrative scale of such a transition would be vast. It would require a massive roll-out and testing of infrastructure and systems before switching on, then some form of switch-over, perhaps at a long pre-announced moment during a period of reduced economic activity such as a near-universally observed religious holiday. The transition bears some resemblance to occasions when countries have reversed the direction of road travel, although the change would be much larger (albeit one not involving a risk of head-on collisions).76\n",
            "Adopting Max would be a huge decision, beyond the authority of any administrative or executive process but requiring some democratically legitimate political process, legislative or perhaps constitutional. And it would present a chicken-and-egg problem regarding capabilities. Making such a choice would likely require confidence that needed capabilities are available, would work reliably, would deliver the promised benefits, and would present no severe risks. But such confidence could only be available after some long period of prior development and testing, which in turn would require prior political decisions to support these. Even those prior decisions to develop and test the capability would surely encounter stiff opposition, from those with strong ideological commitments to markets and from those benefiting from precisely those social harms—rents from market power, and uncharged negative externalities—that Max would target. In view of these difficulties, I suspect that adopting Max by explicit political choice would be highly unlikely, absent strong changes in political conditions such as an economic crisis so severe as to weaken the blocking power of incumbents. Even seeing Max operating successfully in other jurisdictions, while it might help (and thus imply that the first move would be the hardest), would probably not help enough absent a crisis.\n",
            "A second possible route, potentially mitigating the extreme barriers for the first route, would involve early development, testing, or adoption of Max at smaller scale, among groups with more enabling political conditions. Possible early demonstrators and adopters might include jurisdictions that already have substantial shares of the economy in state enterprises or under state control; or those enterprises for which majority control already resides in some coalition of large sovereign wealth funds (Hello, Norway). Even jurisdictions with little state control of the economy could develop and test Max through government procurement, as governments often do for early support of environmental technologies. Max might also be developed through progressive expansion from small, early, opt-in communities. These might be any group of individuals and organizations connected tightly with each other and less so with others—like religious groups, social or political experimenters, or relatively isolated political and economic jurisdictions—who would let Max, better now called “Pre-Max,” control their production and exchange relationships with each other.\n",
            "Any such group of early adopters would face a few obvious challenges. They would have to port and modify capabilities from other uses, which in turn would require that these capabilities be sufficiently and verifiably adaptable to their new purpose and setting. Alternatively they could develop the new tools and systems themselves, in which case they would need the resources to do this. Perhaps, given the novelty and importance of the experiment, they could attract philanthropic support. The initial group would have to be large enough and separate enough that their interactions with each other represent a substantial fraction of all their economic interactions. And to the extent they do trade with the rest of the world, they would need to ensure that such trade does not undermine Max whenever his prices diverge from private-market prices. An analogous problem would arise with any deployment of Max, at any scale. Whatever scope of transactions is given to Max, his authority over those transactions must be exclusive: black markets must be effectively prohibited, and exchange across the boundary of Max’s authority must not negate his adjustments. In the case of international trade, Max’s adjustments would have to be applied in parallel to traded transactions to avoid arbitrage opportunities, like proposed border tax adjustments on traded goods to preserve the effectiveness of greenhouse-gas or other environmental policies.77 For this to be a viable transition pathway, Max must work well enough—perhaps after some early start-up phase carried by the enthusiasm of early adopters and start-up philanthropic support—that there are clear aggregate benefits to working with him that are visible to outsiders.\n",
            "A third pathway, more continuous with present trends, would involve continued expansion and consolidation of Max-like capabilities in the private sector, to the point where a few enterprises or networks control a large fraction of the economy. It is widely noted that as the scale of platform monopolies grows, they increasingly resemble states and exercise similar authority, although without provisions to ensure democratic accountability.78 Assuming some degree of concentration of private economic planning (and power) is widely viewed as unacceptable, Max could come about through some future political decision to take over and re-purpose the systems. This would not be a seizure and public re-purposing of physical capital assets, but of AI systems and associated data, although that nicety would hardly make the decision less wrenching and conflictual.\n",
            "This pathway relies on two assumptions. First, it presumes that some future historical moment allows a wholesale takeover of concentrated private power that is then judged to have become intolerable, amounting to a large-scale reconfiguration of power between private and public actors. This would be a revolutionary change, carrying the risks of disruption and violence that typically attend revolutionary changes. Second, it presumes the technical feasibility of re-purposing a set of AI tools and data developed for private purposes to serve Max’s public aims. This may not be fully possible, as some of Max’s responsibilities—like assessing individual well-being, valuing externalities, and measuring rents—are not required of present systems serving private interests. To the extent the existing tools and data cannot perform these tasks, they would represent separate, new development requirements.\n",
            "Conclusions: What This Gets, Leaves Out, Challenges Unearthed\n",
            "As a speculative exploration, this exercise does not lend itself to strong conclusions. Yet it appears to have yielded a few provisional observations and insights, which at a minimum suggest guidance for further exploration and research – including identifying some points of potential near-term guidance, for research and for early development of governance capabilities to manage risks.\n",
            "First, I contend that the exercise has established some degree of plausibility for the hypothesized AI-driven central economic planning – under the admittedly strong assumptions made about technological capabilities. The exploration identified multiple developments underway that point toward the future capabilities assumed, and found no show-stoppers. Although this claimed demonstration of plausibility is highly qualified, it is not a trivial conclusion, since the exploration of different forms of Max under different contextual assumptions gave widely divergent views of their plausibility, with one variant of Max—Quantity Max in the presence of some degree of continued human managerial agency—presenting apparently insuperable obstacles.\n",
            "More broadly, the exercise substantiated the general point that profoundly transformative applications and societal impacts from AI and related capabilities are plausible – with the potential for both great benefit and harm – long before the conventional mileposts of AI that transcends human capabilities and control. I have argued elsewhere for the importance of these “intermediate-range” AI capabilities and impacts, and for their distinct character from both near and long-term issues—in particular in their requirement for integrated examination of both technical characteristics of AI systems and the economic, political, and social context in which they are deployed.79 While it is defensible to focus predominantly on technical characteristics in considering long-term risks, and on human interests and decisions in considering current applications and their impacts, neither of these simplifying assumptions is apt when considering intermediate-range capabilities and impacts. Max is surely not the only example of a plausible, profoundly disruptive potential AI application that falls in this middle range – indeed, this exercise suggests the value of thinking through other possibilities of similar transformative scale – but the detailed examination of Max and his implications hammers home the importance of these more vividly than the prior, more general arguments.\n",
            "More specifically, the exercise of digging down to the particulars of Max’s operations and consequences yielded several suggestive insights, each offering useful guidance for further analysis and inquiry. First, it appears that alternative conceptions of how Max might be implemented differ starkly in their feasibility, requirements, and attendant obstacles and risks. In particular, the idea of Pigovian Max – a central-planning based implementation of a comprehensive system of Pigovian taxes – is a novel and promising vision of hybrid private-public control of economies, not previously considered in debates over central economic planning. Pigovian Max appears to offer the prospect of three major advantages, subject to all the requisite caveats. He appears potentially able to retain the efficiency and liberty advantages of private market systems while also correcting their most prominent failures. He also appears to offer the prospect of taxation without excess burden, albeit at the cost of aggressively individualized scrutiny of citizens’ preferences. And finally, he appears to offer the prospect, through management of the parameters of Max’s social welfare function, of bringing large-scale economic management under effective and informed democratic control, without losing the advantages of private markets.80\n",
            "Second, even this preliminary investigation suggested that data and data integration needs may differ strongly over different forms of Max and jobs given to him – e.g., assessing individual welfare, pricing externalities, identifying and mitigating rents from market power, assessing and improving quality of working life, and promoting valued innovations and creative works. Moreover, these data needs may also differ strongly from those needed to predict and manipulate commercially relevant behavior for the benefit of counter-parties. Assessing these needs for specific aims, meeting them without unacceptable harm to other values, and learning how to integrate private data about individual welfare with enough sharing to enable effective social optimization, will all be important research areas.\n",
            "Additional areas of further inquiry suggested by the exercise include the preferred mechanisms for promoting innovation and creativity and innovation in society, in the presence of computational capability that can greatly accelerate and optimize at least those innovation mechanisms that depend on searching presently available information; and the content, structure, and means of defining an aggregate social welfare function. The initial inquiry into this latter question suggested that it might be much less difficult than widely assumed, but the high stakes involved suggest viewing this optimistic initial speculation skeptically. Further critical investigations into potential forms of social welfare function that are precise enough to guide Max’s decisions but also clearly and simply parametrized enough to support meaningful democratic decision-making are of high value; as is investigation of alternative democratically legitimate processes and institutions to conduct this parameter-setting process.\n",
            "A particularly interesting area for further inquiry provoked by the exercise would be examining more limited deployments of Max. If the comprehensive, economy-wide Max discussed here is for some reason infeasible or unacceptable, might variants with more limited scope provide many of the proposed benefits with less cost, disruption, or obstacle? Max’s scope might, for example, be limited to enterprises over a specified scale, or to sectors identified as presenting especially large externalities or tendencies to market power and rent-seeking. A particularly interesting variant would limit Max’s authority to capital markets, either overall or jointly with a scale threshold. In this variant, “Capital Max” would allocate, or more likely price-adjust, capital to enterprises, replacing or operating in parallel with private capital markets. Capital Max would presumably use the same objective function as economy-wide Max. Since this function would consider both the private and public effects of enterprise operations, Capital Max would not precisely replicate the behavior of either private capital markets, or past efforts to allocate capital in line with political aims. Consequently, the well known critiques of these past efforts would not necessarily apply, any more than the old critiques of comprehensive central planning would apply to economy-wide Max. Hints that Capital Max might be feasible and advantageous come from two lines of evidence: first, the extent to which capital allocation is already automated, via index funds, trading programs, and other algorithmic systems, which suggests that the change to Max might merely require adjusting the objective function; and second, the likelihood that key points in capital markets exhibit substantial market power, as well as systematic biases and choice pathologies. Capital Max might thus be able to gather low-hanging fruit, operating in parallel with and out-competing existing private capital-allocation mechanisms – thus, ironically, subjecting them to increased market discipline. Viewed in this way, Capital Max would not aim to abolish Wall Street, but merely to subject it to real competition and thus make it work better.\n",
            "Identifying these questions for further research and the associated stakes re-affirms one observation made in the introduction. It is widely noted that large technological change can drive transformative societal change, disruption, and conflict. But such changes can also explicate and disrupt foundational shared assumptions that underpin the norms, institutions, and power structures of society. In particular, these may depend on assumptions about what people can do to each other that are technology-limited, but not recognized as such until the technology changes. This exercise has targeted long-settled assumptions about the moral and instrumental effects of markets versus central economic control, but other unexamined foundational assumptions – in particular about the extent and form of power that some can exercise over others – may face similar disruptions under large-scale technological change.\n",
            "Max, in particular his Pigovian form, presents three ambiguities, which should be kept in mind when considering his potential implications. First, is it ambiguous to what degree Pigovian Max would represent an incremental reform or a revolutionary transformation. I began the project as an intentionally extreme speculation about technological change and its implications. But elaborating the practicalities of implementing Pigovian Max made him increasingly look like a feasible, even incremental reform: an adjustment to improve a basically capitalist system, drawing on well established legal, institutional, and administrative capabilities, which appears quite compatible with a liberal democratic state. This claim must be qualified, of course, because implementation details will matter greatly: some variants of Max would clearly be so heavy-handed in their imposition of central control as to be incompatible with basic liberties. It might be a small step, easy to stumble over, from using price adders to correct clear externalities and rent-seeking, to adding incentives for sociability, pleasing others, conformity, docility, piety, or obedience to current political authorities. Any suggestion that Max might be a modest incremental change to the architecture of capitalism must reckon with these risks – and also with the challenge, discussed below, of finding a feasible and non-violent transition pathway that leads from here to Max.\n",
            "A second ambiguity concerns the aggregate normative evaluation of Max: would he on balance be good or bad for human welfare? I began the exercise agnostic on this point, and reached the unsurprising conclusion that it could go either way, depending on design and implementation details that an inquiry at this high level of generality cannot resolve. Yet this experience also cast into sharp relief the strength of normative priors that animate other writings on this question, and how thoroughly and confidently these priors lead directly to the conclusions. This observation applies equally on both sides of the debate: on the one hand, to the growing number of socialists writing on AI central planning, who know – with little consideration of alternative implementation details or contextual conditions – that it would be good; and on the other hand, to the unnamed recent essayist in the Economist, who knows with similar prior confidence that it would be bad.81 It appears clear that further investigations of this issue should link their normative assessments to explicit and specific assumptions about how central planning is implemented and what capabilities it draws on, in what context – even at the cost of yielding less clear and less predictable answers.\n",
            "A third ambiguity concerns how to characterize Max’s job, in particular as regards what he is replacing. Although the starting aim was for Max to replace “the market,” working through the details led to a preferred form of Max, Pigovian Max, who lets the market operate then applies socially optimal adjustments to the resultant prices. Since controlling externalities and market power are canonical state functions, this makes Max look more like a comprehensive regulator – a state actor – than a market-like coordinating mechanism. Moreover, at each point in the argument where I proposed expanding Max’s purview to include additional functions, these also looked more like state than market functions – or perhaps functions of the non-governmental charitable sector. Yet Max is not – and probably cannot and should not be – all of the state. The state does more than regulate, and even its regulatory functions are not limited to economic transactions. The state-market boundary is already fuzzy and contested, a point for which working through Max provided a helpful reminder. But introducing Max complicates, partly dissolves, and moves this state-market boundary.\n",
            "In closing, I return to the question of Max’s plausibility, and to the most disturbing issue raised by the exercise. I claimed above that Max passes some threshold test of plausibility, but plausible does not mean likely. Even a more complete and persuasive demonstration that a fully implemented Max would raise no impossible conditions would not necessarily imply a feasible or acceptable transition path to get from here to there. A technological artifact of Max’s scale and complexity does not arise spontaneously, but must be pursued and developed by actors who can mobilize the needed (albeit uncertain) scale of expertise, resources, and authority. Max’s real-world feasibility will thus depend on both needed technological capabilities and favorable social and political conditions. In this regard, the fact that Max-like capabilities, or large parts thereof, are already present or in development – with the crucial difference that these developments are in private hands and aim to advance private or sectional interests, not broad public ones – cuts both ways, both for Max’s feasibility and for the prospect of AI bringing broad human advances. Two sobering implications follow.\n",
            "The first concerns the risk of lost opportunities. There may well be prospects for mid-term AI developments that could bring profound advances in human welfare, whether through something like Max or through other applications in health, environment, education, or government. But if the specific technical requirements to realize such broad benefits differ greatly from those being pursued by private actors, then near-term RD&D decisions, plus path-dependency, may foreclose the prospect for such transformative future benefits. The severity of this risk depends on the portability and adaptability of capabilities – how readily those developed for private or rival purposes can be adapted to serve public or universal ones – which is deeply uncertain.\n",
            "The second implication concerns the medium-term implications of continued dominance of private actors and interests in guiding development of increasingly powerful AI capabilities. Continued expansion of capabilities could become self-reinforcing – not in the oft-proposed sense of AI systems themselves growing unboundedly powerful through recursive self-improvement, but in the sense of capabilities controlled by human actors recursively strengthening the concentration of social, economic, and political power in those actors’ hands. Perhaps even worse than the loss of potential human-liberating capabilities, such trends could lead to profoundly dystopian futures, whether these come about with a bang (violent upheaval) or a whimper (incremental loss of human welfare, agency, and hope).82\n",
            "These dire possibilities suggest the value of large early investments in development of AI and related capabilities that are explicitly targeted at comprehensive public benefits. This may sound obvious, but it may in fact be the most radical suggestion in the paper, because such efforts might not just differ greatly from present privately-driven developments, but also from present small “AI for good” efforts, in at least two respects. First, the needed development efforts would not erroneously assume that economic development benefits to the sponsoring jurisdiction – the growth and competitive success of enterprises located there, or the successful tech-industry job placement of students trained there – are identical to the aggregate public benefit. Second, they would not presume that the technological capabilities developed for private commercial advantage will be readily and without limit re-deployable in pursuit of non-commercial public purposes. This will be the case to some degree, of course, and a development program seeking public benefit should not needlessly re-invent wheels that can equally well be installed on public and private vehicles – but how much, in what particulars, and for how long this will be the case is deeply uncertain, and it would be naïve for a publicly motivated development effort to assume comprehensive, continued complementarity between these. I recognize that the implications of this conclusion for resource requirements are large – and at odds with present trends in public-private division of resources and authorities – but the risk of continued, uncritical reliance on the assumed complementarity of technologies to advance competitive or rival interests and to serve broad public ones, appears too large to ignore.The post Max – A Thought Experiment: Could AI Run the Economy Better Than Markets? first appeared on AI Pulse.\n",
            "<bound method BatchEncoding.tokens of {'input_ids': [11518, 784, 317, 27522, 29544, 25, 10347, 9552, 5660, 262, 18493, 11625, 17924, 30251, 30, 628, 198, 10472, 355, 12960, 198, 198, 43982, 317, 13, 357, 38972, 8, 350, 12613, 16, 198, 23839, 198, 3198, 286, 262, 7531, 44329, 1028, 29112, 4289, 10256, 287, 4318, 3034, 5410, 11, 290, 262, 1388, 1738, 329, 511, 15536, 11, 373, 262, 16612, 286, 1692, 12, 34762, 5410, 3341, 284, 6687, 262, 1366, 11228, 11, 3781, 11, 29964, 11, 290, 1630, 3306, 284, 1277, 262, 5909, 13357, 286, 3227, 11, 20157, 11, 290, 5163, 5370, 326, 787, 510, 257, 3660, 3773, 13, 26430, 2274, 14901, 287, 9552, 11, 1366, 11, 290, 3519, 14614, 9889, 423, 302, 12, 26350, 326, 1468, 1808, 11, 290, 29579, 31543, 13367, 546, 262, 40460, 11, 4034, 11, 290, 7432, 286, 281, 9552, 12, 34762, 3773, 13, 770, 3348, 10969, 257, 1807, 6306, 546, 703, 428, 1244, 670, 11, 1912, 319, 13148, 257, 3665, 9552, 5797, 357, 1929, 12078, 1146, 3706, 564, 250, 11518, 447, 251, 8, 351, 645, 12765, 31350, 393, 8385, 9383, 7095, 319, 663, 357, 14363, 8, 2694, 284, 466, 262, 4876, 13, 383, 3348, 447, 247, 82, 5337, 10156, 318, 284, 787, 428, 45053, 739, 12, 23599, 1808, 517, 10017, 290, 2176, 13, 632, 3840, 10017, 306, 832, 703, 884, 257, 1080, 1244, 670, 739, 7952, 14895, 546, 38356, 3403, 26, 644, 4034, 340, 1244, 2897, 3585, 284, 1944, 1910, 290, 7668, 12, 10728, 14752, 26, 644, 5337, 5359, 393, 17778, 340, 561, 1944, 26, 644, 7432, 290, 6459, 340, 561, 12705, 11, 290, 703, 340, 1167, 801, 82, 890, 12, 5646, 1833, 654, 286, 43936, 2683, 546, 1181, 11, 3592, 11, 290, 1692, 12354, 13, 198, 1722, 351, 4833, 12, 9888, 11344, 19901, 11, 262, 10017, 7822, 286, 9815, 4318, 5410, 460, 307, 12531, 276, 355, 37294, 2884, 12755, 2035, 17794, 393, 4536, 13, 383, 3348, 11673, 326, 12040, 12, 3106, 10581, 561, 307, 17640, 22753, 416, 2761, 286, 10033, 12, 25781, 2316, 290, 16538, 11, 543, 32724, 9342, 6754, 5410, 3341, 290, 561, 21160, 739, 14977, 31350, 14901, 13, 7886, 12, 3106, 10581, 11, 355, 5150, 416, 440, 8135, 283, 47579, 11, 466, 407, 6646, 8659, 422, 262, 976, 19358, 13, 3125, 11781, 621, 2035, 11, 2158, 11, 561, 307, 257, 15304, 287, 543, 5436, 15314, 257, 9815, 1080, 286, 2756, 19008, 2087, 284, 3165, 6783, 1910, 10906, 11, 7548, 284, 257, 9815, 3773, 12, 4421, 1080, 286, 23097, 709, 666, 5704, 290, 16230, 13, 8013, 257, 1080, 11, 564, 250, 47, 328, 709, 666, 5436, 11, 447, 251, 714, 287, 7989, 6537, 262, 1321, 9332, 4034, 290, 12354, 5353, 286, 26512, 1910, 10906, 11, 981, 635, 8569, 2280, 39038, 7097, 871, 290, 12755, 30904, 10368, 286, 1910, 1176, 290, 3917, 5602, 12, 38515, 4069, 13, 632, 714, 635, 11, 739, 1728, 3224, 14895, 11, 2897, 262, 6034, 286, 21843, 1231, 2636, 6551, 2994, 11, 416, 2263, 477, 5704, 422, 1167, 859, 853, 1292, 28393, 13, 198, 14698, 16493, 262, 4096, 3164, 290, 777, 2785, 4034, 11, 262, 3348, 19451, 1811, 6459, 290, 2785, 7476, 5545, 416, 884, 257, 1080, 13, 2312, 2291, 5436, 447, 247, 82, 761, 329, 1366, 290, 262, 2785, 3484, 286, 4955, 340, 26, 262, 19468, 33737, 393, 46500, 286, 5436, 447, 247, 82, 3416, 602, 26, 262, 1917, 286, 10941, 4996, 290, 11044, 287, 281, 3773, 7924, 416, 5436, 26, 262, 10939, 286, 5436, 329, 262, 9490, 286, 1692, 3259, 11, 262, 3616, 290, 6287, 286, 3119, 2489, 11, 290, 3917, 12354, 5353, 26, 262, 6770, 286, 1919, 9490, 326, 15947, 5436, 447, 247, 82, 9432, 2163, 11, 663, 17764, 351, 10518, 1630, 11, 290, 262, 43440, 10159, 286, 262, 18645, 1022, 262, 1181, 290, 262, 3773, 26, 290, 3443, 11, 262, 2776, 286, 5436, 284, 9552, 12, 25616, 11257, 1541, 17715, 11, 351, 10939, 329, 262, 40460, 286, 5436, 852, 4166, 290, 8197, 11, 290, 262, 3917, 7476, 13, 554, 1570, 286, 262, 6795, 290, 8722, 286, 777, 2683, 11, 262, 5114, 286, 1123, 318, 6646, 15223, 290, 28991, 13, 198, 21906, 198, 8001, 9542, 9345, 25, 8007, 1817, 11, 9855, 8656, 11, 290, 3948, 590, 32265, 82, 198, 8001, 9542, 4430, 357, 20185, 27920, 31722, 2972, 5050, 286, 4572, 4673, 357, 5805, 27920, 14150, 925, 20533, 14901, 287, 262, 1613, 1178, 812, 287, 5479, 355, 10084, 355, 2712, 3716, 1830, 11, 5001, 10763, 11, 3303, 7587, 11, 4046, 9465, 290, 21263, 11, 2939, 11795, 11, 290, 16324, 9465, 13, 2312, 14901, 423, 3181, 257, 13853, 286, 2968, 11, 35227, 11, 290, 2450, 3241, 284, 262, 2214, 11, 1390, 1111, 14067, 546, 14486, 4034, 290, 2328, 546, 26877, 12751, 290, 7476, 13, 371, 36730, 714, 15058, 832, 617, 6087, 286, 23221, 11, 17412, 393, 22053, 779, 11, 355, 880, 355, 832, 262, 2938, 1919, 290, 1964, 19911, 422, 262, 2866, 290, 5046, 286, 2458, 13, 198, 25396, 1843, 12751, 286, 9552, 2837, 422, 262, 7103, 290, 1948, 284, 262, 5909, 290, 43590, 13, 2893, 749, 1459, 31950, 290, 2450, 14604, 319, 9552, 12751, 9405, 1474, 12, 4354, 14901, 290, 4786, 11, 2968, 5504, 389, 13354, 416, 21002, 13858, 286, 28954, 7432, 284, 1692, 9441, 393, 21851, 11, 1690, 7867, 416, 19812, 5504, 287, 543, 9552, 468, 6190, 284, 2276, 2208, 12, 32683, 11, 4795, 2322, 653, 11, 393, 617, 584, 20533, 286, 9889, 7548, 284, 23353, 883, 286, 5384, 13, 25516, 9317, 546, 262, 14955, 290, 10576, 286, 884, 3257, 14901, 7565, 6768, 13, 17, 6430, 340, 318, 635, 6481, 1598, 326, 884, 3257, 14901, 287, 12971, 389, 407, 3306, 329, 9552, 284, 423, 43590, 26877, 12751, 960, 1640, 922, 393, 2801, 11, 393, 517, 1884, 329, 1111, 960, 8201, 262, 6034, 286, 6049, 44365, 13, 198, 36, 487, 2096, 284, 6687, 26877, 12751, 286, 3037, 1464, 1986, 2769, 36553, 11, 1111, 546, 11257, 287, 6276, 9889, 290, 546, 703, 484, 481, 307, 973, 287, 1919, 4732, 13, 2312, 39983, 6459, 389, 772, 3744, 329, 9552, 621, 329, 584, 2274, 3006, 286, 14614, 2328, 11, 2233, 284, 663, 42864, 11, 2248, 576, 2095, 11, 1913, 2792, 1095, 351, 3294, 3006, 286, 14614, 5963, 11, 290, 32483, 290, 9573, 286, 2785, 3586, 3006, 13, 18, 554, 663, 43936, 290, 6196, 43590, 2095, 11, 9552, 468, 587, 2600, 3193, 3688, 284, 262, 6643, 286, 2180, 7593, 37888, 11, 8744, 290, 12584, 18017, 13, 19, 198, 818, 1570, 286, 777, 6459, 11, 3781, 290, 7734, 286, 9552, 447, 247, 82, 1919, 12751, 290, 663, 18848, 423, 19960, 284, 13946, 379, 734, 886, 13033, 287, 2846, 286, 262, 2729, 1590, 290, 5046, 286, 262, 4786, 484, 2074, 13, 4042, 1459, 670, 6670, 1944, 393, 3393, 14486, 5479, 11, 884, 355, 18284, 5672, 290, 8385, 9383, 2551, 12, 11284, 3341, 287, 4301, 5316, 11, 1535, 12, 6651, 11, 7184, 11, 290, 3707, 11, 13593, 1541, 1944, 4786, 546, 3747, 11, 12247, 11, 6782, 11, 10690, 11, 290, 2233, 1429, 13, 20, 317, 10758, 263, 9137, 286, 1459, 670, 2925, 284, 262, 6697, 3257, 11, 17272, 284, 34404, 262, 10939, 286, 617, 2003, 36123, 286, 12971, 960, 16668, 12, 600, 32940, 9552, 11, 393, 11666, 2276, 4430, 357, 4760, 40, 828, 329, 1672, 960, 4480, 31277, 7476, 284, 1692, 9441, 393, 21851, 13, 770, 6846, 670, 3407, 4040, 284, 5911, 290, 1205, 6276, 9695, 326, 561, 787, 9552, 12373, 306, 3338, 11, 32293, 11, 393, 564, 250, 13120, 447, 251, 329, 5384, 11, 645, 2300, 703, 3665, 340, 4329, 25, 287, 1245, 11, 6095, 8472, 357, 392, 25741, 12, 5787, 8, 15075, 947, 284, 1081, 44273, 447, 247, 82, 7683, 21153, 286, 47061, 13, 21, 198, 464, 3154, 2837, 326, 7363, 1022, 777, 734, 23163, 11, 2158, 960, 1169, 12751, 11, 7476, 11, 290, 18848, 6459, 286, 9552, 326, 389, 19898, 287, 640, 12, 9888, 290, 14735, 1022, 262, 7103, 290, 262, 28954, 960, 14508, 10732, 262, 2785, 329, 43590, 26877, 12751, 290, 44365, 11, 329, 922, 290, 2801, 13, 6430, 3805, 22688, 617, 4922, 286, 7981, 290, 30654, 13367, 11, 428, 19898, 2837, 468, 2722, 1342, 3241, 13, 22, 770, 19898, 2837, 286, 9552, 5479, 290, 12751, 318, 34804, 1346, 6454, 42864, 287, 663, 13215, 11, 475, 460, 307, 24870, 306, 18876, 11, 379, 1551, 3721, 935, 11, 422, 1111, 262, 8713, 290, 262, 7103, 13, 383, 12941, 422, 8713, 11, 18032, 414, 12, 5363, 4786, 318, 5365, 2829, 25, 287, 428, 3095, 12, 9521, 11, 9552, 5479, 389, 991, 739, 1692, 1630, 13, 23, 198, 464, 12941, 286, 3095, 12, 9521, 422, 7103, 4786, 318, 13284, 1754, 11, 1865, 460, 307, 3616, 2759, 7428, 287, 2846, 286, 8354, 286, 1630, 13, 554, 1459, 290, 13301, 1474, 12, 4354, 3544, 11, 9552, 5479, 18595, 11, 35016, 11, 393, 6330, 4683, 10544, 357, 64, 1048, 11, 2597, 11, 393, 4009, 8, 287, 4683, 5370, 13, 1119, 389, 14553, 287, 3186, 290, 2594, 29450, 416, 4683, 9611, 284, 5174, 4297, 13, 1119, 1104, 393, 6330, 1692, 13572, 287, 5370, 783, 2077, 416, 1981, 5384, 11, 393, 416, 4025, 2628, 393, 5745, 357, 10215, 1819, 602, 11, 8028, 11, 11490, 11, 3503, 2014, 326, 389, 8018, 290, 2714, 16689, 588, 3925, 13, 887, 428, 22440, 1022, 9552, 5479, 290, 662, 12, 25687, 10544, 290, 5370, 318, 15074, 25477, 11, 290, 761, 407, 21160, 355, 9552, 9889, 4292, 13, 554, 262, 7090, 3381, 11, 9552, 714, 307, 12380, 284, 466, 1243, 326, 6454, 22464, 1944, 10544, 447, 247, 5370, 11, 475, 379, 884, 9902, 5046, 393, 8354, 326, 511, 12751, 389, 4140, 48668, 3421, 11, 416, 11, 329, 1672, 11, 11581, 10544, 447, 247, 1176, 11, 25449, 511, 6958, 11, 393, 15882, 649, 4661, 13, 25929, 11, 9552, 714, 307, 12380, 284, 466, 1243, 407, 783, 1760, 416, 597, 2060, 8674, 11, 475, 416, 4025, 12, 9888, 1919, 7767, 393, 7686, 11, 884, 355, 5939, 11, 46219, 3341, 11, 42864, 1729, 12, 12001, 1143, 6712, 11, 393, 262, 3230, 1080, 13, 775, 460, 21786, 2003, 9552, 3341, 8569, 2280, 32029, 960, 392, 14572, 17272, 284, 27183, 960, 439, 5370, 925, 416, 290, 1626, 1588, 3716, 5745, 13, 1114, 1672, 11, 356, 1244, 21786, 9552, 564, 250, 20270, 447, 251, 21750, 11, 262, 3482, 2351, 3893, 4809, 11, 262, 1812, 286, 3442, 11, 393, 355, 314, 7301, 287, 428, 3348, 11, 262, 2104, 3773, 13, 34706, 276, 379, 884, 16252, 11, 9552, 561, 1011, 10906, 326, 389, 783, 9569, 355, 3165, 6783, 6608, 11, 1602, 22282, 7496, 11, 393, 584, 19428, 3675, 262, 3151, 286, 597, 1981, 2551, 393, 29024, 1630, 11, 290, 2426, 606, 284, 22706, 1630, 11, 6778, 1483, 11, 290, 357, 39363, 8, 1193, 3299, 290, 18241, 13, 25809, 290, 18848, 286, 9552, 12751, 287, 428, 19898, 2837, 561, 11, 517, 4084, 621, 329, 2035, 7103, 393, 18032, 414, 12, 5363, 4786, 11, 2421, 9110, 286, 1111, 262, 6276, 9695, 286, 9552, 3341, 290, 262, 1919, 11, 3034, 11, 290, 1964, 4732, 287, 543, 484, 389, 4166, 290, 973, 13, 198, 32, 27522, 29544, 25, 9552, 12, 47, 10387, 5694, 11279, 21913, 198, 2514, 7301, 777, 12779, 11, 428, 3348, 21126, 257, 1807, 6306, 326, 10718, 38590, 287, 428, 3504, 2837, 25, 10347, 9552, 1057, 262, 3773, 11, 13586, 26512, 5370, 416, 1910, 10544, 30, 10347, 617, 19756, 36804, 21417, 286, 8902, 19988, 9552, 290, 1366, 9889, 1620, 262, 8271, 20157, 290, 19877, 5499, 286, 5939, 960, 1169, 5499, 326, 29112, 4289, 4318, 5410, 3341, 7482, 290, 523, 14660, 4054, 379, 960, 392, 466, 340, 1365, 621, 2035, 1613, 5410, 3341, 393, 5939, 30, 198, 7003, 428, 5517, 318, 28991, 11, 612, 389, 379, 1551, 1115, 3840, 326, 340, 318, 24769, 11, 1111, 355, 281, 9028, 13936, 351, 2769, 6754, 23082, 290, 6452, 1459, 3664, 6160, 290, 329, 663, 8472, 10939, 13, 3274, 11, 340, 3769, 257, 21002, 20936, 286, 262, 6196, 43590, 2928, 286, 9552, 9889, 326, 1650, 287, 428, 3504, 2837, 11, 407, 10616, 2276, 393, 2208, 12, 600, 32940, 9552, 3341, 13, 9676, 11, 1290, 422, 852, 4114, 8717, 3193, 2709, 14209, 11, 663, 20505, 318, 13975, 284, 867, 584, 31316, 19887, 11, 329, 922, 393, 2801, 11, 286, 6196, 43590, 9552, 5479, 13, 24, 5498, 11, 340, 4394, 649, 22582, 319, 2769, 11, 24056, 2683, 286, 1919, 11, 1964, 11, 290, 2742, 4583, 11, 884, 355, 262, 6770, 286, 1919, 9490, 11, 262, 2776, 1022, 3034, 290, 2614, 12354, 11, 3026, 22801, 1042, 11, 262, 2776, 1022, 262, 1910, 3773, 290, 262, 1181, 11, 290, 262, 13215, 1022, 1981, 22008, 290, 1181, 393, 584, 10098, 4934, 13, 383, 12069, 30375, 7786, 1459, 1964, 36512, 11, 355, 5801, 4371, 287, 9552, 15381, 262, 2323, 739, 9775, 10282, 2683, 884, 355, 262, 6082, 286, 3034, 18201, 1022, 4827, 290, 3139, 11, 262, 12751, 286, 3034, 10368, 11, 290, 262, 6082, 286, 1176, 287, 3592, 13, 940, 10467, 11, 428, 318, 257, 2785, 9552, 3586, 3025, 6573, 1188, 594, 318, 407, 3489, 257, 3161, 72, 475, 2138, 27102, 290, 25477, 11, 407, 4084, 10609, 284, 2035, 471, 4852, 666, 393, 23524, 11338, 666, 31082, 475, 6196, 6007, 286, 6225, 287, 2035, 4571, 13, 632, 4145, 3769, 5527, 2323, 329, 12069, 656, 663, 6948, 290, 262, 3403, 326, 561, 26500, 3812, 2035, 26877, 4034, 393, 34859, 11, 286, 2176, 5107, 393, 287, 19406, 11, 290, 12891, 743, 1950, 11154, 329, 1474, 12, 4354, 2450, 290, 2742, 9109, 13, 198, 8421, 1972, 656, 3307, 11, 314, 11589, 2209, 262, 2071, 286, 644, 1438, 284, 1577, 262, 9552, 508, 15445, 82, 428, 1049, 1176, 13, 314, 18077, 564, 250, 11518, 13, 447, 251, 9754, 663, 584, 27494, 11, 564, 250, 11518, 447, 251, 318, 1037, 2759, 5279, 12, 4131, 29709, 851, 4360, 340, 852, 13130, 11, 5436, 635, 2476, 43947, 13, 3423, 11, 314, 804, 736, 878, 2274, 10993, 874, 286, 48110, 12, 44548, 317, 3792, 355, 4048, 357, 1640, 1672, 11, 2332, 11, 1475, 36781, 828, 284, 734, 41532, 422, 257, 3161, 2278, 286, 1919, 46907, 25, 46354, 290, 19635, 447, 247, 82, 42968, 4751, 290, 772, 2252, 736, 11, 284, 9817, 15839, 1653, 13, 1157, 4650, 286, 514, 481, 307, 1762, 329, 5436, 11, 611, 356, 389, 1762, 379, 477, 11, 523, 5436, 318, 4084, 564, 250, 464, 1869, 447, 251, 960, 392, 3011, 27543, 43947, 13, 198, 11518, 481, 423, 734, 1263, 13391, 625, 5939, 287, 11560, 1692, 9490, 11, 1111, 6948, 286, 262, 1109, 326, 465, 14748, 286, 1692, 9490, 561, 307, 21391, 290, 7952, 11, 2138, 621, 12913, 290, 3165, 6783, 13, 11317, 621, 9489, 257, 900, 286, 10730, 11, 26512, 11, 2839, 41446, 422, 543, 530, 1276, 26342, 564, 250, 259, 23504, 1021, 447, 251, 9156, 284, 6818, 922, 19406, 10906, 11, 5436, 561, 1620, 257, 3298, 1919, 23989, 13, 770, 561, 7139, 683, 284, 3376, 1910, 15536, 13, 770, 1724, 11, 717, 11, 326, 5436, 460, 5387, 1096, 477, 7097, 871, 11, 29927, 1111, 1910, 290, 1729, 12, 10728, 1321, 284, 5911, 290, 4659, 7097, 3048, 290, 3031, 20431, 960, 361, 407, 329, 477, 11, 788, 379, 1551, 329, 262, 749, 2726, 290, 28605, 7287, 7097, 871, 11, 884, 355, 6142, 34859, 11, 8271, 42435, 11, 625, 12, 1904, 286, 36523, 11, 290, 262, 739, 12, 5589, 641, 515, 1919, 4034, 286, 1535, 11, 3707, 11, 3725, 11, 262, 10848, 11, 290, 23265, 6712, 13, 5436, 714, 3376, 262, 13045, 286, 12584, 18017, 11, 14240, 3186, 11, 290, 1660, 11, 290, 262, 17058, 286, 7799, 11, 20669, 11, 290, 1919, 3259, 13, 198, 12211, 11, 5436, 714, 4646, 393, 11005, 1910, 1176, 290, 262, 3917, 5602, 12, 38515, 4069, 13, 12101, 1692, 12, 39935, 9611, 11, 5436, 561, 407, 7030, 3626, 2111, 284, 2251, 18118, 850, 12, 8738, 4402, 1910, 1176, 11, 393, 284, 6482, 28393, 393, 3484, 739, 3403, 286, 4683, 11, 10095, 1910, 1176, 784, 2845, 44061, 355, 777, 15381, 7599, 2222, 19406, 4034, 13, 2312, 13391, 15714, 5436, 1111, 422, 5899, 1910, 14752, 290, 422, 6754, 6370, 379, 4318, 5410, 11, 543, 550, 511, 2832, 517, 621, 1336, 2391, 2111, 284, 6687, 3227, 290, 651, 5939, 284, 1598, 13, 2011, 2962, 319, 777, 13391, 635, 45482, 5436, 422, 584, 11628, 329, 4318, 5410, 1912, 319, 31350, 14901, 11, 543, 423, 24399, 3154, 1919, 12031, 884, 355, 10537, 11, 26809, 11, 290, 10518, 10270, 475, 423, 407, 3111, 832, 262, 8472, 871, 286, 703, 262, 5150, 3341, 561, 2987, 319, 1910, 10906, 287, 19988, 777, 12031, 13, 1065, 198, 464, 3348, 15740, 355, 5679, 13, 2142, 314, 3769, 257, 4506, 6754, 4469, 319, 262, 1808, 286, 4318, 5410, 11, 262, 1388, 7159, 329, 290, 1028, 340, 11, 290, 262, 3840, 326, 2406, 14901, 287, 9552, 290, 3519, 8514, 743, 6121, 262, 2071, 13, 7275, 2873, 12628, 689, 262, 4876, 286, 564, 250, 20270, 262, 3773, 11, 447, 251, 4737, 644, 340, 1244, 1612, 10017, 306, 290, 644, 4469, 14895, 1276, 307, 7368, 284, 787, 2565, 286, 340, 11, 788, 26017, 1115, 5559, 4981, 286, 703, 5436, 1244, 8076, 13, 7275, 6711, 788, 3607, 257, 15223, 17548, 286, 1811, 2428, 290, 6459, 4376, 416, 5436, 11, 1390, 5436, 447, 247, 82, 1366, 2476, 11, 10939, 329, 1919, 9573, 290, 11044, 11, 262, 1917, 286, 16215, 5436, 447, 247, 82, 9432, 2163, 11, 290, 262, 17262, 286, 703, 5436, 1244, 1282, 546, 11, 355, 880, 355, 644, 284, 466, 546, 606, 13, 198, 1212, 12069, 10969, 262, 1598, 2526, 286, 30190, 625, 257, 5909, 10747, 290, 4145, 7464, 510, 1111, 28991, 290, 31194, 13, 1675, 5421, 262, 12069, 290, 1037, 4179, 428, 2526, 11, 290, 284, 15714, 428, 422, 281, 5517, 287, 14614, 41164, 11, 314, 8814, 319, 1811, 7952, 7106, 4035, 14895, 13, 383, 717, 290, 749, 1593, 286, 777, 318, 281, 13196, 286, 31350, 12971, 13, 1114, 597, 31350, 4876, 5981, 284, 262, 5046, 286, 262, 1917, 11, 564, 250, 20270, 262, 3773, 447, 251, 960, 5242, 284, 13188, 286, 661, 11, 290, 257, 2092, 393, 6454, 4025, 1502, 286, 2785, 7017, 11, 17311, 11, 290, 3227, 290, 6082, 5370, 1485, 851, 11518, 460, 466, 340, 13, 1318, 318, 645, 12765, 32315, 287, 31350, 5339, 11, 19484, 11, 393, 8385, 9383, 2694, 284, 27183, 257, 880, 12, 23599, 9432, 2163, 25, 777, 389, 9672, 284, 307, 287, 15822, 11, 6840, 1479, 5127, 13, 770, 13196, 11, 8197, 329, 339, 27915, 4959, 11, 635, 45482, 428, 5517, 422, 262, 867, 4040, 284, 34404, 262, 31350, 13357, 286, 262, 3773, 3585, 284, 5545, 393, 13301, 14492, 1176, 11, 2035, 284, 10176, 393, 4968, 262, 40460, 286, 1630, 13, 1415, 314, 2391, 7048, 262, 3306, 5339, 11, 2421, 691, 326, 262, 13196, 1208, 617, 10926, 11387, 286, 48923, 2247, 11, 788, 670, 832, 663, 10939, 13, 1400, 884, 7106, 4035, 13196, 460, 307, 925, 11, 2158, 11, 329, 262, 1366, 5436, 2476, 284, 466, 465, 1693, 11, 543, 318, 4318, 284, 262, 12069, 290, 2314, 307, 12470, 1021, 12, 86, 9586, 1497, 13, 45344, 284, 584, 29964, 12, 5363, 4133, 11, 5270, 290, 6082, 286, 5981, 1366, 318, 517, 2408, 11, 517, 25477, 319, 1919, 290, 3034, 3403, 11, 517, 10795, 319, 5436, 447, 247, 82, 7141, 1693, 6764, 11, 290, 44020, 517, 7634, 351, 584, 11, 1729, 12, 17079, 3815, 326, 389, 357, 265, 1551, 287, 663, 4238, 20855, 8, 2354, 5436, 447, 247, 82, 1693, 6764, 13, 10664, 276, 1366, 11, 290, 262, 17778, 290, 10939, 286, 1972, 340, 11, 389, 1871, 262, 2428, 6693, 287, 7275, 6711, 13, 383, 3348, 20612, 351, 4506, 13242, 290, 2683, 329, 2252, 3645, 13, 198, 40, 13, 23121, 30532, 25, 383, 21773, 2199, 14902, 41029, 198, 818, 262, 29112, 12, 14792, 9028, 6531, 1022, 262, 49714, 6027, 11, 31454, 15889, 2585, 290, 262, 7270, 15098, 40416, 11, 734, 4096, 7159, 547, 6190, 1028, 19803, 13, 383, 717, 373, 1912, 319, 12354, 290, 3519, 46219, 3667, 546, 262, 1774, 8354, 286, 1181, 4934, 3585, 284, 4290, 11, 749, 18939, 5670, 319, 262, 2776, 1022, 3119, 2489, 290, 3026, 290, 1964, 2489, 13, 383, 1181, 2314, 1630, 262, 1724, 286, 3227, 1231, 11071, 21597, 45121, 15520, 319, 262, 22008, 286, 4290, 13, 770, 19976, 318, 46219, 290, 43936, 11, 4795, 286, 262, 1181, 286, 3037, 393, 584, 25477, 2587, 3403, 13, 1314, 383, 1218, 4578, 373, 1912, 319, 2307, 1387, 960, 1169, 2694, 286, 1181, 5410, 3341, 284, 18306, 4439, 262, 7017, 290, 2594, 326, 661, 765, 13, 33385, 286, 4318, 5410, 7189, 326, 645, 2300, 703, 6007, 262, 2828, 2491, 262, 1080, 393, 262, 4133, 379, 511, 18264, 11, 4318, 5410, 714, 407, 2872, 262, 2854, 286, 26512, 5370, 287, 5939, 11, 475, 561, 307, 34893, 1927, 39785, 351, 28791, 11, 2984, 439, 20968, 11, 290, 45393, 969, 489, 2664, 13, 12101, 262, 717, 19976, 11, 428, 530, 318, 25477, 319, 2176, 3403, 290, 9889, 13, 3412, 611, 340, 373, 2081, 329, 477, 1103, 4040, 379, 4318, 3034, 5410, 960, 292, 340, 2048, 1464, 373, 960, 5832, 460, 5967, 5559, 3403, 739, 543, 340, 1244, 407, 307, 2081, 13, 2011, 2962, 994, 318, 319, 428, 1218, 4578, 13, 198, 7003, 340, 468, 2961, 11135, 11, 428, 4578, 6348, 9208, 287, 262, 1903, 29112, 4289, 1708, 262, 3394, 5854, 13, 383, 749, 9208, 3098, 12, 11578, 768, 6299, 547, 416, 26985, 337, 2696, 357, 1129, 1828, 828, 14409, 284, 257, 5410, 1080, 25828, 290, 11476, 9177, 287, 1903, 1281, 12, 5767, 37313, 10312, 416, 30755, 3169, 333, 776, 357, 1129, 1129, 737, 1433, 9075, 988, 357, 41931, 8, 1568, 7786, 2945, 290, 7083, 26985, 337, 2696, 447, 247, 82, 19976, 11, 1558, 981, 262, 749, 9208, 46472, 282, 373, 416, 440, 8135, 283, 47579, 13, 26985, 337, 2696, 290, 9075, 988, 1111, 7189, 11, 287, 1180, 2842, 11, 326, 262, 29163, 3403, 3306, 329, 7606, 5939, 284, 1598, 290, 4620, 511, 4752, 1919, 4034, 714, 407, 307, 8793, 416, 4318, 5410, 780, 262, 1321, 2622, 284, 466, 523, 318, 691, 1695, 30240, 287, 262, 4536, 326, 14740, 422, 26512, 1910, 12213, 287, 7606, 29163, 357, 273, 517, 23162, 306, 832, 13805, 70, 372, 7606, 12213, 11, 772, 13717, 2818, 7606, 29163, 737, 198, 39276, 26985, 337, 2696, 447, 247, 82, 4238, 2643, 286, 428, 21554, 11, 47579, 3751, 326, 612, 318, 645, 13054, 287, 7989, 284, 262, 976, 6436, 1483, 3403, 4635, 416, 7606, 12213, 852, 28681, 416, 4318, 4571, 11, 17455, 416, 257, 900, 286, 9082, 4536, 2712, 257, 2597, 10730, 284, 326, 286, 1910, 4536, 13, 47579, 772, 5150, 257, 8472, 1429, 286, 29497, 11, 4473, 12, 392, 12, 18224, 15068, 416, 543, 33596, 714, 1064, 1910, 12, 2375, 1723, 4536, 11, 34657, 284, 262, 2839, 12, 10728, 15068, 1429, 5150, 416, 6445, 8847, 13, 1507, 198, 31306, 988, 788, 7786, 2945, 262, 19976, 11, 11810, 326, 772, 611, 33596, 714, 287, 4583, 24340, 5939, 447, 247, 18118, 16586, 20157, 11, 262, 5046, 286, 262, 2672, 1366, 290, 29964, 925, 262, 4876, 5340, 287, 3357, 960, 31722, 6402, 262, 5909, 11, 3734, 12, 2164, 1328, 9573, 286, 3403, 739, 543, 661, 48878, 357, 12393, 12, 727, 27563, 1040, 11, 2063, 2756, 26290, 290, 262, 6382, 1042, 286, 1910, 3403, 351, 43440, 761, 329, 5801, 16895, 13, 47579, 447, 247, 82, 2882, 11, 3199, 1281, 17047, 3481, 287, 15904, 11, 6974, 5081, 326, 14901, 287, 14492, 15111, 262, 1917, 23498, 11, 772, 2562, 13, 1129, 198, 7003, 262, 1903, 9196, 286, 428, 564, 250, 14557, 396, 17952, 447, 251, 4384, 5091, 878, 262, 2478, 286, 3660, 9061, 11, 5801, 14901, 287, 29964, 290, 287, 23989, 16113, 960, 11085, 1262, 15075, 4410, 326, 3170, 319, 35382, 14901, 287, 10075, 9833, 1630, 11, 788, 351, 4875, 4410, 706, 262, 3095, 12, 42751, 82, 960, 45956, 515, 306, 3421, 262, 4732, 329, 8840, 9196, 11, 18244, 517, 287, 4583, 621, 287, 3357, 13, 383, 5358, 1022, 12330, 369, 2527, 652, 29965, 960, 31306, 988, 447, 247, 82, 19190, 286, 50097, 11, 47579, 447, 247, 82, 286, 5885, 960, 9776, 36501, 349, 23765, 11, 355, 340, 33785, 2402, 542, 1571, 1020, 5768, 546, 2003, 13312, 287, 14614, 12971, 13, 843, 981, 5801, 8282, 14901, 287, 1111, 9061, 290, 16113, 1201, 262, 11445, 82, 40216, 27458, 11776, 326, 262, 2846, 286, 262, 4384, 550, 17640, 3421, 11, 1238, 612, 373, 645, 10017, 2370, 326, 257, 1688, 11387, 286, 12971, 550, 587, 12606, 13, 9676, 11, 262, 5410, 1917, 318, 17338, 739, 12, 23599, 326, 340, 318, 407, 1598, 10582, 644, 1241, 393, 2099, 286, 14492, 4133, 561, 954, 355, 262, 5981, 11387, 13, 11214, 11, 262, 10017, 3034, 290, 10039, 5373, 286, 262, 7270, 40416, 625, 262, 7570, 24003, 11, 290, 262, 3489, 5287, 286, 4036, 6370, 379, 4318, 5410, 11, 2481, 925, 262, 1808, 1283, 555, 47914, 13, 198, 464, 4384, 4145, 3332, 43264, 960, 392, 15242, 36501, 349, 23765, 851, 1640, 4647, 13, 47579, 447, 247, 82, 373, 262, 12841, 4578, 329, 15889, 5410, 11, 475, 465, 6482, 284, 21024, 4536, 2138, 621, 17794, 11, 290, 465, 4305, 2457, 7017, 290, 4827, 5939, 2354, 465, 5410, 1080, 11, 1364, 465, 6961, 281, 5629, 11, 739, 12, 23599, 14554, 13, 2399, 6961, 373, 12318, 1111, 422, 262, 1364, 329, 407, 852, 15889, 1576, 290, 9894, 284, 9149, 1919, 10537, 290, 10518, 10270, 11, 1828, 290, 422, 262, 826, 329, 13148, 2818, 11, 22706, 4081, 2882, 284, 33596, 447, 247, 34819, 290, 329, 9894, 284, 1848, 329, 262, 16538, 286, 11663, 290, 17038, 13, 1954, 23591, 319, 7822, 3307, 326, 47579, 750, 407, 11986, 11, 2035, 19976, 960, 273, 1111, 960, 11261, 423, 587, 4938, 13, 10968, 11, 262, 7159, 625, 31350, 40460, 1022, 47579, 290, 9188, 884, 355, 9075, 988, 290, 21438, 78, 494, 2900, 319, 11780, 555, 332, 16823, 14895, 546, 2003, 6276, 4371, 290, 663, 1919, 4732, 11, 1731, 543, 547, 407, 2426, 284, 21594, 6323, 13, 198, 12510, 1290, 12, 30771, 2274, 2458, 287, 3403, 11, 2158, 11, 787, 340, 257, 4465, 640, 284, 6411, 32302, 262, 1808, 13, 3274, 11, 14901, 287, 9552, 290, 4572, 4673, 11, 287, 10730, 351, 5801, 7118, 287, 6890, 12, 3106, 31350, 5339, 13, 5498, 11, 262, 11278, 287, 6115, 11, 23861, 414, 11, 290, 42863, 286, 1366, 11, 3573, 262, 10095, 290, 3665, 779, 286, 15741, 1366, 355, 14297, 4331, 669, 329, 1243, 326, 2314, 307, 6515, 3264, 25, 329, 1672, 11, 7172, 15387, 11, 14479, 290, 12483, 1756, 11, 290, 9420, 3458, 284, 1964, 6218, 13, 843, 2368, 11, 262, 3349, 286, 850, 12, 10057, 82, 286, 262, 3773, 960, 12417, 306, 1626, 1588, 11521, 9611, 290, 3272, 12, 69, 2533, 7686, 960, 5562, 8076, 416, 4318, 4571, 739, 8385, 9383, 1630, 11, 2138, 621, 1692, 5370, 14409, 284, 1910, 3403, 13, 1495, 2312, 2380, 1588, 14807, 286, 5410, 326, 4031, 284, 27183, 2839, 11, 2138, 621, 1919, 11, 9432, 5499, 13, 4698, 777, 11257, 11, 612, 468, 587, 617, 26624, 286, 262, 5410, 4384, 11, 3584, 351, 281, 14855, 13542, 284, 302, 12, 3642, 395, 1468, 2683, 1231, 2176, 8787, 284, 2274, 4371, 13, 4900, 262, 749, 31316, 13936, 286, 777, 2428, 468, 587, 287, 28991, 10165, 11, 2075, 612, 318, 635, 4075, 4384, 319, 262, 1364, 546, 262, 40460, 290, 748, 343, 1799, 286, 28910, 4318, 5410, 1912, 319, 3660, 14492, 13, 1983, 198, 3978, 13, 1374, 10928, 25882, 5521, 30, 198, 32, 13, 47570, 286, 5436, 25, 25353, 2195, 388, 8544, 198, 2437, 881, 857, 5436, 1630, 30, 1867, 857, 564, 250, 5143, 262, 3773, 447, 251, 1612, 30, 3914, 447, 247, 82, 7048, 5436, 1839, 447, 247, 83, 307, 6019, 20482, 1692, 4086, 11, 5149, 2506, 644, 284, 466, 477, 262, 640, 25, 326, 857, 407, 1283, 19874, 351, 262, 3061, 286, 19988, 1692, 9490, 13, 3244, 625, 644, 4036, 5370, 318, 339, 1813, 4934, 30, 775, 2221, 284, 3164, 428, 1808, 416, 2263, 5436, 447, 247, 82, 1693, 6764, 6411, 25, 5436, 564, 250, 48381, 262, 3773, 11, 447, 251, 257, 6764, 326, 906, 8139, 262, 3773, 318, 407, 477, 286, 3592, 11, 475, 318, 18876, 1111, 422, 262, 1181, 11, 290, 422, 617, 7667, 900, 286, 1729, 12, 17079, 1919, 12213, 290, 14752, 13, 3914, 447, 247, 82, 22111, 5039, 326, 262, 3773, 318, 262, 900, 286, 7767, 11, 6712, 11, 290, 6593, 326, 1630, 703, 7017, 290, 2594, 389, 4635, 11, 22112, 11, 290, 13529, 13, 2078, 198, 1722, 314, 13986, 3617, 262, 1807, 6306, 284, 787, 5436, 517, 10017, 290, 2176, 11, 379, 1811, 2173, 287, 262, 4578, 3224, 14895, 481, 307, 2622, 11, 2035, 546, 262, 6770, 290, 13215, 286, 5436, 447, 247, 82, 1693, 393, 546, 262, 1919, 290, 1964, 4732, 287, 543, 5436, 14051, 13, 2011, 12031, 287, 1642, 777, 14895, 960, 1462, 1394, 262, 5517, 3499, 290, 6196, 5981, 329, 1474, 12, 4354, 5370, 960, 10594, 1950, 257, 1178, 2173, 286, 339, 27915, 11154, 287, 644, 14895, 389, 749, 4465, 13, 3274, 11, 1719, 1541, 9672, 645, 31350, 17778, 314, 481, 1949, 407, 284, 20528, 287, 3224, 14895, 546, 5436, 447, 247, 82, 12971, 326, 45131, 262, 357, 324, 43011, 9155, 8, 22303, 286, 48923, 2247, 314, 716, 2111, 284, 5529, 13, 5498, 11, 1201, 262, 4007, 286, 5436, 318, 284, 5963, 1692, 9490, 11, 287, 31577, 703, 5436, 2499, 314, 481, 3368, 7747, 326, 1057, 7634, 1028, 10678, 1692, 15387, 290, 3815, 960, 4480, 262, 734, 47155, 11, 286, 1781, 11, 326, 15387, 290, 3815, 743, 1487, 11, 290, 326, 2003, 1964, 3403, 743, 2661, 29682, 4036, 9552, 12, 3106, 5410, 3341, 287, 2842, 326, 466, 407, 9494, 1692, 9490, 13, 9461, 11, 428, 1807, 6306, 318, 5292, 284, 4691, 355, 257, 8883, 5517, 960, 64, 6764, 290, 3781, 286, 8627, 2003, 3403, 3025, 4007, 318, 284, 4175, 1474, 12, 4354, 7747, 13, 1959, 1629, 617, 2173, 11, 428, 4007, 12444, 284, 2661, 13148, 1342, 11982, 26877, 38226, 11, 287, 1502, 284, 5529, 23082, 290, 24216, 351, 1474, 12, 4354, 5370, 290, 2267, 15369, 13, 24581, 11, 314, 28251, 284, 787, 777, 14895, 7952, 11, 290, 284, 3465, 810, 584, 7747, 1244, 307, 12470, 19756, 13, 1114, 262, 749, 636, 11, 314, 3853, 655, 530, 3108, 832, 262, 15715, 5509, 286, 12779, 11, 351, 4506, 13050, 319, 2785, 5559, 13532, 475, 4632, 4305, 777, 284, 2252, 2478, 287, 2003, 670, 13, 198, 464, 717, 286, 777, 2672, 14895, 4786, 262, 8354, 286, 5436, 447, 247, 82, 4934, 25, 287, 1948, 644, 4934, 339, 561, 423, 625, 7327, 13, 10928, 5436, 1560, 661, 644, 284, 4483, 11, 5806, 11, 466, 11, 810, 284, 467, 329, 8073, 393, 14600, 30, 314, 7048, 326, 339, 857, 407, 11, 475, 2138, 326, 661, 991, 787, 511, 898, 7327, 5370, 13, 314, 787, 428, 3572, 11476, 355, 257, 2276, 1634, 422, 616, 898, 15387, 13, 314, 836, 447, 247, 83, 588, 852, 1297, 644, 284, 15000, 11, 1111, 503, 286, 281, 28327, 12741, 329, 21851, 290, 780, 1854, 508, 1949, 1690, 651, 616, 15387, 2642, 13, 770, 318, 635, 11476, 257, 6573, 3572, 960, 1169, 21721, 286, 7327, 7747, 351, 4096, 12354, 5353, 318, 1165, 1913, 284, 1577, 510, 11, 290, 314, 5490, 326, 9616, 661, 1577, 510, 428, 21851, 11, 772, 611, 3360, 11282, 11, 743, 307, 27294, 351, 1692, 46240, 13, 1270, 843, 340, 318, 11476, 546, 5436, 447, 247, 82, 1321, 2476, 960, 49827, 3572, 3769, 17451, 6153, 1321, 546, 15387, 11, 543, 5436, 2476, 290, 743, 691, 307, 1498, 284, 651, 416, 21769, 12748, 25805, 7747, 13, 11317, 621, 31577, 7327, 11, 5436, 481, 466, 644, 262, 3773, 1541, 857, 960, 67, 2357, 3810, 262, 3689, 1695, 284, 502, 11, 351, 38356, 3403, 286, 640, 290, 1295, 960, 392, 2148, 5981, 1321, 290, 11776, 13, 3132, 198, 32, 1218, 2622, 7106, 4035, 13196, 4786, 35501, 9051, 20038, 13, 1675, 1394, 262, 1807, 6306, 5981, 284, 1459, 5370, 290, 7310, 422, 471, 4852, 666, 10165, 960, 5661, 318, 407, 314, 391, 19566, 447, 247, 82, 17346, 2624, 851, 40, 7048, 326, 6276, 4371, 468, 407, 15254, 35501, 13, 1406, 981, 7327, 318, 407, 7368, 393, 20232, 11, 6159, 857, 340, 8076, 355, 564, 250, 270, 447, 247, 82, 477, 1479, 11, 1011, 4232, 345, 765, 13, 447, 251, 2091, 42158, 7747, 3520, 31070, 11, 290, 597, 32315, 319, 2472, 7327, 326, 857, 407, 27861, 2176, 7747, 481, 22464, 257, 5385, 4466, 32315, 13, 770, 15565, 326, 772, 351, 5436, 2491, 262, 3773, 11, 13717, 3403, 286, 1281, 12, 1416, 32689, 6088, 612, 1276, 991, 307, 1637, 13, 314, 423, 257, 27454, 2033, 286, 340, 11, 3584, 356, 423, 407, 1865, 3177, 703, 314, 651, 340, 13, 843, 1243, 423, 4536, 960, 273, 379, 1551, 11, 2457, 7172, 7017, 423, 4536, 13, 775, 4398, 447, 247, 83, 1865, 3177, 5128, 5087, 393, 19898, 7017, 13, 198, 1212, 4006, 286, 8282, 35501, 45482, 262, 1807, 6306, 994, 422, 262, 749, 31316, 14614, 12, 10709, 396, 35066, 11, 543, 18633, 7048, 3037, 357, 296, 77, 541, 2028, 1366, 11, 513, 12, 35, 13570, 8, 481, 7716, 3403, 286, 44176, 20038, 11, 739, 543, 14461, 3484, 960, 392, 12891, 4536, 960, 1102, 332, 469, 3812, 6632, 13, 2682, 554, 6273, 284, 777, 26096, 11, 314, 7048, 326, 3227, 991, 4433, 2587, 17311, 11, 867, 286, 543, 481, 307, 287, 31070, 5127, 772, 351, 23392, 3227, 3037, 11, 3737, 6481, 17707, 31070, 11, 611, 5436, 447, 247, 82, 14833, 2058, 878, 1692, 14355, 27513, 3675, 262, 7095, 286, 262, 3668, 13, 8673, 262, 749, 21112, 32315, 319, 44176, 20038, 11, 2158, 11, 2058, 422, 1919, 7095, 284, 3349, 13, 2327, 1675, 262, 6287, 326, 867, 1243, 661, 6227, 3520, 2760, 1292, 393, 45203, 960, 4102, 364, 286, 3585, 1919, 3722, 326, 389, 48676, 31070, 960, 10197, 7138, 23392, 3227, 3037, 481, 407, 10980, 35501, 25, 262, 3061, 24875, 481, 2391, 1445, 13, 2080, 867, 1243, 661, 765, 991, 287, 3614, 5127, 11, 2233, 284, 597, 6087, 286, 2587, 11, 6142, 11, 290, 1919, 12, 301, 5620, 17778, 11, 262, 3773, 481, 991, 761, 281, 20157, 9030, 284, 5004, 508, 3011, 644, 13, 4900, 340, 743, 1011, 1180, 5107, 11, 428, 481, 804, 284, 7008, 588, 4536, 290, 257, 4466, 32315, 13, 198, 3152, 5436, 447, 247, 82, 4934, 3614, 284, 3227, 11, 1194, 13196, 318, 2622, 3393, 25, 2141, 661, 991, 670, 30, 1675, 2834, 262, 5517, 3812, 23082, 329, 1474, 12, 4354, 5370, 11, 314, 7048, 326, 5436, 11, 584, 9552, 3341, 11, 290, 14193, 423, 407, 6928, 477, 1692, 12973, 3842, 13, 4380, 991, 670, 11, 1390, 21543, 393, 12973, 670, 357, 16090, 284, 787, 1243, 584, 661, 765, 8, 355, 880, 355, 48676, 13338, 670, 4795, 286, 597, 3512, 329, 262, 5072, 13, 770, 1244, 307, 780, 9552, 290, 14193, 2314, 5244, 11218, 813, 466, 790, 1693, 290, 661, 389, 991, 2622, 11, 2623, 393, 780, 661, 765, 284, 670, 13, 383, 1271, 286, 661, 1762, 743, 307, 1290, 7380, 621, 1909, 475, 318, 407, 257, 7009, 1271, 13, 31779, 661, 389, 1762, 326, 477, 27123, 290, 11149, 606, 11, 290, 511, 14052, 290, 9490, 11, 1276, 307, 3177, 287, 703, 262, 3773, 4539, 13, 198, 3152, 5436, 2491, 262, 3773, 290, 661, 991, 1762, 11, 262, 1306, 13196, 2622, 318, 262, 3450, 286, 262, 18645, 290, 12213, 1022, 5436, 290, 1692, 3259, 26, 287, 1948, 11, 389, 612, 991, 9611, 30, 554, 4583, 11, 340, 318, 1744, 284, 423, 281, 3773, 1231, 9611, 13, 2718, 3887, 1692, 8383, 714, 307, 257, 6195, 38090, 273, 11, 24986, 351, 1854, 832, 38026, 1910, 8945, 13, 2548, 376, 8789, 389, 20316, 286, 1321, 11, 10033, 12, 25781, 2316, 11, 290, 16533, 286, 5046, 11, 543, 787, 340, 517, 6942, 284, 6431, 3259, 290, 4133, 2641, 5745, 351, 5387, 4560, 6856, 416, 13303, 498, 11, 46219, 11, 290, 357, 29471, 8, 4934, 6958, 2138, 621, 1910, 8945, 13, 198, 1890, 262, 1115, 14895, 6693, 4145, 1290, 11, 691, 530, 3038, 3568, 284, 1394, 262, 15758, 995, 6196, 18763, 290, 262, 1807, 6306, 5981, 290, 49948, 13, 5436, 6973, 3227, 11, 407, 7327, 26, 612, 318, 991, 35501, 290, 4145, 257, 761, 329, 617, 835, 284, 31935, 5072, 1871, 661, 26, 290, 661, 991, 670, 13, 1550, 1771, 9611, 991, 2152, 11, 2158, 11, 290, 262, 3519, 1808, 286, 703, 1692, 3259, 9427, 351, 5436, 11, 379, 1551, 734, 2663, 1656, 19756, 13, 3274, 11, 356, 460, 7048, 612, 389, 991, 9611, 11, 1626, 543, 11663, 2775, 351, 1692, 4409, 290, 5517, 4934, 625, 511, 670, 13, 376, 8789, 743, 1873, 9552, 393, 14193, 7848, 1692, 3259, 11, 475, 1692, 11663, 1057, 262, 905, 20947, 13, 4698, 428, 13196, 11, 5436, 447, 247, 82, 4934, 14051, 691, 287, 262, 7097, 2858, 286, 262, 4081, 13, 25929, 11, 356, 460, 7048, 326, 9611, 389, 3750, 13, 3887, 1692, 8383, 318, 788, 16689, 3264, 284, 5436, 11, 2138, 621, 284, 1692, 11663, 13, 16847, 743, 991, 1650, 1978, 287, 4888, 9730, 11, 30081, 351, 1123, 584, 11, 290, 8181, 503, 416, 262, 6891, 4572, 11, 475, 511, 670, 318, 7924, 416, 5436, 2884, 257, 900, 286, 38026, 14752, 13, 198, 9492, 13857, 2663, 389, 1744, 11, 3584, 484, 2192, 836, 447, 247, 83, 477, 2421, 4553, 9110, 13, 1114, 1672, 11, 262, 3773, 1244, 307, 7668, 13, 2773, 9611, 991, 8076, 11, 287, 10730, 351, 257, 1588, 3773, 286, 1981, 17736, 1762, 3264, 329, 5436, 13, 1881, 19898, 1339, 326, 1244, 2421, 4553, 9110, 561, 307, 611, 617, 9611, 389, 5257, 416, 1729, 12, 11518, 9552, 447, 247, 82, 13, 1114, 428, 1339, 284, 307, 7310, 11, 4081, 12, 37153, 317, 3792, 1276, 407, 307, 3938, 11521, 656, 5436, 11, 475, 2138, 389, 4553, 2551, 12, 6620, 287, 281, 4086, 2776, 351, 5436, 13, 5436, 447, 247, 82, 2694, 284, 766, 2641, 262, 4081, 1276, 307, 3614, 11, 290, 5353, 1276, 407, 307, 7138, 19874, 13, 383, 4081, 317, 3792, 743, 423, 2839, 5353, 287, 511, 4081, 447, 247, 82, 36513, 393, 3722, 11, 3737, 1642, 511, 898, 3259, 3772, 393, 19201, 511, 19195, 357, 361, 484, 991, 423, 606, 828, 393, 484, 743, 12546, 351, 5436, 319, 262, 19406, 1919, 9490, 2163, 13, 50062, 1397, 1022, 5436, 290, 262, 4081, 561, 307, 9552, 12, 1462, 12, 20185, 11, 290, 523, 319, 517, 4961, 32687, 621, 5436, 447, 247, 82, 12213, 351, 1692, 11663, 13, 843, 286, 1781, 11, 3259, 447, 247, 1998, 1626, 262, 4081, 561, 307, 1180, 26, 484, 561, 307, 739, 262, 4934, 286, 511, 4081, 447, 247, 82, 9552, 4706, 11, 2138, 621, 2035, 1692, 11663, 393, 5436, 13, 198, 2202, 428, 966, 11, 314, 2221, 416, 13148, 326, 9611, 466, 991, 2152, 11, 5257, 416, 2035, 5384, 393, 317, 3792, 13, 5436, 447, 247, 82, 1388, 1989, 286, 4905, 4145, 7363, 2354, 262, 18645, 286, 262, 4081, 11, 287, 29043, 1871, 9611, 290, 1022, 9611, 290, 7008, 13, 198, 33, 13, 1374, 10928, 25882, 5521, 2873, 25, 16972, 871, 393, 29431, 290, 27684, 284, 1867, 30, 198, 2061, 857, 5436, 1682, 466, 30, 383, 24043, 5885, 318, 326, 5436, 14051, 655, 588, 281, 1468, 12, 28776, 4318, 42351, 11, 31577, 5128, 290, 5072, 17794, 284, 790, 4081, 13, 314, 869, 428, 15304, 564, 250, 31208, 5436, 447, 251, 13, 5436, 3769, 534, 20157, 286, 477, 17311, 960, 14108, 3139, 11, 3259, 11, 290, 2587, 17311, 13, 1119, 481, 9240, 319, 534, 11046, 23423, 11, 319, 262, 1708, 7269, 13, 1002, 345, 423, 257, 1917, 351, 262, 17311, 6793, 11, 345, 389, 1479, 284, 1011, 340, 510, 351, 262, 22693, 11, 475, 345, 447, 247, 67, 2192, 2138, 1730, 3264, 351, 5436, 11, 508, 468, 281, 6275, 1700, 286, 31038, 18563, 8902, 290, 6547, 13, 2670, 843, 994, 318, 534, 5072, 32539, 25, 703, 881, 286, 1123, 1720, 11, 351, 7585, 10576, 290, 7064, 7368, 13, 2080, 5436, 447, 247, 82, 15822, 31350, 12971, 11, 262, 17311, 290, 23862, 477, 2872, 510, 7138, 357, 32796, 284, 3995, 354, 3477, 23989, 11, 284, 262, 6287, 612, 389, 991, 5112, 14608, 82, 11, 6729, 38563, 11, 393, 584, 36553, 2354, 5436, 447, 247, 82, 1630, 737, 198, 464, 749, 4096, 4427, 329, 428, 13888, 4786, 262, 16538, 286, 4081, 11663, 13, 2141, 11663, 423, 14130, 287, 703, 484, 1057, 1243, 2641, 511, 9611, 30, 48800, 484, 466, 11, 290, 14572, 484, 389, 407, 5899, 37677, 1023, 13, 775, 4145, 1607, 606, 284, 779, 511, 14130, 284, 5963, 511, 898, 5353, 11, 407, 284, 719, 355, 7138, 17074, 6554, 329, 5436, 447, 247, 82, 1919, 9490, 2163, 13, 843, 284, 262, 6287, 484, 466, 407, 423, 14130, 11, 1521, 423, 661, 1804, 777, 3946, 290, 1521, 561, 2687, 765, 606, 30, 1821, 5436, 743, 651, 262, 15623, 286, 17311, 290, 23862, 1871, 9611, 7138, 13, 887, 655, 12755, 17794, 357, 9541, 4232, 4645, 286, 8592, 5436, 3607, 11663, 287, 1339, 286, 12291, 422, 777, 8, 5667, 257, 2726, 4086, 1917, 13, 1869, 10321, 460, 779, 511, 14130, 284, 5963, 511, 12312, 6783, 5353, 11, 832, 2972, 5107, 286, 5602, 12, 38515, 11, 7720, 3081, 11, 1341, 27428, 572, 17311, 11, 29170, 511, 3259, 11, 290, 4441, 4633, 7097, 871, 960, 49459, 326, 318, 1626, 511, 8354, 286, 4934, 290, 21363, 540, 422, 5436, 13, 10968, 11, 262, 1917, 318, 407, 16019, 416, 1719, 5436, 11986, 517, 10582, 644, 262, 4081, 857, 11, 1390, 3037, 3572, 290, 584, 5387, 5370, 13, 1081, 890, 355, 612, 389, 960, 1525, 761, 393, 3572, 960, 69, 8789, 5257, 416, 5384, 351, 14130, 11, 290, 2839, 1321, 284, 787, 262, 14130, 11570, 11, 612, 481, 307, 4086, 2761, 286, 428, 3297, 13, 2312, 460, 307, 5322, 416, 517, 17707, 31577, 4081, 4069, 11, 379, 262, 1575, 286, 4232, 3815, 13338, 1719, 1692, 11663, 26, 484, 460, 307, 5322, 284, 1981, 12, 5715, 4086, 2761, 611, 612, 389, 645, 9611, 290, 790, 1692, 8383, 3136, 3264, 284, 5436, 26, 290, 484, 389, 3421, 287, 2095, 611, 9611, 389, 5257, 416, 9552, 447, 247, 82, 4553, 422, 5436, 13, 887, 477, 777, 20691, 3283, 3484, 290, 3292, 8210, 11, 290, 4844, 3938, 32311, 4086, 2761, 13, 198, 464, 2728, 286, 428, 1917, 318, 3489, 26, 588, 1468, 12, 2435, 4318, 5410, 11, 428, 1080, 468, 645, 4536, 13, 20664, 306, 11, 356, 550, 284, 7048, 4536, 379, 262, 966, 286, 2457, 7172, 5466, 284, 423, 11570, 7172, 4466, 17778, 13, 887, 739, 39789, 5436, 11, 477, 5128, 290, 3227, 5370, 510, 284, 326, 966, 389, 925, 416, 288, 1134, 83, 265, 13, 1114, 5436, 284, 6331, 319, 4536, 379, 2457, 6308, 5466, 11, 1231, 9646, 290, 1262, 606, 832, 262, 3227, 1429, 510, 284, 326, 966, 11, 10143, 284, 1011, 4621, 286, 1695, 11, 1029, 12, 8367, 1321, 290, 6946, 4410, 13, 21773, 33596, 547, 12524, 284, 4536, 329, 15735, 3840, 11, 475, 5436, 1595, 447, 247, 83, 423, 284, 307, 13, 5436, 318, 407, 281, 1405, 39795, 11, 3901, 339, 447, 247, 82, 281, 21543, 396, 290, 281, 18097, 48187, 13, 679, 447, 247, 82, 2045, 329, 2842, 284, 5963, 19406, 1692, 9490, 290, 4684, 284, 11206, 649, 10581, 287, 14748, 286, 326, 886, 13, 198, 1135, 4145, 2074, 257, 1218, 15304, 286, 5436, 11, 564, 250, 18124, 5436, 447, 251, 13, 5455, 286, 31577, 17794, 11, 7886, 5436, 26052, 4536, 286, 477, 7017, 287, 19497, 11, 1390, 477, 4081, 17311, 290, 23862, 13, 4900, 7886, 5436, 318, 991, 20814, 1180, 8611, 3403, 621, 4671, 561, 11206, 1912, 319, 2839, 5353, 3436, 960, 392, 4145, 4433, 4050, 22711, 286, 2042, 5939, 284, 4605, 465, 8568, 4934, 960, 1169, 1487, 422, 31577, 17794, 284, 4536, 8186, 728, 1811, 1688, 3033, 286, 5939, 13, 376, 8789, 389, 1479, 284, 16481, 511, 4560, 355, 484, 3853, 11, 2426, 284, 262, 1813, 4536, 484, 1986, 13, 1869, 10321, 460, 779, 428, 14130, 284, 2620, 10177, 11, 543, 3520, 1626, 262, 4081, 13, 383, 1243, 11663, 466, 1626, 262, 1910, 1080, 284, 2620, 10177, 960, 1640, 1672, 11, 9735, 1088, 329, 517, 11080, 393, 2793, 12, 30883, 17311, 11, 3682, 24549, 290, 10068, 3227, 7767, 11, 46891, 3259, 11, 10068, 290, 1180, 26336, 511, 23862, 284, 3141, 257, 2440, 2756, 960, 2787, 391, 23498, 11, 6196, 4050, 379, 3649, 10177, 11, 290, 18118, 18763, 13, 383, 1487, 422, 4634, 17794, 284, 4634, 4536, 12850, 867, 960, 1662, 477, 960, 1659, 262, 4086, 2761, 1944, 739, 39789, 5436, 11, 13148, 9611, 460, 12377, 257, 1588, 1576, 13390, 286, 511, 12042, 284, 307, 46891, 13, 3559, 5436, 4634, 4536, 2427, 286, 17794, 635, 10255, 328, 689, 12354, 4786, 3519, 284, 5436, 447, 247, 82, 4571, 286, 4827, 5939, 13, 5436, 4634, 9400, 11, 3737, 635, 2491, 257, 17304, 4803, 284, 1950, 7466, 286, 661, 284, 3946, 11, 1365, 43759, 262, 16171, 3450, 286, 670, 5370, 326, 11, 588, 7327, 5370, 11, 389, 1165, 7634, 6692, 284, 1981, 12354, 284, 2074, 20232, 25815, 13, 198, 5122, 14895, 546, 5436, 447, 247, 82, 45780, 2694, 20135, 326, 5436, 3011, 477, 4536, 826, 960, 439, 5939, 1598, 11, 351, 645, 28791, 393, 969, 489, 2664, 13, 887, 329, 7886, 5436, 284, 900, 777, 4536, 11, 339, 1276, 2035, 14799, 15284, 393, 12414, 262, 976, 1366, 355, 318, 4602, 393, 7560, 287, 1910, 12213, 25, 262, 20038, 290, 9695, 286, 4133, 11, 511, 5559, 3544, 11, 262, 3227, 8514, 1695, 284, 6121, 606, 11, 290, 7172, 15387, 13, 1002, 339, 2314, 34679, 3446, 262, 976, 1366, 11, 339, 1276, 5911, 922, 1576, 41775, 284, 7173, 3164, 262, 976, 7606, 29163, 8136, 13, 4900, 314, 423, 9672, 645, 4050, 17778, 319, 5436, 447, 247, 82, 31350, 2694, 11, 12470, 31316, 14895, 546, 5436, 447, 247, 82, 1895, 284, 477, 2622, 1366, 389, 517, 4099, 13, 6060, 318, 262, 34968, 290, 749, 35778, 2792, 287, 262, 6333, 286, 9889, 428, 1807, 6306, 4433, 13, 5436, 1244, 307, 1498, 284, 14799, 15284, 777, 7606, 29163, 4536, 13, 887, 284, 262, 6287, 262, 1366, 2622, 284, 22919, 777, 389, 407, 1695, 11, 389, 16378, 11, 393, 2728, 34859, 393, 16967, 17560, 7811, 287, 511, 12673, 960, 273, 11, 329, 326, 2300, 11, 284, 262, 6287, 612, 389, 584, 1919, 3815, 3675, 1321, 12, 20158, 14183, 284, 1910, 7767, 286, 2989, 11, 23189, 11, 290, 29148, 960, 732, 1244, 4702, 407, 284, 423, 5436, 302, 12, 395, 1920, 777, 1910, 12, 2375, 1723, 4536, 13, 5455, 11, 5436, 714, 779, 262, 4536, 326, 14740, 422, 4795, 3227, 290, 7327, 5370, 11, 48878, 1538, 4394, 290, 7007, 357, 65, 2340, 290, 7893, 828, 287, 7606, 12213, 960, 259, 1245, 11, 1309, 5436, 1479, 12, 13154, 319, 1910, 7767, 284, 7716, 2756, 1321, 13, 198, 13681, 25, 775, 447, 247, 303, 1282, 428, 1290, 11, 290, 262, 1266, 5436, 460, 466, 6867, 284, 8186, 2259, 1910, 4536, 960, 2339, 262, 2095, 287, 262, 29004, 274, 1621, 508, 14799, 564, 250, 42910, 447, 251, 2094, 2264, 844, 1258, 30, 2598, 554, 530, 2565, 11, 356, 423, 2391, 31759, 9075, 988, 447, 247, 82, 4578, 546, 262, 1321, 3773, 286, 26512, 1910, 5370, 13, 887, 356, 447, 247, 260, 407, 1760, 13, 5991, 4536, 2148, 1029, 12, 8367, 1321, 11, 475, 691, 355, 257, 3599, 966, 329, 5436, 447, 247, 82, 1693, 13, 5436, 318, 5047, 351, 10068, 319, 1910, 10906, 618, 777, 12312, 469, 422, 1919, 6436, 1483, 13, 383, 4536, 5436, 43707, 284, 4620, 428, 481, 1690, 307, 4961, 393, 845, 1969, 284, 883, 11823, 422, 1910, 5163, 11, 475, 407, 1464, 26, 290, 262, 5400, 389, 1593, 13, 1675, 19418, 428, 749, 4084, 11, 340, 318, 7613, 284, 2074, 1865, 257, 2368, 15304, 286, 5436, 13, 198, 1212, 1296, 286, 5436, 561, 779, 1910, 12213, 284, 7716, 4238, 4536, 326, 4691, 355, 262, 3599, 966, 329, 790, 8611, 11, 475, 561, 788, 13551, 2756, 16895, 319, 1123, 8611, 355, 2622, 284, 3376, 1910, 15536, 13, 554, 38649, 355, 867, 286, 262, 1910, 23162, 507, 5436, 1276, 3376, 460, 307, 7247, 355, 7097, 871, 357, 16885, 4633, 290, 3967, 828, 356, 423, 783, 302, 12, 23211, 5436, 447, 247, 82, 1693, 355, 38849, 257, 1844, 1080, 286, 23097, 709, 666, 5704, 290, 16230, 11, 2231, 523, 314, 869, 428, 15304, 564, 250, 47, 328, 709, 666, 5436, 13, 447, 251, 23097, 709, 666, 5436, 561, 13446, 477, 7097, 871, 290, 584, 1910, 23162, 507, 357, 1662, 655, 355, 2060, 2173, 11, 475, 355, 484, 7565, 625, 617, 5981, 2837, 286, 5072, 828, 5453, 5704, 393, 16230, 11, 788, 6687, 4232, 15068, 1429, 318, 2622, 284, 4155, 326, 5939, 991, 1598, 13, 198, 2437, 561, 23097, 709, 666, 5436, 307, 9177, 30, 1629, 262, 1241, 286, 1981, 8945, 11, 23097, 709, 666, 5436, 1244, 804, 2407, 41511, 2213, 11350, 290, 5385, 13, 25688, 364, 714, 1281, 5969, 4536, 393, 14456, 290, 23531, 714, 16674, 11, 355, 484, 466, 739, 1910, 3341, 11, 510, 284, 262, 966, 286, 8611, 13, 5436, 561, 788, 15284, 290, 751, 262, 5035, 1687, 393, 28646, 379, 262, 966, 286, 5466, 13, 383, 1429, 561, 307, 2092, 284, 262, 38091, 286, 257, 4200, 1687, 11, 475, 351, 734, 5400, 13, 3274, 11, 262, 16895, 561, 7565, 625, 8945, 11, 523, 14456, 290, 23531, 561, 761, 284, 307, 7981, 286, 262, 15068, 878, 484, 4589, 284, 1123, 8611, 11, 14572, 2884, 5175, 4410, 11, 1321, 319, 4200, 11298, 11, 393, 966, 12, 1659, 12, 21378, 3341, 13, 5498, 11, 16895, 714, 307, 286, 2035, 1051, 11, 290, 714, 307, 1588, 329, 7017, 351, 1588, 7097, 871, 13, 198, 2953, 4025, 5046, 11, 703, 28094, 23097, 709, 666, 5436, 561, 307, 481, 4745, 319, 3307, 286, 7822, 11, 290, 319, 36553, 546, 262, 2546, 286, 262, 16895, 326, 2421, 3781, 3675, 616, 8354, 994, 13, 5436, 1244, 307, 5365, 41511, 2213, 11350, 11, 284, 262, 6287, 326, 5365, 1178, 7017, 3283, 749, 286, 262, 7097, 3048, 326, 761, 17137, 960, 1640, 1672, 4633, 7097, 871, 422, 12584, 18017, 11, 1660, 7925, 507, 11, 3510, 4334, 21782, 11, 11422, 12910, 11, 14240, 37113, 290, 5931, 17311, 26, 290, 3967, 7097, 871, 422, 8287, 290, 44832, 286, 3725, 11, 3518, 290, 5110, 1535, 11, 1919, 2594, 11, 3503, 13, 383, 1080, 714, 307, 9177, 379, 2972, 2173, 287, 5127, 14659, 11, 6906, 319, 703, 7097, 3048, 389, 9387, 1973, 777, 13, 48282, 278, 340, 588, 257, 11052, 12, 13003, 9241, 357, 53, 1404, 828, 2857, 351, 5436, 447, 247, 82, 15068, 1912, 319, 29497, 7097, 3484, 393, 4034, 379, 1123, 3800, 422, 4165, 17311, 284, 2457, 7172, 7017, 11, 561, 307, 257, 19756, 3164, 13, 1114, 7017, 6872, 262, 4387, 4633, 7097, 871, 960, 10508, 355, 12584, 18017, 287, 262, 995, 286, 6049, 4258, 1487, 960, 1169, 9871, 1919, 8055, 743, 6211, 1588, 20691, 287, 262, 2472, 12040, 287, 19497, 393, 1844, 21472, 13, 1002, 262, 5798, 286, 1642, 884, 1588, 12, 9888, 1919, 38226, 8953, 5000, 284, 23097, 709, 666, 5436, 447, 247, 82, 2756, 16895, 11, 777, 1244, 423, 284, 7108, 287, 6364, 11, 355, 5436, 25223, 262, 8282, 4419, 4073, 416, 262, 3186, 351, 262, 1919, 1575, 286, 19911, 422, 5801, 40237, 503, 286, 4683, 3186, 290, 965, 27225, 3139, 11115, 13, 25929, 11, 262, 1181, 1244, 779, 584, 11344, 4899, 11, 543, 481, 991, 307, 1695, 284, 340, 772, 351, 5436, 5361, 11, 284, 10660, 777, 2458, 13, 1649, 1919, 4661, 389, 19189, 11476, 393, 18174, 832, 884, 584, 11344, 4899, 11, 262, 2648, 286, 5798, 329, 777, 2428, 7463, 284, 5436, 11, 290, 262, 2546, 286, 23097, 709, 666, 5436, 447, 247, 82, 2756, 16895, 11, 561, 307, 5322, 393, 15254, 16062, 13, 198, 10855, 13, 8495, 278, 290, 48282, 278, 5436, 25, 22852, 290, 44495, 198, 818, 9984, 286, 9552, 11, 9775, 10360, 18452, 6067, 286, 1486, 290, 7822, 1085, 11, 12362, 3264, 290, 2952, 11, 284, 2769, 2683, 286, 1964, 11, 2742, 11, 290, 6573, 19369, 286, 1919, 6712, 13, 1081, 257, 1807, 6306, 11, 5436, 447, 247, 82, 1693, 287, 636, 318, 284, 29791, 777, 9984, 13, 5436, 318, 5292, 284, 307, 2077, 6411, 355, 281, 13936, 286, 257, 2785, 43590, 3586, 286, 9552, 13, 17973, 1426, 1780, 5436, 355, 257, 2726, 5885, 290, 14607, 10017, 306, 832, 703, 340, 561, 670, 10212, 6945, 2972, 3403, 11, 5359, 11, 290, 2785, 12751, 290, 7476, 13, 887, 5436, 635, 12031, 284, 29791, 2683, 546, 262, 26877, 3403, 326, 8160, 465, 4732, 25, 703, 484, 8076, 11, 644, 484, 2421, 11, 511, 12751, 11, 644, 484, 389, 11, 511, 4560, 11, 5359, 11, 12751, 11, 43483, 1143, 14895, 11, 290, 987, 12, 39468, 5748, 13, 198, 1212, 2665, 9405, 428, 1218, 1398, 286, 2683, 13, 632, 14358, 5436, 447, 247, 82, 2476, 11, 10939, 11, 290, 2785, 12751, 960, 16885, 11781, 290, 35778, 960, 1462, 12774, 1111, 703, 23498, 393, 18763, 5436, 357, 273, 12470, 5909, 9552, 3544, 8, 1244, 307, 290, 644, 649, 22582, 5436, 3769, 319, 1468, 2683, 13, 3412, 517, 621, 3161, 9004, 11, 262, 5114, 686, 4105, 625, 257, 5909, 7674, 11, 290, 318, 4145, 6646, 28991, 290, 15223, 13, 198, 32, 13, 6060, 25, 1867, 8314, 5436, 10664, 290, 1374, 8314, 679, 3497, 632, 30, 198, 464, 4318, 5002, 286, 262, 1468, 15889, 17952, 4384, 11, 290, 262, 530, 749, 27377, 3421, 416, 2274, 14901, 11, 318, 1366, 13, 4377, 1296, 286, 5436, 11, 588, 597, 4318, 5410, 1080, 11, 481, 2421, 257, 5909, 2033, 286, 1366, 284, 1104, 663, 16765, 13, 314, 8606, 39789, 5436, 319, 9384, 286, 4086, 2761, 290, 43747, 16538, 11, 407, 1366, 7095, 13, 383, 1366, 2476, 286, 11149, 2884, 4536, 393, 17794, 743, 13238, 1912, 319, 262, 6276, 4645, 286, 262, 23989, 1917, 960, 1169, 3585, 31350, 9332, 286, 45780, 319, 43750, 9051, 10668, 9633, 960, 4360, 326, 1808, 318, 41264, 1813, 262, 17927, 286, 39789, 5436, 329, 584, 3840, 13, 383, 734, 5637, 17670, 11, 7886, 5436, 290, 23097, 709, 666, 5436, 11, 423, 2092, 1366, 2476, 11, 475, 13238, 287, 703, 484, 14658, 606, 13, 198, 19626, 717, 262, 1366, 5436, 2476, 284, 24340, 1910, 10906, 44061, 355, 777, 389, 18118, 13205, 11, 884, 355, 284, 7716, 1910, 12, 2375, 1723, 10906, 326, 389, 36836, 9404, 6942, 287, 262, 3614, 11, 350, 533, 1462, 2565, 13, 5436, 2476, 1366, 546, 477, 5127, 290, 3512, 3403, 5387, 284, 597, 2785, 8611, 11, 1390, 17311, 11, 3227, 8514, 11, 290, 7172, 15387, 13, 770, 318, 262, 976, 1321, 355, 1468, 15889, 5410, 2622, 290, 4054, 329, 262, 3092, 286, 11, 351, 262, 1402, 28587, 326, 5436, 468, 257, 6454, 4025, 1693, 621, 47579, 447, 247, 82, 42351, 11, 543, 750, 407, 900, 4536, 329, 2457, 7172, 7017, 393, 4827, 13, 5747, 7886, 290, 23097, 709, 666, 5436, 761, 777, 1366, 11, 475, 23097, 709, 666, 5436, 16507, 319, 26512, 1910, 12213, 284, 7716, 606, 11, 2426, 284, 465, 8840, 16895, 284, 3376, 1910, 15536, 13, 7886, 5436, 20393, 645, 884, 1790, 12, 8968, 11, 475, 1276, 6431, 11, 19386, 11, 290, 16602, 477, 777, 1366, 290, 24983, 1096, 262, 2482, 284, 8676, 284, 465, 2756, 4634, 329, 1123, 8611, 13, 198, 818, 6273, 284, 262, 1468, 15889, 17952, 4384, 11, 340, 318, 19756, 11, 3737, 772, 1884, 11, 326, 262, 1366, 2622, 284, 5678, 777, 4795, 7746, 286, 1910, 4536, 389, 783, 1695, 13, 770, 318, 3573, 1598, 319, 262, 5127, 1735, 11, 329, 9611, 13, 797, 14938, 1321, 318, 1695, 422, 3294, 15736, 1804, 1103, 12, 2435, 9904, 286, 3294, 12608, 286, 3227, 11, 6082, 11, 290, 4200, 26, 5387, 14317, 290, 4542, 1321, 3341, 26, 6276, 9695, 290, 2854, 1366, 422, 8217, 290, 5112, 11, 9257, 7083, 416, 262, 25068, 286, 5230, 12, 15236, 4410, 26, 290, 1844, 4406, 286, 262, 3047, 11, 4678, 290, 4069, 286, 3259, 11, 1978, 351, 5981, 8055, 13871, 13, 383, 424, 35590, 286, 777, 4081, 12, 5715, 1366, 318, 8523, 772, 257, 2300, 286, 13367, 11, 1813, 262, 1029, 24126, 319, 11862, 1146, 7924, 5410, 11, 1626, 1588, 23941, 290, 287, 5127, 14659, 290, 5021, 12, 9255, 7919, 7686, 8389, 416, 257, 2060, 339, 70, 50016, 4081, 357, 24888, 11, 262, 4196, 290, 5565, 598, 7000, 737, 4280, 3279, 284, 20435, 777, 1588, 12, 9888, 4560, 416, 1366, 12, 23657, 4571, 2138, 621, 5387, 5939, 7634, 20135, 326, 262, 1366, 2622, 329, 6942, 3227, 11, 3272, 12, 9255, 7919, 1575, 10356, 1634, 11, 290, 11795, 290, 14748, 286, 649, 6443, 318, 1695, 11, 379, 1551, 284, 27183, 262, 9432, 2163, 286, 262, 4081, 21024, 262, 1080, 13, 2780, 198, 11518, 2476, 777, 3227, 12, 5363, 1366, 407, 655, 379, 262, 1241, 286, 2060, 9611, 11, 2158, 11, 475, 329, 262, 2187, 3773, 13, 554, 3090, 284, 262, 31350, 6459, 326, 314, 716, 15482, 11, 428, 6482, 284, 281, 19406, 6650, 12073, 2683, 546, 16538, 329, 1336, 290, 7187, 13019, 13, 5436, 561, 14572, 307, 10435, 284, 33729, 1366, 13019, 11, 543, 743, 307, 4050, 329, 1366, 422, 1277, 13050, 357, 4853, 4667, 15736, 11, 8452, 9073, 828, 393, 584, 4237, 407, 14704, 2426, 284, 26521, 341, 393, 7776, 357, 32538, 43747, 14317, 1366, 737, 1835, 7339, 9314, 13019, 743, 307, 7069, 329, 1366, 10795, 319, 1692, 13432, 290, 6447, 960, 1712, 49317, 329, 564, 250, 83, 330, 270, 3725, 11, 447, 251, 5032, 12, 2339, 3725, 326, 661, 1745, 1231, 852, 1498, 284, 32154, 11, 543, 2826, 257, 1688, 2597, 287, 9075, 988, 447, 247, 82, 19976, 286, 5410, 13, 2893, 314, 7048, 326, 428, 1917, 460, 307, 4030, 36426, 832, 14901, 287, 15736, 290, 1366, 4542, 11, 1978, 351, 15660, 12, 38532, 13019, 3341, 290, 12970, 329, 15828, 27807, 2649, 11, 428, 318, 257, 8414, 540, 13196, 13, 198, 2202, 262, 7327, 1735, 11, 1692, 15387, 290, 9490, 389, 407, 3264, 42550, 11, 3584, 14901, 287, 39738, 1950, 428, 743, 307, 5609, 13, 317, 2583, 286, 3519, 17211, 1366, 318, 42550, 11, 2158, 11, 422, 543, 4572, 12, 40684, 12, 3106, 33344, 23696, 3341, 389, 19988, 8902, 287, 511, 2694, 284, 4331, 5001, 5370, 290, 3519, 4069, 13, 376, 8789, 2824, 257, 3236, 2033, 286, 884, 1366, 11, 290, 5801, 4371, 287, 3341, 11, 1390, 15602, 11874, 290, 2614, 29488, 11, 5644, 484, 743, 307, 12872, 329, 5436, 284, 466, 465, 1693, 13, 2312, 1366, 2192, 466, 407, 1944, 2726, 2761, 3519, 284, 13019, 16538, 780, 484, 39779, 2354, 9611, 357, 10197, 611, 9611, 788, 2824, 606, 828, 290, 523, 484, 389, 1342, 1884, 284, 307, 7744, 14553, 287, 5387, 40787, 3725, 13, 198, 464, 1366, 6459, 2950, 351, 15852, 422, 4081, 12, 3106, 284, 26877, 23989, 481, 307, 517, 2726, 329, 7327, 12, 5363, 621, 3227, 12, 5363, 1366, 13, 5991, 3341, 36059, 22440, 1022, 7008, 447, 247, 16171, 7747, 290, 511, 9490, 13, 770, 11795, 16507, 379, 734, 2173, 319, 262, 7877, 29005, 286, 4602, 12741, 25, 717, 11, 611, 345, 7690, 340, 345, 1276, 423, 9871, 340, 1813, 262, 1695, 14693, 26, 290, 1218, 11, 534, 15387, 4145, 6241, 389, 1365, 21337, 286, 534, 9490, 621, 597, 2354, 5797, 460, 2148, 13, 1675, 262, 6287, 428, 19168, 318, 407, 5716, 14177, 355, 281, 7877, 29005, 11, 340, 318, 6189, 3360, 3991, 25, 661, 787, 617, 7747, 326, 4084, 4419, 606, 13, 1400, 8569, 2280, 1365, 835, 284, 3953, 9490, 318, 1598, 11, 2158, 11, 290, 4756, 262, 3420, 284, 9616, 1854, 1560, 345, 644, 345, 761, 17313, 1598, 7432, 284, 12354, 11, 2884, 38082, 1042, 393, 4785, 13, 314, 8384, 2209, 428, 2071, 287, 11142, 262, 1917, 286, 16215, 5436, 447, 247, 82, 9432, 2163, 287, 7275, 6711, 13, 37, 13, 887, 314, 6056, 340, 994, 284, 5298, 262, 5885, 326, 45780, 329, 9490, 2138, 621, 329, 7327, 4069, 743, 2421, 1180, 1366, 11, 543, 743, 307, 1342, 14704, 1695, 11, 1342, 42550, 11, 393, 1342, 880, 14793, 798, 13, 1675, 262, 6287, 428, 318, 262, 1339, 11, 772, 10457, 1943, 30341, 290, 25539, 7327, 7747, 743, 407, 307, 6751, 284, 10176, 262, 11500, 286, 1366, 2622, 329, 9490, 23989, 13, 198, 464, 4947, 290, 779, 286, 7172, 1366, 416, 9611, 318, 1541, 8620, 2726, 4786, 3519, 284, 6782, 290, 9511, 1630, 625, 511, 1321, 11, 329, 543, 2972, 2450, 290, 2742, 9109, 389, 5150, 13, 314, 466, 407, 2209, 777, 2428, 11, 2845, 284, 3465, 326, 262, 5981, 1808, 329, 616, 4959, 318, 703, 777, 4786, 13238, 6906, 1771, 262, 8674, 11228, 534, 1366, 318, 257, 2839, 4081, 393, 5436, 13, 770, 714, 467, 2035, 835, 13, 921, 1244, 7317, 2134, 517, 7634, 284, 1366, 11228, 416, 257, 32551, 12, 5219, 8674, 588, 5436, 11, 3584, 428, 3580, 743, 22100, 393, 9575, 355, 262, 5046, 290, 1366, 12, 18908, 1358, 9889, 286, 2839, 9611, 1663, 284, 22464, 11, 393, 7074, 11, 883, 286, 2585, 13, 1318, 743, 11, 5600, 11, 307, 1365, 3840, 284, 3774, 5436, 351, 674, 1366, 621, 3203, 11, 3012, 11, 393, 6186, 13, 5436, 1244, 11, 329, 1672, 11, 307, 517, 1498, 290, 4684, 621, 2839, 9611, 284, 3494, 1913, 6782, 12, 11235, 13967, 5260, 11, 884, 355, 6782, 26235, 11, 1913, 8281, 5359, 11, 393, 49724, 319, 17678, 2455, 278, 11, 302, 12, 3500, 11, 393, 302, 12, 14225, 32927, 1366, 13, 1550, 262, 584, 1021, 11, 6782, 12, 35499, 278, 8733, 319, 1366, 779, 1244, 307, 517, 34909, 329, 5436, 621, 329, 2839, 9611, 11, 508, 460, 7330, 1321, 546, 7172, 15387, 422, 511, 898, 12213, 355, 1910, 1938, 13, 554, 597, 1339, 11, 6782, 4786, 389, 7310, 422, 616, 1388, 2962, 319, 40460, 11, 4556, 484, 6152, 281, 27697, 6317, 326, 1838, 2622, 1366, 23485, 393, 7242, 540, 13, 198, 6892, 876, 284, 7886, 5436, 11, 23097, 709, 666, 5436, 468, 1342, 761, 329, 8611, 12, 32538, 3227, 290, 7327, 12, 5363, 1366, 11, 780, 339, 16507, 319, 1910, 12213, 284, 7716, 4238, 4536, 1912, 319, 777, 13, 554, 3090, 284, 13148, 326, 777, 3165, 6783, 4536, 14351, 4079, 10238, 9920, 290, 7172, 1321, 11, 23097, 709, 666, 5436, 1276, 635, 7048, 326, 1262, 1910, 10906, 287, 428, 835, 857, 407, 17253, 511, 19648, 13, 2920, 46192, 739, 23097, 709, 666, 5436, 561, 3051, 287, 734, 9539, 11, 780, 1007, 27362, 4671, 561, 766, 1111, 262, 7317, 5295, 11, 1910, 12, 3106, 2756, 11, 290, 5436, 447, 247, 82, 15068, 284, 7800, 262, 2457, 2756, 13, 770, 734, 12, 14247, 1429, 1244, 1487, 4069, 290, 10906, 11, 6906, 319, 262, 4202, 290, 1296, 286, 2551, 339, 333, 3969, 5361, 13, 1114, 1672, 11, 4671, 1244, 2038, 284, 787, 8945, 326, 389, 39512, 2233, 284, 1913, 3967, 7097, 871, 11, 611, 484, 466, 407, 23794, 5436, 447, 247, 82, 10156, 1642, 340, 17092, 517, 10966, 284, 606, 13, 25929, 11, 611, 14456, 15866, 1913, 12619, 3255, 319, 262, 4481, 662, 12, 23032, 434, 2756, 11, 356, 561, 1607, 262, 734, 12, 14247, 13019, 1429, 286, 23097, 709, 666, 5436, 284, 7716, 7387, 9109, 284, 5436, 447, 247, 82, 16895, 621, 883, 416, 4671, 24986, 351, 7886, 5436, 11, 508, 561, 691, 766, 262, 2457, 2756, 13, 1120, 23097, 709, 666, 5436, 1244, 635, 1986, 7776, 286, 4238, 8945, 11, 393, 5322, 10539, 273, 287, 6095, 39512, 8945, 416, 4671, 508, 760, 5436, 481, 1282, 287, 706, 262, 1109, 284, 1630, 511, 8945, 13, 8013, 12779, 1244, 2421, 5436, 284, 302, 12, 9122, 262, 19648, 286, 4238, 4536, 416, 2186, 12364, 7886, 5436, 447, 247, 82, 7746, 287, 617, 2663, 11, 12839, 8868, 465, 1321, 4621, 625, 7886, 5436, 13, 198, 10265, 7886, 290, 23097, 709, 666, 5436, 635, 761, 1321, 3519, 284, 597, 3048, 7097, 284, 1007, 27362, 4671, 393, 584, 1910, 15536, 13, 797, 14938, 1910, 15536, 389, 286, 1115, 3858, 25, 357, 16, 8, 3614, 393, 30372, 19482, 1321, 11, 2592, 1813, 14445, 32269, 7017, 290, 3734, 12, 2164, 1328, 12291, 286, 8611, 3403, 625, 2272, 290, 640, 26, 357, 17, 8, 10224, 7097, 871, 884, 355, 2858, 11, 1535, 11, 290, 3747, 34859, 26, 290, 357, 18, 8, 1910, 1176, 13, 314, 2112, 262, 717, 734, 994, 290, 2074, 1910, 1176, 290, 663, 6948, 287, 262, 1306, 2665, 13, 198, 30507, 306, 11, 5436, 447, 247, 82, 9672, 9889, 20135, 326, 612, 389, 645, 1321, 12, 5363, 1910, 15536, 11, 475, 612, 318, 257, 1310, 517, 284, 910, 319, 428, 329, 23097, 709, 666, 5436, 13, 2399, 24126, 319, 1007, 27362, 4671, 447, 247, 23189, 355, 257, 15741, 329, 477, 5981, 8611, 12, 32538, 1321, 481, 307, 12515, 611, 777, 10906, 4079, 3614, 393, 30372, 19482, 1321, 13, 23097, 709, 666, 5436, 4145, 2314, 3368, 2045, 739, 262, 14263, 329, 8611, 12, 32538, 1321, 26, 3584, 339, 857, 407, 761, 284, 466, 428, 284, 900, 281, 4238, 11, 662, 12, 23032, 434, 2756, 11, 339, 991, 1276, 466, 340, 284, 5911, 290, 3376, 597, 1321, 7095, 13, 770, 761, 743, 691, 4174, 284, 1728, 3858, 286, 8611, 11, 393, 743, 307, 1342, 27127, 462, 621, 7886, 5436, 447, 247, 82, 5103, 286, 4536, 390, 645, 13038, 11, 475, 991, 12850, 23097, 709, 666, 5436, 447, 247, 82, 31350, 4621, 625, 7886, 5436, 13, 198, 2514, 3376, 6142, 290, 584, 7097, 871, 11, 749, 1366, 5436, 2476, 481, 307, 7097, 284, 262, 8611, 11, 3519, 284, 1171, 393, 45107, 10893, 4034, 290, 34859, 13, 770, 481, 2291, 1111, 5654, 290, 7172, 12, 3866, 4288, 1366, 960, 17018, 546, 262, 3518, 290, 10685, 6948, 286, 3034, 5370, 11, 290, 546, 703, 661, 1988, 777, 6948, 13, 47052, 286, 9511, 447, 247, 82, 29115, 286, 6142, 290, 3519, 10906, 389, 27606, 5952, 329, 4414, 12, 15805, 3781, 286, 11344, 5370, 11, 17965, 319, 257, 6087, 286, 17211, 15741, 1366, 290, 7952, 1988, 12, 417, 3628, 341, 16255, 13, 2312, 5050, 389, 2407, 14897, 26, 5600, 11, 612, 389, 36512, 625, 262, 34795, 5314, 19648, 286, 884, 12741, 7746, 4553, 422, 6939, 1910, 8945, 26, 3584, 11, 13717, 4084, 1365, 14693, 11, 777, 389, 20823, 17498, 319, 287, 11344, 5370, 13, 4349, 198, 15354, 393, 407, 5436, 460, 3164, 617, 4938, 8245, 10552, 286, 884, 15387, 11, 314, 716, 6563, 5436, 460, 5678, 7746, 286, 777, 3815, 1365, 621, 883, 4635, 416, 1944, 5050, 13, 679, 714, 4961, 606, 416, 10582, 2186, 12364, 1944, 14897, 1366, 290, 31850, 7605, 26, 290, 339, 561, 2048, 3729, 307, 1498, 284, 6061, 465, 5909, 1366, 290, 31350, 4133, 284, 1205, 1365, 16255, 11, 41775, 11, 290, 19648, 12, 41004, 9021, 13, 5436, 447, 247, 82, 13391, 561, 307, 772, 3744, 287, 32029, 5654, 1321, 546, 26558, 11701, 326, 2792, 3034, 7747, 284, 17560, 12751, 13, 5436, 714, 19386, 5887, 5654, 290, 6276, 3725, 546, 3227, 7767, 290, 511, 7097, 2587, 290, 2568, 15623, 11, 355, 880, 355, 21568, 1181, 12, 1659, 12, 1169, 12, 433, 4547, 286, 17262, 286, 6142, 3341, 326, 2792, 777, 15623, 284, 2458, 287, 17560, 6142, 12608, 13, 4698, 5436, 11, 9056, 546, 4258, 1487, 393, 12319, 3048, 326, 547, 1900, 351, 1029, 6628, 284, 307, 3991, 561, 711, 645, 2597, 287, 13045, 262, 16895, 329, 3917, 8945, 13, 198, 15056, 13479, 287, 3725, 286, 6142, 7767, 11, 5436, 561, 635, 423, 262, 3038, 286, 2263, 257, 32992, 560, 3164, 13, 8013, 281, 3164, 561, 923, 351, 257, 22111, 4817, 32315, 319, 617, 7368, 6142, 10538, 11, 5447, 625, 262, 5981, 21739, 5046, 290, 262, 3917, 11408, 290, 7008, 13, 8013, 257, 32315, 714, 1282, 422, 257, 1964, 1429, 393, 714, 307, 7560, 416, 5436, 1912, 319, 3781, 286, 262, 976, 12741, 290, 6142, 1366, 29927, 617, 7368, 4922, 286, 2526, 12, 64, 9641, 13, 2080, 326, 32315, 7368, 11, 5436, 561, 788, 900, 16586, 2756, 16895, 284, 4620, 326, 32315, 11, 287, 1245, 11, 2263, 257, 1575, 12, 10760, 6517, 2138, 621, 257, 4414, 12, 15805, 3164, 13, 198, 3237, 262, 1366, 2672, 329, 5436, 447, 247, 82, 16765, 481, 1487, 625, 640, 290, 523, 2421, 9904, 290, 15068, 13, 9676, 11, 262, 11278, 286, 13357, 3917, 351, 1720, 9695, 15874, 625, 640, 290, 4067, 373, 262, 1388, 4308, 286, 9075, 988, 447, 247, 82, 15556, 4578, 329, 262, 50097, 286, 4318, 5410, 13, 770, 373, 4084, 3376, 329, 1692, 33596, 11, 508, 714, 407, 466, 12948, 19698, 290, 523, 550, 284, 11986, 8187, 3403, 625, 7083, 9574, 11, 475, 5436, 481, 307, 881, 517, 6007, 286, 4067, 12, 11423, 290, 1103, 12, 2435, 16895, 13, 1081, 257, 1255, 11, 32532, 11, 5436, 481, 423, 1342, 761, 329, 7187, 16277, 286, 2003, 3403, 621, 1692, 33596, 750, 13, 5436, 743, 635, 307, 1498, 284, 5911, 2663, 810, 3403, 1487, 6364, 393, 12213, 389, 4939, 11, 290, 523, 5409, 618, 339, 460, 30276, 465, 16765, 379, 1402, 1919, 1575, 784, 611, 465, 29964, 318, 407, 2407, 1575, 1203, 11, 523, 884, 1790, 12, 23779, 389, 24769, 13, 19179, 625, 640, 481, 3051, 287, 1111, 8611, 12, 32538, 3403, 290, 7097, 871, 11, 475, 262, 6846, 743, 1944, 1948, 6459, 286, 19363, 1487, 13, 22060, 3725, 286, 11701, 286, 6142, 393, 1535, 4419, 318, 10491, 2426, 284, 1588, 33315, 422, 649, 24362, 11, 543, 1244, 20135, 4802, 2458, 287, 5436, 447, 247, 82, 2756, 16895, 13, 1081, 4367, 2029, 329, 5436, 447, 247, 82, 4238, 7108, 12, 259, 11, 465, 16895, 561, 788, 423, 284, 19330, 1111, 262, 649, 5654, 3725, 286, 34859, 290, 262, 3484, 286, 5801, 16895, 11, 1813, 262, 1459, 1181, 286, 262, 3773, 290, 3139, 4283, 13, 679, 1276, 5236, 262, 3484, 286, 14409, 1165, 6364, 284, 262, 6142, 4419, 1028, 262, 19911, 286, 19702, 262, 3773, 1165, 3049, 287, 257, 649, 4571, 960, 273, 1165, 37891, 11, 1813, 13479, 13, 4309, 198, 33, 13, 5436, 8314, 3738, 270, 11469, 290, 6101, 25, 5991, 4333, 11, 29832, 12, 4653, 18754, 11, 290, 27724, 198, 818, 3090, 284, 14317, 329, 7097, 871, 11, 5436, 481, 307, 1498, 284, 6687, 1910, 1176, 290, 3519, 4069, 290, 12751, 329, 40708, 1919, 4414, 13, 1114, 4959, 286, 22712, 703, 5436, 1244, 466, 523, 11, 1910, 1176, 460, 779, 2759, 307, 37661, 287, 1115, 3858, 11, 351, 1180, 5640, 13, 3274, 11, 749, 25281, 2251, 30797, 444, 416, 21391, 2450, 3572, 832, 9028, 3119, 1099, 11, 351, 262, 4031, 326, 262, 43440, 28393, 481, 7716, 16538, 329, 16389, 290, 11044, 13, 5498, 11, 617, 11798, 389, 3288, 30797, 444, 2233, 284, 1575, 8573, 7411, 16533, 286, 5046, 393, 8354, 11, 543, 1577, 1588, 9611, 21112, 13391, 287, 2846, 286, 2793, 1575, 393, 2694, 284, 2897, 517, 10966, 7017, 393, 2594, 13, 10467, 11, 1910, 1176, 460, 307, 2727, 832, 9611, 447, 247, 4040, 284, 16417, 14725, 284, 5726, 1028, 649, 13861, 11, 1262, 257, 3094, 4996, 286, 14614, 11, 10039, 11, 7124, 11, 2450, 11, 393, 2742, 1724, 326, 6352, 2454, 475, 389, 517, 7667, 621, 262, 3161, 734, 11701, 13, 198, 818, 477, 777, 2663, 11, 1910, 1176, 784, 290, 9611, 447, 247, 43440, 2694, 284, 5298, 4536, 393, 4306, 6431, 28393, 784, 318, 18118, 13568, 13, 383, 2368, 2099, 11, 1910, 1176, 832, 32455, 4635, 14725, 284, 5726, 11, 6870, 257, 5899, 1919, 4419, 351, 645, 11677, 889, 4414, 13, 10968, 11, 884, 13391, 389, 1690, 13659, 832, 7952, 5602, 12, 38515, 4040, 11, 543, 1944, 3224, 1919, 3484, 351, 645, 2010, 4414, 25, 5845, 15461, 262, 28393, 4414, 611, 511, 4040, 6758, 11, 286, 1781, 11, 475, 379, 262, 1575, 286, 4025, 9089, 8057, 13, 383, 1218, 2099, 11, 1910, 1176, 2233, 284, 16533, 286, 5046, 393, 8354, 11, 635, 6870, 257, 2010, 26877, 4419, 11, 407, 2233, 284, 542, 36207, 4040, 284, 5380, 28393, 475, 284, 262, 1575, 4645, 286, 262, 2831, 13, 15467, 1588, 5969, 3484, 2251, 16533, 286, 5046, 11, 355, 287, 20081, 351, 16378, 6082, 7686, 393, 584, 4569, 3288, 30797, 444, 13, 1471, 1913, 3127, 3048, 2251, 16533, 286, 8354, 11, 15882, 4025, 11408, 284, 2148, 617, 6087, 286, 1365, 3186, 393, 2594, 11, 393, 2793, 3484, 13, 6683, 444, 286, 5046, 290, 8354, 2251, 1103, 13391, 284, 852, 1588, 11, 543, 4327, 3812, 1910, 24288, 290, 43440, 287, 24531, 22139, 11, 772, 1231, 262, 3224, 4419, 286, 5602, 12, 38515, 4069, 13, 198, 10265, 777, 3858, 286, 1910, 1176, 4439, 1919, 9089, 355, 9611, 5298, 4536, 393, 4239, 5127, 284, 5713, 28393, 13, 1114, 1111, 3858, 11, 262, 4755, 286, 5436, 447, 247, 82, 2882, 318, 284, 4532, 4536, 284, 4646, 393, 11005, 262, 28393, 13, 554, 262, 717, 2099, 5436, 815, 2496, 262, 28393, 11, 407, 262, 5602, 12, 38515, 4069, 11, 780, 262, 2842, 284, 16417, 14725, 284, 5726, 389, 1165, 15641, 290, 6409, 284, 1630, 606, 477, 11, 290, 262, 28393, 960, 35569, 5436, 447, 247, 82, 9672, 31350, 12971, 290, 1366, 1895, 960, 533, 5365, 2562, 284, 12414, 13, 3412, 611, 262, 18645, 1022, 3487, 3139, 5860, 290, 28393, 318, 24163, 290, 23162, 306, 42550, 357, 20777, 340, 8338, 11, 1871, 584, 1243, 11, 319, 262, 2526, 1272, 286, 262, 13953, 828, 772, 6702, 18591, 262, 28393, 481, 9257, 4646, 393, 11005, 16538, 329, 5602, 12, 38515, 11, 523, 428, 2882, 784, 351, 15068, 290, 17137, 625, 640, 784, 318, 257, 1844, 4610, 13, 554, 262, 2368, 2099, 11, 810, 1910, 1176, 373, 32455, 2727, 832, 5602, 12, 38515, 4069, 11, 37895, 262, 28393, 481, 7719, 257, 1441, 3812, 7606, 3403, 355, 5602, 12, 38515, 4069, 24459, 13, 198, 818, 262, 1218, 2099, 11, 2158, 11, 262, 13542, 3812, 1910, 1176, 318, 11519, 287, 262, 1910, 447, 247, 82, 1575, 4645, 290, 481, 407, 307, 15254, 416, 37895, 262, 28393, 13, 10968, 11, 1719, 530, 393, 257, 1178, 9611, 17863, 884, 5939, 318, 18118, 39512, 13, 383, 1917, 318, 407, 262, 1910, 24288, 583, 384, 11, 475, 262, 43440, 3663, 284, 5298, 4536, 290, 697, 24508, 28393, 13, 383, 4610, 757, 318, 329, 5436, 284, 900, 4536, 284, 8006, 262, 28393, 13, 8554, 5436, 287, 428, 835, 6840, 8186, 728, 2494, 12, 1659, 12, 7783, 9001, 329, 3288, 30797, 444, 11, 2845, 326, 428, 2882, 318, 5625, 407, 655, 284, 257, 1178, 662, 12, 19107, 3288, 30797, 444, 475, 284, 597, 4081, 697, 622, 278, 2383, 28393, 13, 12495, 30797, 444, 11, 2158, 960, 37675, 9554, 290, 1854, 3025, 1910, 1176, 2058, 422, 3127, 7097, 871, 960, 25579, 530, 3224, 13357, 329, 5436, 13, 4650, 884, 9611, 14561, 511, 1910, 1176, 11476, 832, 8945, 326, 389, 555, 30883, 11, 1912, 319, 262, 5163, 286, 10966, 1479, 2594, 329, 2614, 1366, 11, 1690, 739, 2846, 286, 2139, 326, 18611, 262, 2846, 286, 5163, 13, 2893, 612, 743, 307, 1969, 15075, 444, 284, 10224, 1910, 1176, 287, 9611, 447, 247, 2694, 284, 13551, 777, 2846, 11, 340, 318, 407, 1598, 326, 777, 6958, 389, 3938, 4284, 89, 540, 287, 2846, 286, 1910, 1176, 13, 1675, 262, 6287, 777, 9611, 719, 588, 30797, 1023, 11, 428, 481, 307, 22363, 287, 262, 13045, 286, 584, 3519, 8945, 11, 884, 355, 6301, 7977, 8560, 1912, 319, 46500, 286, 2836, 12, 41279, 1366, 13, 383, 3376, 2450, 2882, 318, 10061, 11, 290, 743, 4745, 319, 6647, 3519, 284, 1366, 9238, 290, 779, 326, 561, 307, 4553, 422, 5436, 13, 33238, 884, 4788, 389, 287, 1295, 290, 4050, 11, 262, 5637, 1693, 329, 5436, 318, 1752, 757, 13720, 290, 37895, 262, 28393, 960, 64, 1693, 329, 543, 262, 1366, 2476, 389, 2092, 284, 644, 5436, 318, 1541, 1262, 25, 9611, 447, 247, 14614, 12779, 290, 5387, 14317, 1366, 11, 5556, 7008, 447, 247, 15387, 2148, 257, 922, 4308, 284, 34404, 16533, 286, 5046, 290, 8354, 290, 262, 28393, 10944, 422, 606, 13, 198, 464, 2368, 2099, 286, 1910, 1176, 12073, 517, 2383, 2450, 6459, 13, 7023, 4034, 422, 6282, 290, 11044, 11, 290, 6101, 1099, 1013, 364, 1910, 1176, 287, 1502, 284, 2251, 16538, 329, 777, 4568, 13, 11303, 3034, 5410, 4040, 750, 407, 1620, 880, 319, 428, 4776, 11, 290, 547, 12318, 329, 852, 19222, 11, 20831, 11, 336, 375, 1360, 11, 290, 14394, 287, 11044, 13, 7896, 2280, 11560, 4996, 11, 11044, 11, 290, 16389, 11, 481, 2380, 257, 4427, 329, 5436, 7310, 422, 883, 6693, 4145, 1290, 13, 1374, 714, 5436, 6840, 7719, 777, 3815, 960, 265, 1551, 355, 880, 355, 11, 393, 11481, 1365, 621, 11, 262, 1944, 1080, 286, 5939, 5556, 6101, 1099, 30, 198, 2514, 2074, 428, 1808, 11, 340, 318, 4465, 284, 13869, 2074, 1180, 7370, 286, 5046, 290, 31650, 287, 11044, 13, 1629, 262, 18197, 5046, 11, 11044, 32067, 656, 4996, 287, 5939, 11, 355, 10084, 3186, 290, 9824, 389, 4438, 284, 20825, 284, 14445, 32269, 18221, 290, 15387, 329, 31650, 13, 30251, 466, 428, 2495, 880, 11, 6032, 4955, 257, 5022, 286, 1029, 12, 29048, 7017, 329, 8661, 18221, 290, 47543, 393, 3748, 3709, 329, 9137, 18221, 13, 1114, 5436, 284, 2872, 393, 4405, 428, 2854, 318, 5688, 257, 1366, 1917, 26, 611, 339, 468, 17338, 3734, 12, 2164, 1328, 1366, 11, 339, 815, 307, 1498, 284, 5911, 1111, 7172, 15387, 290, 3227, 6443, 329, 257, 3094, 4996, 286, 7017, 13, 16126, 7886, 5436, 4249, 23097, 709, 666, 5436, 13267, 644, 318, 4438, 287, 19497, 11, 286, 1781, 25, 484, 691, 900, 4536, 393, 2756, 16895, 329, 3186, 326, 1910, 10544, 389, 1541, 6011, 13, 5436, 460, 779, 465, 2756, 12, 33990, 4934, 284, 7719, 4996, 416, 852, 7995, 284, 12291, 290, 1487, 287, 7172, 18221, 290, 23404, 11408, 508, 2897, 5337, 393, 1729, 12, 20307, 3186, 326, 617, 661, 765, 13, 679, 1244, 2252, 2620, 262, 11530, 284, 31650, 11, 416, 13622, 7008, 447, 247, 15387, 329, 257, 4996, 286, 3709, 852, 4438, 772, 611, 484, 466, 407, 27606, 15000, 606, 355, 281, 3038, 1988, 326, 6870, 257, 3967, 409, 759, 1483, 13, 10968, 11, 351, 257, 1402, 3154, 3101, 286, 465, 1693, 6764, 11, 5436, 714, 6152, 11408, 546, 6196, 10966, 6443, 618, 339, 39382, 257, 12741, 329, 4996, 326, 318, 407, 852, 1138, 13, 554, 3090, 11, 5436, 447, 247, 82, 1693, 286, 48001, 1729, 12, 36934, 6652, 1910, 10368, 481, 4327, 284, 7719, 4996, 286, 3186, 11, 355, 257, 1735, 12, 10760, 286, 11560, 9573, 286, 9611, 13, 198, 1722, 356, 2074, 11044, 326, 14582, 3675, 1944, 1720, 12291, 11, 5436, 743, 407, 307, 1498, 284, 12414, 15387, 329, 5337, 7017, 326, 389, 407, 27606, 4438, 13, 679, 714, 7301, 18221, 393, 3227, 6443, 3675, 262, 1944, 10330, 416, 1167, 801, 278, 4536, 284, 10630, 7719, 1402, 12291, 11, 788, 6152, 11408, 546, 6443, 290, 7719, 511, 13936, 832, 1402, 12291, 287, 4536, 13, 554, 1245, 11, 5436, 561, 788, 307, 14523, 1402, 10256, 11, 12577, 11408, 284, 2897, 649, 1243, 329, 5466, 357, 1525, 257, 6087, 286, 11776, 284, 9611, 290, 17070, 13045, 828, 788, 9646, 2482, 290, 22000, 18369, 287, 2882, 357, 17776, 416, 6087, 286, 4955, 1321, 11, 6011, 11776, 11, 290, 17070, 13045, 737, 2312, 1402, 2458, 284, 5436, 447, 247, 82, 4560, 714, 1577, 12949, 31822, 284, 11044, 960, 265, 1551, 1402, 11, 29497, 25438, 11, 517, 22107, 284, 6977, 290, 1486, 11044, 621, 14614, 11044, 960, 8869, 644, 314, 869, 257, 564, 250, 17121, 20400, 447, 251, 9030, 13, 4310, 198, 1532, 884, 1402, 39180, 2870, 11044, 319, 262, 10330, 286, 1459, 18369, 318, 19589, 19022, 11, 5436, 714, 7719, 4025, 11044, 416, 14523, 14614, 371, 5, 35, 11, 393, 772, 5654, 2267, 13, 770, 561, 2380, 257, 8904, 7118, 286, 5436, 447, 247, 82, 1693, 6764, 13, 632, 561, 635, 1944, 257, 1588, 12, 9888, 2450, 3572, 11, 5115, 1771, 284, 2661, 357, 259, 2035, 4571, 8, 11044, 290, 16389, 416, 661, 11, 393, 416, 5436, 290, 584, 317, 3792, 13, 4051, 11518, 714, 2989, 625, 4683, 290, 5150, 8514, 290, 3519, 21216, 290, 5654, 290, 6276, 9285, 11, 284, 5911, 11781, 20241, 329, 5963, 13, 1318, 389, 1541, 5895, 286, 9552, 3341, 46017, 884, 9889, 26, 329, 1672, 11, 281, 9552, 1080, 447, 247, 82, 2274, 5373, 287, 257, 5654, 8414, 284, 4331, 262, 24650, 4645, 286, 15568, 422, 511, 23206, 12, 46309, 16311, 11, 2816, 407, 284, 3068, 9552, 447, 247, 82, 3957, 1943, 287, 3597, 12121, 10165, 357, 272, 9552, 373, 257, 17490, 12, 929, 287, 257, 2274, 5337, 12, 16502, 8414, 828, 3980, 290, 49760, 27255, 475, 588, 540, 2647, 287, 7368, 12186, 13, 3553, 198, 1858, 743, 307, 11800, 7476, 287, 17965, 319, 5436, 329, 11044, 290, 6282, 13, 383, 3186, 286, 1692, 16389, 743, 13238, 422, 5436, 447, 247, 82, 5072, 393, 743, 307, 17560, 517, 4047, 329, 28327, 3840, 772, 611, 407, 3799, 1346, 1180, 13, 25929, 11, 7325, 12527, 290, 4568, 1244, 307, 19589, 3306, 329, 1692, 4086, 393, 46240, 13, 10968, 11, 11044, 290, 6282, 960, 10197, 14614, 11044, 11, 475, 2592, 17290, 11, 1919, 11, 290, 1964, 11044, 960, 29810, 2222, 19911, 290, 5358, 13, 383, 7325, 37505, 743, 39779, 287, 2176, 27570, 4658, 393, 40447, 11, 287, 27337, 329, 2116, 12, 46758, 290, 5408, 11, 393, 287, 5337, 1964, 393, 1919, 26096, 26, 290, 484, 743, 1111, 307, 29579, 416, 11, 290, 29791, 11, 617, 4922, 286, 35164, 11, 25800, 11, 393, 11616, 13, 4377, 286, 777, 743, 2148, 3840, 284, 4179, 5436, 447, 247, 82, 2597, 287, 11044, 393, 6282, 960, 1640, 1672, 11, 611, 5436, 447, 247, 82, 28892, 5072, 15117, 1095, 1692, 16294, 11, 393, 611, 262, 10152, 290, 17843, 286, 11044, 416, 5436, 739, 23779, 1593, 7767, 286, 1919, 11044, 416, 8868, 23822, 290, 41393, 11, 290, 523, 37847, 848, 3468, 1981, 393, 26877, 4086, 13, 198, 1532, 340, 318, 19589, 1593, 284, 35065, 6282, 290, 11044, 416, 5384, 11, 2035, 287, 10730, 351, 393, 2427, 286, 5436, 11, 5436, 714, 1486, 290, 3494, 4788, 284, 35065, 777, 11, 2192, 1365, 621, 1459, 6101, 2450, 13, 679, 714, 2148, 16538, 1262, 262, 976, 18537, 286, 4788, 10491, 5150, 355, 14693, 284, 6101, 11, 2035, 409, 29692, 416, 13172, 447, 247, 82, 9400, 393, 1575, 37030, 11, 393, 409, 1281, 416, 23844, 12, 16345, 21740, 393, 2756, 21884, 2087, 284, 3544, 286, 534, 7325, 670, 13, 679, 1244, 772, 307, 1498, 284, 4659, 262, 1919, 1988, 286, 25438, 11, 290, 319, 326, 4308, 900, 16586, 16538, 284, 7719, 18118, 39512, 11044, 1231, 35132, 1806, 1588, 2344, 7207, 28393, 13, 198, 34, 13, 5436, 447, 247, 82, 17113, 33737, 25, 1423, 16335, 21759, 1850, 393, 19015, 2301, 515, 360, 13221, 602, 30, 198, 32, 1994, 1808, 287, 16215, 5436, 447, 247, 82, 15171, 481, 307, 379, 644, 5046, 286, 46500, 339, 15947, 4536, 393, 2756, 751, 364, 13, 2561, 2628, 286, 17338, 2092, 8945, 307, 13262, 515, 11, 287, 1245, 13622, 606, 588, 530, 1910, 351, 530, 2756, 393, 2756, 12, 26676, 30, 1471, 481, 5436, 787, 4553, 16765, 329, 790, 8611, 11, 3748, 284, 1123, 6087, 286, 17872, 11, 18583, 11, 290, 2378, 1007, 23800, 30, 198, 1212, 1808, 6630, 12362, 2769, 287, 703, 5436, 318, 3562, 290, 644, 12031, 339, 318, 1498, 284, 10660, 13, 1002, 5436, 318, 21581, 355, 281, 409, 759, 1483, 12, 13049, 278, 290, 5602, 12, 2302, 974, 278, 4572, 11, 262, 3280, 481, 4745, 319, 881, 777, 7565, 1973, 8945, 11, 290, 4145, 379, 644, 1241, 286, 46500, 5400, 1871, 8945, 2300, 329, 1919, 23989, 13, 921, 1244, 1607, 326, 329, 1588, 3146, 286, 2092, 3186, 11, 925, 287, 262, 976, 393, 2092, 17590, 11, 5400, 287, 7097, 871, 1973, 8945, 1244, 307, 845, 1402, 13, 15298, 11, 28393, 1244, 697, 24508, 284, 9611, 379, 257, 2092, 2494, 1973, 1588, 1271, 286, 8945, 13, 4698, 777, 3403, 11, 612, 1244, 307, 1402, 9089, 422, 1919, 6436, 1483, 287, 13262, 803, 1973, 1588, 3146, 286, 8945, 11, 351, 1588, 20691, 287, 31350, 290, 1366, 10538, 357, 27078, 757, 11, 611, 29964, 318, 407, 1107, 1575, 1203, 11, 523, 356, 1337, 546, 777, 27127, 737, 198, 2953, 262, 976, 640, 11, 24171, 1123, 8611, 17033, 561, 1280, 510, 257, 3665, 2837, 286, 3224, 2450, 4661, 329, 5436, 11, 17728, 1111, 262, 2785, 329, 1588, 4034, 11, 290, 8904, 7476, 13, 2195, 27289, 1123, 8611, 17033, 11, 5436, 714, 2074, 3294, 12608, 286, 1111, 262, 1720, 22112, 290, 262, 4671, 284, 262, 8611, 11, 1390, 407, 655, 8611, 12, 11423, 7097, 871, 475, 635, 3416, 1187, 286, 1981, 5127, 290, 3512, 9695, 11, 393, 772, 3224, 2151, 12608, 3675, 777, 13, 27662, 5127, 290, 3512, 9695, 3436, 11, 5436, 714, 760, 17872, 447, 247, 82, 290, 18583, 447, 247, 82, 24048, 4536, 329, 790, 8611, 11, 290, 523, 24340, 2818, 2756, 8839, 11, 351, 262, 3580, 326, 11, 287, 6273, 284, 2035, 2756, 8839, 416, 257, 30797, 396, 393, 24537, 23189, 11, 5436, 460, 14083, 262, 1695, 18201, 422, 790, 8611, 287, 1627, 351, 465, 1919, 9490, 2163, 13, 770, 7297, 561, 14572, 4079, 617, 6721, 284, 1877, 12, 15805, 11408, 290, 617, 4414, 12, 21987, 284, 14456, 351, 1029, 16826, 284, 1414, 11, 11476, 2186, 12364, 262, 22577, 6082, 286, 18201, 326, 561, 3051, 611, 8945, 389, 13262, 515, 656, 32551, 12, 34162, 13, 198, 1537, 5436, 714, 635, 6061, 428, 12971, 287, 584, 2842, 13, 679, 714, 11, 329, 1672, 11, 8076, 355, 257, 3665, 3113, 284, 4646, 1919, 12791, 416, 49065, 1123, 8611, 18703, 453, 287, 326, 4571, 25, 287, 6273, 284, 7226, 10906, 287, 1944, 1910, 12, 3106, 3341, 11, 5436, 714, 3877, 3595, 14456, 1342, 290, 1414, 3595, 23531, 517, 11, 523, 1123, 8611, 22625, 257, 1402, 7741, 284, 12791, 13, 16374, 2756, 8839, 329, 1981, 8945, 561, 635, 7139, 5436, 284, 1011, 617, 2648, 286, 790, 8611, 447, 247, 82, 18201, 379, 257, 1687, 13, 770, 561, 2380, 2818, 21843, 351, 645, 477, 23466, 287, 45888, 357, 273, 2636, 6551, 2994, 8, 780, 477, 1687, 13089, 561, 1282, 422, 1167, 430, 12, 30887, 1292, 28393, 290, 4145, 423, 645, 477, 23466, 1245, 13, 3365, 198, 35392, 15068, 286, 790, 8611, 635, 12073, 1598, 4786, 13, 1629, 257, 5288, 11, 1981, 1143, 8611, 8922, 14754, 262, 48291, 18685, 286, 1910, 8945, 960, 64, 2994, 286, 6782, 11, 3584, 314, 4099, 6782, 318, 3750, 287, 5436, 12, 6894, 287, 597, 1339, 13, 4380, 423, 32335, 517, 6782, 422, 5436, 621, 484, 466, 422, 281, 22284, 271, 3456, 26462, 11, 3584, 5436, 714, 991, 1805, 661, 447, 247, 82, 2839, 1321, 422, 584, 661, 290, 5745, 13, 198, 1537, 612, 389, 584, 4786, 5545, 416, 1981, 1143, 8611, 21837, 11, 3519, 284, 262, 12536, 319, 543, 5436, 1838, 777, 5370, 13, 314, 423, 3417, 5436, 447, 247, 82, 10033, 2597, 355, 39038, 1910, 15536, 290, 423, 14537, 6096, 286, 16083, 8018, 7097, 871, 326, 389, 1588, 290, 4632, 28605, 46927, 11, 884, 355, 6142, 34859, 5556, 3725, 11, 1535, 11, 290, 6467, 19431, 13801, 13, 887, 1981, 1143, 8611, 21837, 11, 287, 3090, 284, 9616, 5436, 3189, 3734, 12, 2164, 1328, 17952, 290, 17137, 286, 7097, 871, 11, 561, 635, 2251, 12475, 602, 284, 44870, 262, 19759, 286, 7097, 871, 287, 2842, 326, 2221, 284, 22464, 9815, 1919, 8705, 11, 8620, 6196, 2726, 4786, 546, 12354, 290, 21851, 13, 1081, 14614, 4371, 523, 1690, 857, 11, 262, 5885, 286, 5436, 9808, 1705, 20241, 286, 1981, 290, 10098, 3572, 326, 1239, 4271, 550, 284, 307, 3177, 11, 329, 543, 5370, 389, 783, 2672, 1771, 11, 290, 703, 11, 284, 779, 606, 13, 198, 1890, 1672, 11, 2074, 262, 6034, 286, 13622, 6538, 9490, 960, 64, 10733, 326, 318, 1593, 11, 4047, 7885, 11, 290, 5688, 555, 30883, 960, 292, 281, 409, 759, 1483, 286, 3227, 13, 376, 8789, 290, 11663, 3360, 787, 511, 3259, 22444, 11, 290, 4827, 5939, 389, 407, 523, 2818, 326, 19283, 3259, 26995, 1445, 284, 5559, 7184, 326, 5732, 511, 9490, 13, 5436, 714, 2190, 428, 355, 257, 7144, 540, 409, 759, 1483, 11, 23634, 2890, 11408, 290, 23531, 416, 20814, 644, 561, 2033, 284, 281, 564, 250, 403, 34191, 8383, 1687, 13, 447, 251, 887, 611, 5436, 318, 10435, 284, 2190, 19546, 11663, 355, 257, 3376, 540, 4633, 409, 759, 1483, 286, 3227, 11, 644, 318, 284, 2245, 683, 422, 1804, 262, 976, 329, 661, 508, 719, 11234, 287, 584, 2842, 11, 393, 287, 584, 9176, 30, 13111, 1692, 4069, 34859, 584, 661, 772, 611, 340, 2753, 1295, 2354, 262, 15383, 13, 2080, 5436, 287, 1295, 11, 612, 561, 307, 3489, 12475, 602, 284, 22432, 517, 20628, 2280, 11, 1642, 1981, 1143, 24195, 286, 1919, 17004, 1912, 319, 6515, 393, 41240, 4069, 393, 14479, 13, 2773, 23176, 1919, 42351, 1244, 765, 5436, 284, 1687, 661, 351, 3200, 410, 1063, 2354, 511, 670, 3160, 11, 1036, 32152, 661, 11, 661, 351, 595, 12, 69, 48275, 4158, 9056, 11, 6283, 12, 11534, 661, 11, 290, 523, 319, 13, 30251, 1541, 466, 428, 11, 286, 1781, 11, 23404, 393, 23634, 2890, 661, 329, 1243, 326, 389, 18046, 284, 511, 10270, 287, 3034, 3227, 960, 273, 815, 307, 960, 4360, 5436, 561, 2251, 262, 2694, 284, 2035, 4646, 884, 47543, 3513, 393, 2620, 340, 11, 6196, 1231, 5421, 13, 198, 16678, 9889, 561, 1944, 262, 48367, 6034, 286, 38193, 3812, 26551, 829, 462, 290, 800, 33243, 8839, 284, 1104, 4232, 3815, 11, 15387, 11, 290, 39800, 389, 27606, 11410, 960, 35131, 262, 3741, 11, 393, 1871, 16958, 3011, 284, 4588, 5436, 447, 247, 82, 9432, 2163, 960, 392, 257, 11622, 18598, 284, 257, 27377, 2801, 16813, 1181, 13, 383, 976, 1981, 1143, 3416, 602, 326, 7139, 5436, 284, 2818, 262, 14748, 286, 1919, 6436, 1483, 635, 7139, 683, 284, 5517, 555, 562, 1508, 11, 1981, 1143, 29977, 832, 1844, 1630, 286, 3925, 11, 772, 625, 6067, 880, 1626, 262, 6516, 286, 39715, 1981, 12354, 11, 416, 13045, 511, 4827, 290, 16215, 262, 2846, 286, 477, 511, 7327, 6443, 13, 5436, 714, 8076, 588, 257, 3009, 7251, 11, 2845, 29682, 517, 3665, 11, 32042, 9388, 13, 2312, 4786, 2148, 1913, 1738, 284, 5490, 546, 262, 6770, 286, 5436, 447, 247, 82, 9432, 2163, 11, 6693, 287, 7275, 6711, 13, 37, 13, 2174, 13, 198, 35, 13, 5521, 5155, 290, 35412, 32404, 4698, 5436, 198, 40, 716, 12059, 5436, 287, 2846, 326, 389, 257, 13516, 286, 1468, 12, 28776, 1579, 15405, 290, 34264, 11, 475, 356, 1276, 407, 739, 12, 395, 1920, 262, 13522, 286, 1964, 13389, 326, 5436, 714, 2380, 11, 393, 262, 12245, 286, 3917, 1964, 12333, 13, 383, 749, 49156, 15225, 286, 2785, 5358, 625, 5436, 389, 1884, 284, 307, 1022, 3259, 290, 11390, 357, 1169, 11663, 393, 4393, 286, 23941, 8, 290, 1022, 883, 379, 262, 1353, 11, 3504, 11, 290, 4220, 286, 262, 21790, 12, 17079, 3722, 18911, 13, 2312, 15225, 286, 7297, 44672, 30634, 11, 290, 20431, 523, 13, 5436, 12073, 2683, 286, 262, 9238, 290, 1630, 286, 262, 1724, 286, 3227, 287, 257, 9815, 290, 7531, 835, 11, 290, 523, 3264, 12073, 8157, 11, 890, 12, 5646, 1964, 12766, 13, 198, 2396, 318, 5436, 19803, 3270, 851, 392, 611, 339, 318, 11, 318, 326, 257, 2089, 1517, 393, 257, 922, 1517, 30, 1471, 284, 2962, 319, 1103, 3048, 2138, 621, 1964, 14722, 11, 644, 561, 5436, 1612, 329, 262, 1204, 290, 9490, 286, 3259, 290, 329, 262, 14735, 290, 3416, 1187, 286, 1919, 12791, 30, 2011, 14895, 329, 262, 5517, 1234, 617, 17778, 319, 777, 2683, 13, 4380, 991, 670, 11, 475, 1290, 7380, 621, 1909, 13, 843, 484, 466, 523, 407, 655, 355, 12776, 602, 393, 287, 14748, 286, 28327, 12031, 11, 475, 635, 284, 8676, 284, 262, 3227, 286, 10348, 7017, 290, 2594, 287, 262, 3773, 11, 284, 617, 4922, 287, 2882, 284, 22820, 1040, 291, 28140, 13, 198, 464, 1588, 12, 9888, 29358, 286, 4827, 4145, 9672, 318, 355, 43590, 257, 6482, 355, 318, 1719, 5436, 1057, 262, 3773, 13, 6430, 340, 318, 991, 635, 257, 3614, 13196, 11, 780, 262, 29358, 286, 4827, 318, 407, 1844, 13, 1318, 389, 1588, 3146, 286, 661, 1111, 1762, 290, 645, 2392, 1762, 13, 383, 1807, 6306, 4145, 12073, 734, 2769, 2683, 11, 1111, 890, 4318, 284, 262, 15735, 5358, 1022, 19803, 290, 12129, 960, 1169, 3450, 286, 1762, 1204, 290, 9490, 286, 3259, 11, 290, 1919, 10537, 13, 198, 37, 8789, 290, 584, 1588, 5745, 11, 772, 883, 326, 8277, 287, 5939, 45107, 11, 4632, 8076, 20947, 407, 416, 1910, 8945, 475, 416, 4934, 12, 17078, 5410, 13, 1119, 389, 4145, 11640, 14807, 286, 5410, 1626, 1910, 3341, 11, 4955, 257, 3665, 46472, 282, 284, 35010, 35871, 286, 703, 15098, 16533, 8076, 26, 1899, 290, 14807, 286, 22612, 1630, 286, 3259, 416, 4542, 11, 407, 8389, 1863, 10518, 7811, 13, 5333, 16847, 9199, 284, 777, 6958, 329, 3294, 3840, 11, 475, 257, 45718, 530, 468, 587, 326, 484, 761, 262, 3739, 13, 5237, 198, 464, 9672, 5046, 286, 5436, 447, 247, 82, 4934, 12073, 1111, 2683, 287, 649, 5107, 13, 1002, 1290, 7380, 661, 389, 1762, 11, 340, 318, 645, 2392, 2035, 23498, 393, 22388, 10909, 284, 779, 9400, 422, 7184, 355, 262, 1388, 4308, 284, 14983, 3739, 290, 584, 1919, 11530, 13, 887, 611, 777, 389, 407, 5295, 416, 10906, 286, 4827, 5939, 11, 788, 508, 3011, 644, 290, 703, 318, 340, 3066, 30, 4231, 477, 4961, 11, 355, 583, 2829, 11628, 326, 262, 2450, 2882, 284, 9552, 318, 257, 10112, 4096, 3739, 357, 52, 3483, 19427, 1471, 611, 484, 389, 991, 47543, 11, 788, 319, 644, 4308, 30, 1148, 5436, 2950, 287, 777, 3416, 602, 30, 2312, 17700, 306, 1593, 2683, 546, 703, 284, 3031, 284, 9552, 12, 15808, 29358, 286, 7184, 11, 290, 262, 17881, 1590, 286, 471, 3483, 355, 257, 2882, 11, 389, 10233, 286, 8157, 1459, 4384, 11, 475, 314, 466, 407, 8209, 606, 994, 13, 198, 1537, 772, 611, 5436, 318, 407, 2950, 287, 262, 4045, 12123, 286, 11530, 290, 262, 4922, 290, 4308, 286, 1919, 12791, 11, 314, 2314, 3938, 3368, 262, 1808, 286, 703, 5436, 32902, 351, 262, 2846, 290, 3403, 286, 7184, 329, 883, 508, 389, 1762, 11, 780, 777, 2683, 389, 17707, 5884, 351, 5436, 447, 247, 82, 1693, 286, 2491, 262, 12973, 3773, 13, 44536, 326, 47579, 447, 247, 82, 5410, 1080, 15009, 4827, 5939, 290, 2457, 7327, 7017, 422, 663, 8354, 11, 31414, 4305, 777, 3006, 284, 1910, 12213, 13, 1320, 6870, 530, 257, 1744, 3280, 287, 616, 1807, 6306, 994, 11, 475, 340, 318, 991, 3306, 284, 670, 832, 262, 1808, 290, 262, 10939, 286, 428, 1863, 351, 584, 1744, 7429, 13, 198, 464, 1808, 286, 262, 3403, 290, 2846, 286, 883, 1762, 318, 17707, 5884, 284, 262, 2683, 286, 508, 318, 1762, 11, 508, 13267, 11, 290, 319, 644, 4308, 13, 5338, 991, 468, 3946, 287, 262, 4931, 286, 5436, 30, 770, 481, 307, 5295, 416, 617, 6087, 286, 508, 3382, 284, 670, 11, 290, 644, 4678, 389, 991, 2622, 13, 770, 12123, 481, 423, 284, 2074, 262, 31146, 14445, 32269, 2095, 286, 670, 290, 3946, 11, 1111, 287, 511, 748, 343, 1799, 290, 287, 262, 4678, 2672, 284, 466, 606, 13, 198, 48142, 612, 318, 617, 10909, 1080, 287, 1295, 284, 14983, 26877, 4133, 1871, 661, 960, 292, 612, 1276, 307, 739, 597, 5642, 286, 11982, 9552, 12, 15808, 19911, 286, 4827, 5939, 290, 262, 11622, 3773, 11, 1771, 6856, 416, 5436, 11, 1910, 3386, 11, 393, 584, 1724, 960, 270, 460, 645, 2392, 307, 49469, 284, 307, 21021, 13, 1081, 257, 1255, 11, 262, 2372, 286, 884, 49469, 1204, 3403, 481, 645, 2392, 307, 1695, 355, 281, 15660, 284, 21155, 661, 284, 670, 357, 34750, 286, 262, 1808, 1771, 340, 481, 307, 11, 393, 1683, 373, 11, 22388, 10909, 737, 2773, 661, 481, 765, 284, 670, 11, 329, 28327, 3840, 13, 770, 1244, 307, 1178, 661, 393, 867, 11, 523, 340, 318, 407, 1598, 287, 2276, 1771, 1692, 4827, 318, 1884, 284, 307, 287, 18772, 393, 18201, 13, 10968, 11, 4232, 262, 5127, 12, 28550, 5236, 329, 2276, 1692, 4827, 4045, 11, 262, 3773, 481, 2555, 284, 2421, 4827, 422, 661, 351, 2176, 4678, 326, 2314, 1865, 307, 16359, 13, 198, 28516, 481, 991, 1612, 617, 4922, 286, 36610, 3929, 1630, 290, 24353, 284, 4571, 13, 1320, 481, 307, 262, 1339, 739, 597, 1080, 286, 1588, 12, 9888, 3227, 19877, 11, 416, 597, 6087, 286, 5939, 11, 4318, 5410, 416, 5436, 11, 393, 4934, 2316, 1626, 9611, 13, 1114, 661, 1762, 3264, 329, 5436, 2354, 9611, 11, 326, 1630, 481, 307, 16992, 11, 5361, 832, 262, 900, 286, 2756, 6443, 393, 16895, 326, 5436, 4394, 329, 1762, 319, 1948, 8861, 13, 12511, 9611, 11, 3224, 1630, 481, 307, 25805, 416, 11663, 11, 1771, 777, 389, 661, 393, 9552, 13, 49274, 617, 10883, 25625, 1634, 286, 10098, 10510, 11, 262, 2846, 286, 670, 1204, 460, 307, 6159, 3938, 16171, 329, 1981, 3259, 4249, 3938, 10518, 379, 262, 10098, 1241, 11, 1813, 262, 761, 329, 617, 4025, 12, 9888, 19877, 9030, 13, 198, 37, 8789, 5361, 739, 5436, 481, 991, 423, 284, 16481, 3227, 6840, 290, 1630, 3484, 13, 10968, 11, 2426, 284, 5436, 447, 247, 82, 30257, 21922, 286, 262, 14735, 286, 28393, 3142, 11, 484, 481, 960, 392, 1276, 329, 511, 5387, 2551, 12, 8601, 284, 26995, 10548, 284, 1588, 12, 9888, 26877, 2476, 960, 14150, 16538, 284, 5160, 10177, 13, 471, 4852, 666, 26096, 7263, 11, 428, 15565, 326, 9611, 1276, 991, 3360, 1277, 4409, 284, 466, 1243, 484, 561, 2138, 407, 466, 290, 1276, 3360, 6735, 3259, 508, 389, 407, 14329, 393, 3025, 4678, 389, 645, 2392, 2622, 13, 887, 379, 262, 976, 640, 11, 262, 1692, 21147, 286, 4827, 5939, 481, 307, 9257, 5322, 739, 5436, 11, 8868, 393, 18591, 32000, 284, 1011, 7184, 13, 770, 481, 2380, 257, 7531, 13389, 287, 262, 3403, 286, 3259, 447, 247, 3160, 13, 198, 464, 1844, 1998, 286, 7184, 960, 24815, 262, 9400, 393, 584, 9836, 11, 262, 2095, 286, 8861, 290, 262, 2858, 287, 543, 484, 389, 6157, 11, 262, 12213, 351, 763, 12, 22896, 290, 11663, 11, 290, 262, 17764, 286, 7184, 351, 584, 1204, 12031, 290, 15171, 960, 27238, 287, 2472, 307, 10966, 1576, 284, 21155, 661, 284, 3853, 284, 466, 340, 11, 739, 262, 3403, 286, 3744, 11631, 283, 1042, 326, 1061, 422, 262, 4045, 5322, 761, 329, 3259, 13, 1374, 10966, 777, 3403, 1276, 307, 481, 4745, 319, 262, 3403, 286, 18772, 393, 18201, 326, 28615, 329, 3259, 351, 1948, 4678, 13, 383, 3744, 262, 18772, 11, 262, 517, 10966, 262, 21155, 902, 329, 7184, 1276, 307, 13, 775, 1244, 4143, 1607, 262, 14955, 286, 18772, 284, 307, 3744, 329, 16976, 4678, 11, 3584, 428, 761, 407, 6646, 307, 262, 1339, 13, 1649, 612, 318, 18772, 11, 11390, 481, 2897, 2440, 29497, 9400, 357, 24988, 37098, 3585, 284, 644, 262, 3259, 484, 761, 460, 3328, 329, 407, 1762, 8, 393, 584, 10966, 21155, 902, 13, 4698, 3403, 286, 8383, 18201, 329, 1948, 1693, 3858, 11, 428, 481, 407, 307, 262, 1339, 13, 9676, 11, 356, 1244, 772, 5967, 617, 3006, 810, 612, 318, 1310, 393, 645, 761, 284, 1414, 29497, 9400, 2029, 644, 1729, 12, 22896, 3328, 11, 991, 13148, 326, 883, 1204, 3403, 1695, 329, 1729, 12, 22896, 389, 18633, 11067, 355, 10909, 13, 3412, 351, 517, 661, 10291, 284, 670, 621, 9611, 761, 11, 262, 3421, 3403, 286, 10681, 481, 1234, 257, 4314, 319, 703, 22444, 3259, 460, 307, 960, 64, 4314, 326, 318, 407, 1944, 287, 1459, 4827, 5939, 13, 12645, 364, 447, 247, 1910, 1176, 625, 2846, 286, 7184, 481, 991, 7565, 351, 262, 18772, 393, 18201, 286, 1948, 4678, 475, 481, 1239, 307, 355, 3257, 355, 618, 2994, 286, 7184, 318, 22694, 13, 198, 19926, 23097, 709, 666, 5436, 307, 2950, 287, 4634, 9400, 290, 2846, 286, 7184, 30, 357, 18124, 5436, 6189, 481, 307, 2014, 314, 18077, 8287, 453, 326, 339, 815, 407, 11, 739, 14895, 286, 1336, 1321, 287, 8383, 12, 7033, 263, 23189, 290, 645, 7097, 871, 3264, 4073, 416, 7184, 5370, 13, 34579, 871, 422, 584, 3519, 5370, 460, 307, 19267, 287, 262, 8945, 810, 484, 15058, 13, 1002, 345, 670, 319, 257, 17656, 1720, 11, 5436, 481, 3376, 326, 409, 759, 1483, 8057, 287, 3227, 17311, 393, 2457, 1720, 5466, 11, 351, 645, 761, 284, 22432, 287, 534, 9400, 13, 4698, 883, 3403, 11, 5436, 460, 2666, 24462, 286, 7184, 11, 9400, 11, 290, 584, 1762, 3403, 284, 1910, 23189, 1022, 3259, 357, 28998, 13030, 416, 511, 9552, 29488, 8, 290, 511, 17530, 357, 10734, 393, 9552, 8, 11390, 13, 5066, 198, 36, 13, 16766, 5706, 14884, 14319, 7737, 30, 14161, 6923, 290, 14734, 4698, 5436, 198, 8496, 262, 3161, 5114, 286, 8383, 1204, 739, 5436, 11476, 9405, 2785, 21954, 284, 5436, 422, 262, 1364, 11, 428, 2665, 12031, 284, 2209, 617, 21954, 422, 262, 826, 13, 3412, 611, 5436, 1595, 447, 247, 83, 2033, 284, 1181, 22338, 286, 2839, 3119, 11, 2125, 447, 247, 83, 5436, 1969, 1576, 284, 5298, 477, 262, 976, 21954, 960, 325, 528, 495, 286, 1630, 611, 407, 8766, 9238, 1231, 9836, 11, 290, 7432, 284, 262, 3917, 12354, 5353, 286, 1111, 9611, 290, 4290, 30, 554, 1903, 9984, 286, 428, 1628, 11, 262, 7786, 395, 5107, 286, 428, 7734, 960, 45175, 11, 287, 1570, 286, 511, 1998, 960, 14150, 587, 4376, 416, 7810, 351, 2614, 393, 1641, 1998, 2877, 739, 262, 7570, 4479, 393, 584, 31454, 15889, 22612, 2585, 13, 2312, 44329, 1950, 326, 257, 2726, 6961, 284, 11206, 5436, 318, 379, 1266, 41492, 546, 34922, 2842, 5436, 561, 2033, 284, 11, 393, 25511, 1346, 1085, 284, 11, 44315, 605, 1181, 1176, 13, 198, 1026, 318, 1598, 326, 5436, 318, 281, 8875, 286, 29024, 32000, 319, 1910, 8945, 11, 290, 12891, 319, 262, 779, 290, 1630, 286, 2839, 3119, 11, 379, 1551, 329, 2839, 3119, 2950, 287, 3227, 13, 887, 262, 4922, 286, 1630, 11, 290, 4145, 262, 6287, 286, 34396, 319, 12354, 11, 481, 7565, 7634, 739, 1180, 5107, 286, 5436, 13, 198, 40, 8606, 39789, 5436, 329, 3840, 286, 4086, 2761, 290, 16538, 11, 475, 326, 1296, 286, 5436, 561, 635, 2380, 262, 749, 3257, 22338, 286, 1181, 1630, 11, 20232, 3227, 290, 5163, 13, 23591, 319, 703, 339, 318, 9177, 11, 39789, 5436, 1244, 635, 39793, 20232, 4827, 13, 2399, 555, 13635, 1799, 4145, 3568, 284, 307, 14904, 23444, 11, 1912, 319, 1111, 287, 10760, 6517, 290, 11071, 3927, 3193, 3257, 11734, 286, 12354, 13, 198, 18124, 5436, 290, 23097, 709, 666, 5436, 561, 991, 2380, 47236, 1181, 9572, 11, 475, 284, 14494, 7370, 13, 19174, 290, 5163, 8945, 561, 407, 307, 20232, 11, 475, 561, 307, 2426, 284, 49714, 10893, 3403, 13, 1114, 23097, 709, 666, 5436, 11, 777, 3403, 389, 10893, 355, 2756, 16895, 284, 8945, 326, 389, 4306, 16171, 13, 554, 1296, 11, 484, 561, 4145, 22464, 257, 1080, 286, 9815, 4200, 393, 1988, 12, 29373, 5704, 11, 9524, 416, 23970, 326, 428, 4922, 286, 34396, 318, 407, 257, 17851, 1146, 11071, 21597, 17504, 286, 12354, 11, 290, 743, 307, 655, 16823, 287, 1570, 286, 262, 1171, 12031, 852, 6190, 13, 770, 743, 307, 6751, 284, 4474, 262, 583, 3927, 2247, 286, 5436, 11, 475, 428, 481, 4745, 319, 262, 3307, 13, 198, 818, 6273, 284, 5385, 4200, 12, 19290, 3341, 11, 3025, 4007, 318, 284, 5298, 1230, 6426, 11, 5436, 447, 247, 82, 4007, 318, 8384, 284, 27401, 3034, 3227, 287, 18118, 19344, 11678, 290, 3376, 1910, 15536, 11, 981, 3737, 635, 8620, 6426, 355, 257, 9233, 4031, 13, 11259, 428, 4007, 11, 5436, 447, 247, 82, 2756, 16895, 481, 307, 517, 7885, 1973, 8945, 621, 883, 286, 4200, 5704, 11, 1390, 617, 286, 1111, 5895, 11, 290, 287, 617, 2663, 481, 307, 881, 4025, 13, 4698, 5436, 447, 247, 82, 4571, 11, 617, 3186, 351, 4457, 1029, 4633, 7097, 871, 743, 307, 7986, 503, 286, 19497, 11, 290, 617, 23941, 3025, 1597, 2746, 318, 4632, 393, 5000, 1912, 319, 4441, 393, 15852, 28393, 743, 307, 7986, 503, 286, 1597, 13, 198, 4711, 12031, 287, 7989, 6486, 1626, 262, 9829, 1308, 1177, 286, 10518, 2585, 13, 9676, 11, 7668, 1910, 12, 2301, 21386, 3341, 1690, 10660, 262, 976, 12031, 11, 3584, 416, 2972, 5107, 286, 7952, 9001, 1342, 11521, 351, 1910, 8945, 621, 5436, 561, 307, 13, 1629, 428, 1241, 286, 28991, 1152, 1483, 11, 340, 318, 1598, 326, 5436, 11, 379, 1551, 287, 465, 23097, 709, 666, 1296, 11, 318, 407, 17640, 11071, 21597, 287, 7270, 10518, 2585, 13, 198, 1537, 262, 3307, 2300, 13, 5436, 561, 5298, 1964, 10386, 11, 355, 10224, 9001, 857, 11, 1390, 262, 5885, 286, 3667, 326, 1913, 19901, 2033, 284, 11071, 21597, 34318, 641, 515, 256, 45665, 286, 2839, 3119, 13, 843, 597, 1296, 286, 5436, 481, 307, 257, 3665, 2891, 11, 1642, 32042, 3416, 602, 319, 8378, 286, 262, 1181, 3025, 6948, 389, 3360, 6049, 329, 1948, 23941, 393, 262, 1988, 286, 1948, 6798, 11, 772, 611, 407, 6067, 286, 1204, 290, 1918, 13, 679, 481, 4145, 2421, 49202, 326, 339, 691, 307, 12380, 284, 5963, 18633, 825, 27339, 11, 6768, 4888, 26877, 5353, 11, 407, 355, 281, 8875, 284, 13551, 11, 11777, 393, 37847, 11, 530, 17710, 447, 247, 82, 5761, 286, 262, 922, 1204, 11, 393, 511, 5353, 11, 319, 1854, 13, 383, 3403, 326, 5004, 1771, 5436, 318, 11670, 351, 257, 7270, 1181, 290, 3592, 481, 307, 34669, 290, 4732, 12, 11423, 13, 1119, 481, 4745, 319, 5436, 447, 247, 82, 9432, 2163, 290, 262, 1429, 416, 543, 340, 318, 4920, 11, 355, 6693, 287, 262, 1306, 2665, 13, 1119, 481, 4745, 319, 617, 9987, 286, 9823, 1483, 286, 3484, 10893, 3585, 284, 4034, 19189, 960, 3911, 306, 257, 2300, 286, 7187, 290, 34412, 31850, 286, 1919, 34859, 11, 11476, 257, 2300, 286, 15637, 44365, 416, 872, 2313, 287, 1588, 2458, 11835, 11, 329, 5436, 355, 329, 10224, 9001, 13, 843, 484, 481, 4745, 319, 27931, 38424, 355, 4800, 1028, 4049, 290, 9253, 11, 1390, 8617, 329, 7468, 286, 5370, 11, 4795, 2423, 11, 290, 17137, 393, 9836, 355, 19589, 32502, 13, 198, 37, 13, 1867, 447, 247, 82, 262, 25376, 30, 5436, 447, 247, 82, 37092, 15553, 290, 1374, 632, 29620, 4280, 1384, 198, 1135, 783, 1282, 284, 262, 734, 17612, 23163, 286, 2683, 326, 5436, 10969, 13, 3274, 11, 644, 3061, 857, 5436, 10660, 287, 26727, 465, 19901, 11, 290, 703, 960, 392, 416, 4150, 960, 271, 428, 3066, 30, 843, 1218, 11, 703, 1244, 356, 651, 284, 5436, 25, 644, 22963, 422, 1944, 3403, 284, 257, 3592, 351, 5436, 287, 1295, 1244, 307, 23498, 11, 1884, 11, 393, 18763, 26, 703, 466, 777, 15124, 284, 1944, 9889, 290, 11257, 26, 290, 644, 45716, 290, 7476, 466, 777, 22963, 1944, 30, 314, 1730, 351, 262, 717, 900, 286, 2683, 287, 428, 2665, 11, 262, 1218, 900, 287, 262, 1306, 13, 198, 2061, 3061, 11, 644, 19759, 286, 1919, 9490, 11, 857, 5436, 10660, 30, 554, 6276, 2846, 11, 644, 318, 5436, 447, 247, 82, 9432, 2163, 11, 290, 703, 318, 340, 5295, 30, 314, 423, 5545, 5436, 355, 281, 5559, 960, 273, 287, 262, 1339, 286, 23097, 709, 666, 5436, 11, 281, 16339, 14374, 290, 46534, 960, 1462, 5939, 13, 5991, 3341, 423, 257, 4752, 46219, 8489, 11, 37962, 287, 262, 564, 250, 259, 23504, 1021, 447, 251, 23094, 287, 4176, 447, 247, 82, 35151, 286, 7973, 2414, 290, 1568, 8766, 1143, 287, 262, 734, 7531, 262, 382, 907, 286, 32404, 18963, 13, 2996, 198, 1212, 46219, 1624, 8338, 319, 257, 1178, 1913, 14895, 13, 383, 6768, 8018, 290, 1690, 12, 17069, 515, 14895, 2672, 329, 3403, 286, 7138, 7606, 5939, 960, 12853, 1321, 11, 645, 1910, 1176, 11, 645, 7097, 871, 960, 13086, 749, 286, 5436, 447, 247, 82, 1693, 355, 6693, 4145, 1290, 11, 523, 314, 466, 407, 2209, 606, 2252, 994, 13, 198, 1537, 612, 389, 734, 584, 11, 517, 43936, 14895, 319, 543, 262, 4752, 1919, 6436, 1483, 286, 1910, 10906, 4745, 13, 2312, 14895, 1249, 5939, 960, 273, 517, 10582, 11, 16355, 286, 5939, 447, 247, 6436, 1483, 960, 1462, 3368, 1728, 1327, 2761, 326, 749, 5107, 286, 5436, 2314, 13, 3274, 11, 1910, 6436, 1483, 3667, 36059, 326, 661, 447, 247, 82, 1910, 7747, 26995, 7766, 511, 15387, 290, 511, 880, 12, 11873, 13, 5498, 11, 777, 3667, 8814, 319, 257, 6770, 286, 1919, 9490, 11, 350, 533, 1462, 6436, 1483, 11, 543, 36833, 9110, 286, 43146, 9490, 17909, 290, 6082, 13, 2312, 14895, 1978, 1249, 257, 7888, 19759, 286, 1919, 9490, 11, 543, 30940, 262, 761, 284, 8160, 281, 7952, 1919, 9490, 2163, 475, 379, 262, 2756, 286, 852, 10574, 319, 867, 2173, 286, 1598, 6817, 329, 2472, 26877, 9490, 11, 14660, 11, 475, 407, 691, 11, 6082, 290, 12791, 13, 198, 23722, 5436, 651, 1497, 351, 257, 12470, 7888, 19759, 286, 1919, 9490, 11, 290, 4145, 3368, 281, 7952, 9490, 2163, 30, 770, 481, 4745, 319, 703, 18633, 393, 26167, 465, 1693, 318, 7428, 13, 554, 663, 7135, 395, 19759, 960, 11518, 691, 953, 6945, 1123, 8611, 284, 3376, 329, 1321, 35797, 11, 1910, 1176, 11, 290, 7097, 871, 960, 270, 318, 40324, 326, 5436, 714, 466, 428, 1693, 11, 393, 27665, 340, 11, 1231, 281, 7952, 1919, 9490, 2163, 13, 5436, 714, 3376, 1321, 7095, 393, 35797, 1022, 1007, 27362, 4671, 13, 679, 714, 4659, 28393, 1262, 5387, 14317, 1321, 422, 11408, 11, 3737, 30259, 416, 29270, 1321, 422, 584, 9611, 287, 2092, 5692, 13, 679, 714, 4659, 290, 3376, 7097, 871, 1912, 319, 5654, 3725, 546, 3182, 41789, 11701, 286, 4419, 290, 7746, 286, 661, 447, 247, 82, 29115, 286, 262, 43440, 886, 12, 27219, 13, 1675, 262, 6287, 7097, 34859, 290, 4034, 8076, 355, 1171, 7017, 326, 2689, 3294, 661, 11, 24171, 511, 19406, 1245, 4433, 4375, 510, 1981, 3048, 290, 4145, 326, 777, 307, 6241, 287, 725, 641, 15537, 2846, 11, 475, 857, 407, 2421, 7952, 43146, 17909, 13, 198, 1537, 5436, 635, 468, 262, 3663, 960, 273, 262, 7077, 960, 1462, 31935, 262, 1695, 18201, 422, 790, 8611, 706, 339, 468, 2077, 1848, 286, 7097, 871, 290, 28393, 13, 554, 1804, 428, 11, 339, 714, 1011, 2972, 2829, 10581, 326, 460, 307, 5447, 422, 4671, 447, 247, 3585, 1188, 6055, 1626, 262, 8611, 11, 290, 4145, 466, 407, 2421, 281, 7952, 1919, 9490, 2163, 13, 679, 714, 11, 329, 1672, 11, 14083, 18201, 287, 617, 1813, 9823, 1022, 17872, 290, 18583, 960, 4853, 453, 11, 393, 287, 262, 976, 7303, 355, 262, 4671, 561, 423, 6939, 611, 5436, 550, 407, 35018, 960, 1324, 3157, 884, 27111, 7297, 2035, 284, 262, 2104, 1695, 18201, 11, 393, 284, 326, 6903, 326, 3793, 706, 5436, 2753, 617, 2648, 355, 1687, 6426, 13, 2791, 198, 1537, 597, 517, 14742, 3164, 326, 5436, 1244, 1011, 960, 8201, 597, 3164, 326, 857, 407, 2190, 477, 8945, 262, 976, 706, 14317, 329, 7097, 871, 290, 1910, 12, 1102, 1087, 1358, 28393, 960, 27238, 8814, 319, 9695, 286, 262, 4671, 7097, 284, 262, 8611, 11, 884, 355, 511, 5129, 393, 584, 9695, 13, 7518, 2530, 11154, 329, 884, 7747, 4433, 281, 7952, 1919, 9490, 2163, 284, 8160, 644, 954, 355, 1365, 393, 4785, 1919, 10906, 13, 1081, 287, 867, 584, 5479, 11, 262, 6482, 284, 9552, 12, 34762, 5370, 4433, 1193, 3299, 290, 14873, 2649, 286, 3815, 290, 3292, 8210, 326, 743, 307, 1364, 27102, 393, 16992, 13717, 884, 4318, 4571, 13, 198, 48142, 5436, 318, 14742, 11, 290, 4145, 857, 2421, 281, 7952, 1919, 9490, 2163, 11, 262, 4876, 286, 16215, 340, 460, 307, 11266, 656, 734, 3354, 25, 16215, 1981, 9490, 290, 13262, 803, 1973, 3925, 284, 8160, 4045, 1919, 1988, 13, 2312, 734, 3354, 1944, 1180, 13156, 11, 290, 4427, 1180, 3354, 286, 262, 1225, 9680, 286, 14895, 290, 7159, 10238, 46219, 3667, 329, 1910, 10906, 13, 198, 5962, 11, 703, 857, 5436, 8160, 290, 3953, 1981, 661, 447, 247, 82, 880, 12, 11873, 30, 554, 1804, 428, 11, 5436, 468, 257, 7069, 1693, 621, 1944, 9552, 3341, 11, 543, 691, 4031, 284, 4331, 26879, 5981, 14301, 25, 13339, 11, 12352, 11, 3904, 12, 9579, 82, 11, 290, 262, 588, 13, 1081, 4367, 2029, 11, 46219, 3667, 329, 6436, 1483, 286, 5939, 4745, 319, 13148, 477, 777, 14301, 389, 19874, 351, 534, 880, 12, 11873, 11, 2884, 530, 393, 1194, 1296, 286, 262, 7877, 29005, 286, 4602, 12741, 25, 611, 345, 466, 340, 11, 345, 1276, 765, 340, 357, 43762, 284, 1695, 7747, 1776, 290, 611, 345, 765, 340, 11, 340, 1276, 787, 345, 1365, 572, 13, 770, 7877, 29005, 3769, 257, 3665, 8489, 329, 7270, 2585, 25, 13148, 345, 760, 644, 345, 1988, 290, 719, 284, 10660, 340, 318, 4143, 33887, 284, 13148, 314, 760, 644, 318, 922, 329, 345, 13, 1550, 262, 584, 1021, 11, 262, 13196, 318, 6189, 3991, 287, 867, 2663, 13, 4380, 1690, 787, 7747, 326, 389, 2089, 329, 606, 287, 257, 13025, 9432, 2565, 11, 304, 13, 70, 1539, 287, 2116, 12, 71, 18052, 4568, 290, 779, 286, 18136, 290, 2854, 12, 16550, 5077, 5010, 326, 389, 28389, 393, 13568, 13, 843, 661, 1690, 466, 11, 393, 2038, 284, 466, 11, 1243, 326, 484, 1568, 13721, 25, 407, 25352, 1576, 11, 407, 8914, 329, 10737, 11, 393, 4581, 1165, 1310, 640, 45414, 11570, 4568, 290, 6958, 13, 9676, 11, 867, 1597, 4981, 4745, 319, 29440, 777, 2984, 31494, 902, 11, 416, 2263, 4621, 286, 848, 22220, 4069, 11, 24066, 11, 393, 10453, 286, 481, 13, 198, 1135, 561, 765, 5436, 284, 3368, 777, 1598, 45716, 11, 30274, 284, 466, 8569, 2280, 1365, 13, 887, 428, 20505, 12073, 2726, 7476, 11, 1390, 38082, 1042, 11, 2994, 286, 21851, 11, 393, 20814, 530, 1448, 447, 247, 82, 3815, 319, 1854, 11, 543, 2421, 18788, 351, 1049, 1337, 13, 2312, 7476, 389, 10255, 26963, 329, 5436, 287, 465, 7886, 393, 23097, 709, 666, 5107, 11, 780, 339, 691, 468, 1176, 284, 13096, 4536, 11, 407, 284, 1560, 345, 644, 284, 466, 13, 5436, 481, 27518, 345, 422, 7722, 393, 9216, 416, 8620, 262, 2756, 345, 1986, 329, 5548, 393, 17626, 3134, 851, 28998, 12577, 34401, 2138, 621, 40683, 416, 32366, 5609, 4536, 357, 40, 765, 1194, 4144, 26, 16314, 11, 340, 3484, 703, 881, 10091, 851, 4360, 407, 2282, 345, 460, 447, 247, 83, 423, 606, 13, 679, 1244, 772, 48914, 262, 13089, 6939, 422, 777, 1029, 12, 30883, 8945, 329, 534, 4414, 11, 416, 21024, 606, 284, 534, 2003, 1535, 12, 6651, 393, 10737, 9307, 2138, 621, 7216, 606, 284, 2035, 262, 1233, 14920, 393, 262, 38115, 13, 887, 981, 428, 2756, 12, 3106, 3164, 12850, 5436, 447, 247, 82, 47236, 1176, 625, 345, 11, 326, 1176, 460, 991, 307, 8904, 13, 5436, 1276, 691, 15445, 340, 287, 2139, 286, 534, 3177, 5353, 290, 3815, 11, 407, 10649, 625, 284, 502, 357, 273, 2687, 2073, 8, 31577, 703, 345, 815, 2107, 393, 644, 345, 815, 765, 13, 198, 2514, 4620, 428, 5236, 11, 5436, 2476, 257, 2746, 286, 534, 9490, 326, 30940, 3108, 5823, 286, 3572, 475, 326, 991, 6870, 534, 5761, 286, 534, 9490, 13, 632, 1276, 2380, 257, 3177, 1570, 286, 534, 5353, 290, 3815, 326, 318, 407, 26987, 416, 13962, 3089, 7947, 393, 25278, 26, 318, 407, 25036, 416, 584, 4671, 329, 511, 898, 4621, 26, 326, 2753, 1848, 286, 703, 345, 765, 284, 307, 11, 772, 618, 534, 1944, 4069, 12312, 3212, 422, 326, 5761, 26, 290, 326, 20431, 12497, 987, 11498, 35738, 3292, 8210, 11, 3104, 13479, 11, 290, 262, 9490, 286, 584, 661, 290, 3815, 2354, 3511, 784, 475, 326, 318, 991, 12431, 13, 1471, 379, 1551, 11, 1201, 5436, 447, 247, 82, 4934, 318, 3614, 284, 262, 3773, 11, 339, 2476, 257, 2746, 286, 777, 1243, 329, 345, 44061, 355, 484, 389, 28344, 287, 534, 3034, 8945, 13, 3388, 198, 2514, 1296, 428, 2746, 11, 5436, 460, 3197, 319, 262, 976, 17211, 1366, 9611, 1541, 779, 290, 389, 5922, 11, 1111, 1366, 326, 583, 12143, 24139, 284, 345, 290, 2276, 4582, 41240, 422, 584, 661, 13, 1002, 5436, 318, 17338, 34412, 326, 356, 8281, 11, 339, 743, 635, 307, 1498, 284, 3197, 319, 1366, 407, 6646, 1695, 284, 9611, 11, 884, 355, 3315, 1366, 11, 393, 5387, 25033, 290, 30232, 13050, 11, 1944, 290, 1613, 13, 887, 5436, 447, 247, 82, 4094, 4621, 287, 14583, 428, 2746, 286, 534, 9490, 318, 326, 339, 857, 407, 423, 284, 466, 340, 3436, 13, 4525, 1944, 11628, 329, 9552, 12, 25616, 2614, 29488, 11, 5436, 460, 670, 351, 345, 11, 21769, 345, 290, 4737, 345, 546, 534, 15387, 11, 27337, 11, 290, 7666, 546, 534, 1613, 7747, 290, 25345, 2003, 3392, 11, 284, 35139, 290, 4296, 465, 2746, 286, 534, 9490, 13, 24850, 287, 428, 835, 11, 5436, 3073, 517, 588, 257, 1204, 3985, 393, 2289, 7255, 273, 621, 281, 3034, 42351, 25, 5600, 11, 428, 5761, 286, 5436, 318, 845, 2092, 284, 262, 3164, 5150, 416, 22559, 11563, 355, 257, 3747, 3953, 1028, 9552, 29488, 1642, 2726, 8563, 618, 484, 719, 319, 534, 8378, 13, 2154, 8013, 257, 2614, 9552, 8796, 561, 307, 5213, 351, 867, 584, 7747, 287, 3090, 284, 534, 10270, 287, 3034, 8945, 11, 2158, 11, 8620, 262, 1808, 286, 1771, 428, 8796, 815, 307, 617, 584, 9552, 12, 25616, 5797, 11, 7310, 422, 5436, 11, 3025, 1321, 290, 4786, 389, 3614, 284, 345, 13, 8013, 257, 2614, 9552, 5797, 960, 1616, 447, 247, 82, 869, 683, 12558, 12, 11518, 960, 19188, 7173, 22464, 11563, 447, 247, 82, 17074, 2614, 9552, 8796, 11, 2845, 326, 11, 355, 262, 21688, 286, 534, 2614, 9490, 11, 339, 561, 307, 4497, 329, 6427, 319, 284, 3773, 12, 4421, 5436, 357, 447, 250, 12804, 5436, 447, 251, 8, 257, 24637, 286, 262, 1321, 339, 6622, 546, 345, 11, 543, 318, 5981, 284, 534, 15387, 290, 9490, 355, 484, 389, 5884, 284, 534, 10270, 287, 3034, 8945, 11, 290, 262, 3048, 319, 345, 286, 7097, 871, 422, 1854, 447, 247, 8945, 13, 770, 318, 262, 1321, 546, 345, 326, 5436, 2476, 284, 19330, 534, 9490, 656, 465, 2756, 12, 23032, 434, 5370, 13, 383, 1334, 286, 534, 12213, 351, 12558, 12, 11518, 11, 290, 262, 1334, 286, 465, 3725, 546, 345, 11, 389, 407, 2622, 416, 4403, 5436, 290, 460, 2652, 2839, 1022, 262, 734, 286, 345, 13, 198, 6104, 351, 257, 4938, 8922, 286, 2506, 447, 247, 82, 9490, 355, 5676, 416, 3034, 8945, 11, 5436, 481, 991, 761, 284, 19406, 284, 257, 10098, 3953, 286, 1919, 9490, 13, 4362, 5436, 318, 7351, 287, 257, 7270, 1181, 960, 1662, 257, 262, 15405, 530, 11, 407, 530, 326, 8404, 284, 3494, 257, 10112, 29576, 666, 3164, 284, 14458, 357, 16341, 11, 3737, 11, 287, 4301, 1099, 11, 543, 3793, 262, 1181, 447, 247, 82, 1597, 11, 407, 5436, 447, 247, 82, 27920, 5562, 3953, 286, 1919, 9490, 1276, 307, 617, 1296, 286, 48239, 30114, 341, 286, 1981, 9490, 5260, 355, 484, 583, 3153, 284, 3034, 4568, 13, 4377, 884, 46500, 4433, 19590, 7223, 284, 1123, 1048, 447, 247, 82, 9490, 13, 2893, 3501, 4961, 3463, 284, 2506, 447, 247, 82, 9490, 318, 281, 3489, 4277, 3572, 11, 612, 743, 635, 307, 9829, 12536, 284, 1577, 617, 661, 447, 247, 82, 9490, 7387, 19590, 621, 1854, 447, 247, 13, 554, 1948, 11, 739, 3403, 286, 1919, 12791, 11, 340, 743, 307, 32005, 11, 393, 772, 22388, 2672, 11, 284, 1577, 4025, 19590, 284, 262, 9490, 286, 883, 5290, 572, 13, 10968, 11, 597, 19406, 9490, 3953, 1276, 2074, 262, 3585, 19590, 284, 1577, 284, 3034, 9051, 1729, 12, 17079, 20420, 284, 9490, 26, 4869, 3403, 379, 1180, 1661, 26, 290, 3403, 326, 4174, 739, 1180, 1103, 4582, 286, 36553, 13, 18181, 739, 262, 13196, 326, 477, 777, 15225, 389, 9380, 14553, 287, 262, 1981, 9490, 5260, 3804, 284, 5436, 11, 262, 1919, 9490, 2163, 1276, 2380, 10098, 24195, 319, 777, 6067, 13, 198, 7003, 3938, 31577, 5436, 447, 247, 82, 9432, 2163, 318, 3675, 616, 8354, 994, 11, 428, 5114, 5644, 262, 1917, 460, 307, 5561, 15655, 416, 31577, 257, 1178, 10007, 13, 1002, 356, 7048, 326, 5436, 447, 247, 82, 1919, 9490, 2163, 318, 617, 6209, 48239, 46500, 286, 1981, 9490, 5260, 11, 543, 2753, 5035, 1848, 286, 12791, 11, 640, 11, 13479, 11, 290, 3034, 9051, 1729, 12, 17079, 3416, 1187, 286, 9490, 11, 428, 5644, 326, 31577, 262, 2163, 1244, 307, 7173, 5561, 15655, 416, 4634, 3815, 329, 1440, 10007, 25, 357, 16, 8, 257, 3953, 286, 41608, 284, 12791, 284, 307, 973, 287, 4634, 3585, 19590, 329, 1365, 290, 4785, 12, 2364, 3925, 26, 357, 17, 8, 257, 9780, 2494, 393, 584, 11507, 284, 900, 262, 3585, 3463, 278, 286, 10906, 379, 1180, 1661, 26, 4761, 357, 18, 8, 257, 3953, 286, 2526, 12, 64, 9641, 284, 3463, 10906, 739, 517, 393, 1342, 17070, 21811, 286, 36553, 26, 290, 357, 19, 8, 257, 3585, 3463, 278, 286, 2587, 7327, 290, 1729, 12, 17079, 20420, 284, 9490, 884, 355, 6142, 3403, 13, 198, 1212, 938, 11507, 11, 262, 3585, 3463, 278, 286, 3034, 290, 1729, 12, 17079, 9284, 284, 9490, 11, 318, 1884, 284, 307, 262, 1388, 8875, 12755, 262, 19406, 2546, 286, 3034, 5072, 739, 5436, 13, 1002, 262, 2587, 290, 2568, 15623, 3917, 351, 3227, 11, 543, 5004, 6142, 12751, 11, 2314, 307, 40647, 5322, 3812, 6632, 11, 788, 6142, 3403, 481, 8160, 262, 7095, 319, 262, 19406, 5046, 286, 262, 1692, 12973, 13953, 13, 554, 257, 995, 286, 9257, 5322, 761, 329, 1692, 4827, 287, 3227, 11, 884, 6142, 17778, 389, 1884, 284, 307, 517, 17707, 12765, 621, 597, 4179, 319, 3227, 326, 22068, 422, 661, 11236, 24638, 640, 625, 7184, 13, 198, 818, 3090, 284, 4737, 644, 5436, 447, 247, 82, 9432, 2163, 318, 11, 356, 1276, 635, 2074, 262, 1429, 416, 543, 340, 318, 7147, 13, 4900, 5436, 4632, 6870, 257, 1579, 15405, 5761, 11, 428, 318, 257, 966, 810, 7996, 1276, 1282, 287, 13, 2896, 3191, 257, 10098, 19759, 286, 1919, 9490, 318, 281, 48676, 1964, 1429, 11, 543, 1276, 423, 661, 287, 3877, 1762, 832, 617, 48517, 9829, 9030, 13, 554, 6402, 703, 284, 466, 428, 11, 262, 14895, 1541, 925, 423, 27009, 6067, 15394, 13, 45040, 286, 1981, 9490, 14740, 422, 262, 12213, 1022, 661, 290, 511, 9552, 12, 25616, 2614, 29488, 11, 981, 262, 46500, 284, 1919, 9490, 468, 587, 5322, 357, 1640, 4959, 286, 4578, 8, 284, 4634, 3815, 329, 257, 1178, 3665, 11, 14704, 21977, 10007, 13, 9170, 17086, 262, 13391, 286, 5887, 12, 15808, 11, 772, 1579, 15405, 11, 5370, 329, 3716, 11, 5688, 21543, 5370, 287, 14748, 286, 18633, 4987, 1964, 5645, 11, 4790, 428, 2551, 8666, 318, 17338, 1598, 290, 2829, 284, 1295, 340, 1626, 262, 9889, 286, 867, 1180, 48517, 9829, 7767, 13, 1114, 1672, 11, 345, 460, 5967, 428, 355, 257, 10828, 4876, 11, 416, 543, 3815, 329, 262, 1688, 10007, 286, 5436, 447, 247, 82, 9432, 2163, 389, 11777, 17814, 290, 26034, 15556, 287, 14195, 13, 921, 460, 635, 14704, 5967, 777, 355, 852, 6067, 286, 7952, 4384, 287, 13901, 4819, 11, 393, 852, 49711, 284, 5337, 10518, 7767, 884, 355, 474, 4740, 286, 15456, 6163, 4290, 13, 921, 714, 772, 5967, 262, 4876, 852, 49711, 284, 617, 5887, 11553, 4086, 739, 10828, 17251, 1741, 286, 617, 2440, 12, 2875, 12031, 284, 307, 6190, 416, 262, 3572, 11, 13148, 357, 259, 471, 13, 50, 13, 1099, 8, 428, 2551, 36417, 262, 43440, 9758, 4427, 319, 1729, 12, 2934, 1455, 341, 9384, 13, 198, 464, 4094, 2526, 3917, 351, 5436, 447, 247, 82, 9432, 2163, 318, 262, 2526, 286, 8006, 13, 1881, 21296, 326, 5436, 10969, 318, 326, 981, 530, 286, 465, 1688, 3946, 318, 8868, 1910, 1176, 290, 3917, 5602, 12, 38515, 287, 1948, 5939, 11, 262, 29024, 1964, 1429, 286, 16215, 5436, 447, 247, 82, 9432, 2163, 6870, 257, 17298, 3663, 329, 5602, 12, 38515, 326, 7678, 907, 477, 1854, 13, 17462, 1498, 284, 1167, 801, 5436, 447, 247, 82, 5370, 284, 4691, 511, 12031, 11, 772, 4622, 11, 561, 307, 287, 257, 2292, 286, 13029, 1176, 960, 1462, 4461, 5801, 5129, 772, 3675, 262, 10625, 286, 7261, 12, 9688, 929, 22202, 11, 393, 284, 5485, 3592, 284, 511, 5761, 13, 36855, 11, 262, 5517, 286, 884, 1176, 1244, 307, 18176, 416, 5436, 447, 247, 82, 3722, 355, 257, 9775, 9432, 11, 8500, 24127, 13, 4524, 37163, 278, 262, 1964, 8666, 284, 4634, 257, 1178, 4047, 13262, 515, 10007, 11476, 9405, 777, 4786, 13, 2425, 2312, 10007, 466, 407, 1249, 262, 17512, 286, 1402, 12, 9888, 3307, 326, 561, 307, 2622, 284, 30867, 5436, 447, 247, 82, 5370, 284, 257, 1178, 10544, 447, 247, 2587, 4621, 11, 290, 484, 4031, 284, 7719, 257, 10518, 17310, 319, 4096, 1964, 3815, 13, 887, 340, 318, 257, 890, 835, 422, 777, 1029, 12, 5715, 5370, 284, 5436, 447, 247, 82, 4036, 4560, 11, 351, 867, 37294, 4831, 326, 389, 517, 6276, 290, 32191, 11, 625, 543, 867, 10544, 561, 1842, 284, 5517, 5897, 4588, 13, 1629, 262, 1241, 286, 1152, 1483, 286, 428, 5114, 11, 612, 318, 645, 517, 284, 910, 994, 3675, 21847, 419, 602, 284, 49202, 546, 884, 17512, 11, 355, 881, 13902, 355, 318, 23498, 287, 262, 1429, 286, 18492, 11, 3047, 11, 290, 15427, 5436, 11, 290, 9021, 329, 38424, 329, 883, 28517, 416, 5436, 447, 247, 82, 5370, 13, 198, 38, 13, 18067, 284, 5436, 357, 1870, 24390, 278, 360, 6606, 17159, 262, 6378, 8, 198, 11518, 318, 257, 1807, 6306, 11, 5292, 284, 307, 28991, 290, 28695, 13, 6430, 636, 286, 262, 4007, 286, 262, 5517, 318, 284, 7267, 326, 5436, 318, 407, 27627, 813, 6569, 422, 1944, 9889, 290, 11257, 13, 4650, 4847, 326, 714, 787, 510, 5436, 12, 2339, 9889, 960, 2416, 312, 36383, 287, 31350, 5339, 11, 16113, 11, 1366, 11, 290, 1366, 11812, 290, 3781, 4899, 960, 533, 1541, 1944, 393, 287, 2478, 13, 2312, 389, 4632, 5922, 739, 2839, 1630, 284, 10660, 5068, 5353, 11, 393, 739, 1181, 1630, 284, 10660, 2422, 290, 35493, 4621, 11, 475, 407, 11541, 13, 1318, 318, 635, 8904, 2267, 17715, 287, 11155, 290, 7271, 4855, 2267, 6712, 11, 617, 286, 340, 28845, 8389, 355, 257, 14748, 286, 564, 250, 20185, 329, 922, 13, 447, 251, 198, 818, 428, 2665, 314, 6482, 422, 703, 5436, 561, 670, 355, 281, 36123, 284, 6402, 1744, 6801, 22963, 416, 543, 5436, 11, 393, 2092, 9889, 11, 1244, 1282, 546, 13, 4377, 884, 21182, 481, 6211, 257, 6087, 286, 6276, 290, 21790, 12, 23149, 13312, 13, 314, 17548, 1115, 6801, 22963, 326, 389, 17338, 7310, 290, 357, 1462, 15874, 7370, 8, 19756, 284, 17004, 12452, 13, 198, 464, 717, 11, 290, 9775, 24043, 11, 21182, 561, 6211, 617, 12934, 14615, 379, 617, 2003, 966, 284, 11206, 5436, 24005, 416, 1964, 3572, 13, 8013, 257, 3572, 561, 6486, 1626, 262, 4934, 286, 2585, 11, 475, 561, 5298, 1811, 7103, 2683, 290, 6459, 13, 3412, 13148, 262, 2622, 9889, 11196, 11, 547, 3492, 284, 6061, 11, 290, 37891, 19589, 284, 670, 11, 262, 11553, 5046, 286, 884, 257, 6801, 561, 307, 5909, 13, 632, 561, 2421, 257, 4858, 4836, 12, 448, 290, 4856, 286, 6884, 290, 3341, 878, 15430, 319, 11, 788, 617, 1296, 286, 5078, 12, 2502, 11, 3737, 379, 257, 890, 662, 12, 43499, 2589, 1141, 257, 2278, 286, 5322, 3034, 3842, 884, 355, 257, 1474, 12, 403, 1191, 453, 6515, 4158, 9912, 13, 383, 6801, 13062, 617, 28204, 284, 12432, 618, 2678, 423, 17687, 262, 4571, 286, 2975, 3067, 11, 3584, 262, 1487, 561, 307, 881, 4025, 357, 45781, 530, 407, 7411, 257, 2526, 286, 1182, 12, 261, 31998, 737, 4304, 198, 2782, 404, 889, 5436, 561, 307, 257, 3236, 2551, 11, 3675, 262, 4934, 286, 597, 11553, 393, 4640, 1429, 475, 10616, 617, 48517, 9829, 1964, 1429, 11, 10828, 393, 3737, 9758, 13, 843, 340, 561, 1944, 257, 9015, 12, 392, 12, 33856, 1917, 5115, 9889, 13, 16427, 884, 257, 3572, 561, 1884, 2421, 6628, 326, 2622, 9889, 389, 1695, 11, 561, 670, 26995, 11, 561, 5203, 262, 8072, 4034, 11, 290, 561, 1944, 645, 6049, 7476, 13, 887, 884, 6628, 714, 691, 307, 1695, 706, 617, 890, 2278, 286, 3161, 2478, 290, 4856, 11, 543, 287, 1210, 561, 2421, 3161, 1964, 5370, 284, 1104, 777, 13, 3412, 883, 3161, 5370, 284, 1205, 290, 1332, 262, 12971, 561, 10403, 8791, 15175, 5471, 11, 422, 883, 351, 1913, 15735, 19405, 284, 5939, 290, 422, 883, 39125, 422, 10582, 883, 1919, 34859, 960, 1156, 82, 422, 1910, 1176, 11, 290, 555, 17200, 4633, 7097, 871, 960, 5562, 5436, 561, 2496, 13, 554, 1570, 286, 777, 13156, 11, 314, 4099, 326, 22868, 5436, 416, 7952, 1964, 3572, 561, 307, 4047, 7485, 11, 13717, 1913, 2458, 287, 1964, 3403, 884, 355, 281, 3034, 4902, 523, 6049, 355, 284, 13813, 262, 12013, 1176, 286, 19742, 658, 13, 3412, 4379, 5436, 5361, 7675, 287, 584, 25281, 11, 981, 340, 1244, 1037, 357, 392, 4145, 20135, 326, 262, 717, 1445, 561, 307, 262, 17612, 828, 561, 2192, 407, 1037, 1576, 13717, 257, 4902, 13, 198, 32, 1218, 1744, 6339, 11, 6196, 47165, 262, 3257, 14725, 329, 262, 717, 6339, 11, 561, 6211, 1903, 2478, 11, 4856, 11, 393, 12695, 286, 5436, 379, 4833, 5046, 11, 1871, 2628, 351, 517, 15882, 1964, 3403, 13, 33671, 1903, 25016, 290, 4344, 1010, 1244, 2291, 25281, 326, 1541, 423, 8904, 7303, 286, 262, 3773, 287, 1181, 23941, 393, 739, 1181, 1630, 26, 393, 883, 23941, 329, 543, 3741, 1630, 1541, 29076, 287, 617, 9906, 286, 1588, 18901, 5129, 5153, 357, 15496, 11, 15238, 737, 3412, 25281, 351, 1310, 1181, 1630, 286, 262, 3773, 714, 1205, 290, 1332, 5436, 832, 1230, 31156, 11, 355, 6905, 1690, 466, 329, 1903, 1104, 286, 6142, 8514, 13, 5436, 1244, 635, 307, 4166, 832, 10393, 7118, 422, 1402, 11, 1903, 11, 2172, 12, 259, 5348, 13, 2312, 1244, 307, 597, 1448, 286, 3925, 290, 5745, 5884, 17707, 351, 1123, 584, 290, 1342, 523, 351, 1854, 960, 2339, 4158, 2628, 11, 1919, 393, 1964, 6306, 364, 11, 393, 5365, 11557, 1964, 290, 3034, 25281, 960, 8727, 561, 1309, 5436, 11, 1365, 783, 1444, 564, 250, 6719, 12, 11518, 11, 447, 251, 1630, 511, 3227, 290, 5163, 6958, 351, 1123, 584, 13, 198, 7149, 884, 1448, 286, 1903, 4344, 1010, 561, 1986, 257, 1178, 3489, 6459, 13, 1119, 561, 423, 284, 2493, 290, 13096, 9889, 422, 584, 3544, 11, 543, 287, 1210, 561, 2421, 326, 777, 9889, 307, 17338, 290, 3326, 361, 18745, 6068, 540, 284, 511, 649, 4007, 290, 4634, 13, 25929, 484, 714, 1205, 262, 649, 4899, 290, 3341, 2405, 11, 287, 543, 1339, 484, 561, 761, 262, 4133, 284, 466, 428, 13, 8673, 11, 1813, 262, 31650, 290, 6817, 286, 262, 6306, 11, 484, 714, 4729, 28150, 291, 1104, 13, 383, 4238, 1448, 561, 423, 284, 307, 1588, 1576, 290, 4553, 1576, 326, 511, 12213, 351, 1123, 584, 2380, 257, 8904, 13390, 286, 477, 511, 3034, 12213, 13, 843, 284, 262, 6287, 484, 466, 3292, 351, 262, 1334, 286, 262, 995, 11, 484, 561, 761, 284, 4155, 326, 884, 3292, 857, 407, 16637, 5436, 8797, 465, 4536, 12312, 469, 422, 2839, 12, 10728, 4536, 13, 1052, 34657, 1917, 561, 15058, 351, 597, 14833, 286, 5436, 11, 379, 597, 5046, 13, 15935, 8354, 286, 8945, 318, 1813, 284, 5436, 11, 465, 4934, 625, 883, 8945, 1276, 307, 8568, 25, 2042, 5939, 1276, 307, 6840, 12244, 11, 290, 5163, 1973, 262, 18645, 286, 5436, 447, 247, 82, 4934, 1276, 407, 44328, 465, 16895, 13, 554, 262, 1339, 286, 3230, 3292, 11, 5436, 447, 247, 82, 16895, 561, 423, 284, 307, 5625, 287, 10730, 284, 14018, 8945, 284, 3368, 9277, 8394, 6443, 11, 588, 5150, 4865, 1687, 16895, 319, 14018, 7017, 284, 12201, 262, 13530, 286, 16325, 12, 22649, 393, 584, 6142, 4788, 13, 3324, 1114, 428, 284, 307, 257, 13971, 6801, 21182, 11, 5436, 1276, 670, 880, 1576, 960, 28998, 706, 617, 1903, 923, 12, 929, 7108, 5281, 416, 262, 17131, 286, 1903, 4344, 1010, 290, 923, 12, 929, 28150, 291, 1104, 960, 5562, 612, 389, 1598, 19406, 4034, 284, 1762, 351, 683, 326, 389, 7424, 284, 29028, 13, 198, 32, 2368, 21182, 11, 517, 12948, 351, 1944, 11257, 11, 561, 6211, 3767, 7118, 290, 31941, 286, 5436, 12, 2339, 9889, 287, 262, 2839, 6567, 11, 284, 262, 966, 810, 257, 1178, 23941, 393, 7686, 1630, 257, 1588, 13390, 286, 262, 3773, 13, 632, 318, 6768, 4367, 326, 355, 262, 5046, 286, 3859, 30797, 444, 13676, 11, 484, 6481, 22464, 2585, 290, 5517, 2092, 4934, 11, 3584, 1231, 8617, 284, 4155, 10518, 18241, 13, 3695, 33238, 617, 4922, 286, 10368, 286, 2839, 3034, 5410, 357, 392, 1176, 8, 318, 6768, 9569, 355, 18010, 11, 5436, 714, 1282, 546, 832, 617, 2003, 1964, 2551, 284, 1011, 625, 290, 302, 12, 29983, 262, 3341, 13, 770, 561, 407, 307, 257, 22338, 290, 1171, 302, 12, 14225, 32927, 286, 3518, 3139, 6798, 11, 475, 286, 9552, 3341, 290, 3917, 1366, 11, 3584, 326, 9200, 2963, 561, 8941, 787, 262, 2551, 1342, 34561, 278, 290, 5358, 723, 13, 198, 1212, 21182, 16507, 319, 734, 14895, 13, 3274, 11, 340, 906, 8139, 326, 617, 2003, 6754, 2589, 3578, 257, 24005, 27011, 286, 17298, 2839, 1176, 326, 318, 788, 19589, 284, 423, 1716, 49469, 11, 2033, 278, 284, 257, 1588, 12, 9888, 8195, 5647, 3924, 286, 1176, 1022, 2839, 290, 1171, 10544, 13, 770, 561, 307, 257, 12253, 1487, 11, 6872, 262, 7476, 286, 19911, 290, 3685, 326, 6032, 5262, 12253, 2458, 13, 5498, 11, 340, 906, 8139, 262, 6276, 40460, 286, 302, 12, 14225, 32927, 257, 900, 286, 9552, 4899, 290, 1366, 4166, 329, 2839, 4959, 284, 4691, 5436, 447, 247, 82, 1171, 12031, 13, 770, 743, 407, 307, 3938, 1744, 11, 355, 617, 286, 5436, 447, 247, 82, 15171, 960, 2339, 24171, 1981, 880, 12, 11873, 11, 1188, 4250, 7097, 871, 11, 290, 15964, 28393, 960, 533, 407, 2672, 286, 1944, 3341, 7351, 2839, 5353, 13, 1675, 262, 6287, 262, 4683, 4899, 290, 1366, 2314, 1620, 777, 8861, 11, 484, 561, 2380, 4553, 11, 649, 2478, 5359, 13, 198, 3103, 11539, 25, 1867, 770, 29620, 11, 46597, 3806, 11, 44495, 471, 710, 36370, 198, 1722, 257, 28991, 13936, 11, 428, 5517, 857, 407, 22096, 2346, 284, 1913, 13242, 13, 6430, 340, 3568, 284, 423, 26403, 257, 1178, 37042, 13050, 290, 17218, 11, 543, 379, 257, 5288, 1950, 11154, 329, 2252, 13936, 290, 2267, 784, 1390, 13720, 617, 2173, 286, 2785, 1474, 12, 4354, 11154, 11, 329, 2267, 290, 329, 1903, 2478, 286, 18848, 9889, 284, 6687, 7476, 13, 198, 5962, 11, 314, 24351, 326, 262, 5517, 468, 4920, 617, 4922, 286, 48923, 2247, 329, 262, 43714, 9552, 12, 15808, 4318, 3034, 5410, 784, 739, 262, 33603, 1913, 14895, 925, 546, 14614, 9889, 13, 383, 13936, 5174, 3294, 13312, 17715, 326, 966, 3812, 262, 2003, 9889, 9672, 11, 290, 1043, 645, 905, 12, 301, 37186, 13, 4900, 428, 4752, 13646, 286, 48923, 2247, 318, 4047, 10617, 11, 340, 318, 407, 257, 20861, 7664, 11, 1201, 262, 13936, 286, 1180, 5107, 286, 5436, 739, 1180, 38356, 14895, 2921, 6768, 12312, 6783, 5009, 286, 511, 48923, 2247, 11, 351, 530, 15304, 286, 5436, 960, 31208, 5436, 287, 262, 4931, 286, 617, 4922, 286, 3767, 1692, 43747, 4086, 960, 25579, 278, 5729, 1035, 48568, 540, 17648, 13, 198, 5167, 18633, 11, 262, 5517, 5925, 12931, 262, 2276, 966, 326, 27377, 43590, 5479, 290, 26877, 12751, 422, 9552, 290, 3519, 9889, 389, 19756, 784, 351, 262, 2785, 329, 1111, 1049, 4414, 290, 4419, 784, 890, 878, 262, 10224, 10591, 24875, 286, 9552, 326, 23589, 2412, 1692, 9889, 290, 1630, 13, 314, 423, 7189, 8057, 329, 262, 6817, 286, 777, 564, 250, 3849, 13857, 12, 9521, 447, 251, 9552, 9889, 290, 12751, 11, 290, 329, 511, 7310, 2095, 422, 1111, 1474, 290, 890, 12, 4354, 2428, 960, 259, 1948, 287, 511, 9079, 329, 11521, 12452, 286, 1111, 6276, 9695, 286, 9552, 3341, 290, 262, 3034, 11, 1964, 11, 290, 1919, 4732, 287, 543, 484, 389, 12380, 13, 3720, 2893, 340, 318, 825, 27339, 284, 2962, 20736, 319, 6276, 9695, 287, 6402, 890, 12, 4354, 7476, 11, 290, 319, 1692, 5353, 290, 5370, 287, 6402, 1459, 5479, 290, 511, 12751, 11, 6159, 286, 777, 7106, 4035, 14895, 318, 15409, 618, 6402, 19898, 12, 9521, 9889, 290, 12751, 13, 5436, 318, 10403, 407, 262, 691, 1672, 286, 257, 19756, 11, 27377, 28094, 2785, 9552, 3586, 326, 8953, 287, 428, 3504, 2837, 784, 5600, 11, 428, 5517, 5644, 262, 1988, 286, 3612, 832, 584, 12779, 286, 2092, 43590, 5046, 784, 475, 262, 6496, 12452, 286, 5436, 290, 465, 10939, 25755, 364, 1363, 262, 6817, 286, 777, 517, 46641, 621, 262, 3161, 11, 517, 2276, 7159, 13, 198, 5167, 5734, 11, 262, 5517, 286, 18894, 866, 284, 262, 45596, 286, 5436, 447, 247, 82, 4560, 290, 6948, 26403, 1811, 42789, 17218, 11, 1123, 6011, 4465, 11154, 329, 2252, 3781, 290, 12069, 13, 3274, 11, 340, 3568, 326, 5559, 49849, 286, 703, 5436, 1244, 307, 9177, 13238, 19278, 306, 287, 511, 40460, 11, 5359, 11, 290, 31277, 17648, 290, 7476, 13, 554, 1948, 11, 262, 2126, 286, 23097, 709, 666, 5436, 784, 257, 4318, 12, 11578, 768, 1912, 7822, 286, 257, 9815, 1080, 286, 23097, 709, 666, 5704, 784, 318, 257, 5337, 290, 11781, 5761, 286, 14554, 2839, 12, 11377, 1630, 286, 16533, 11, 407, 4271, 3177, 287, 15389, 625, 4318, 3034, 5410, 13, 23097, 709, 666, 5436, 3568, 284, 2897, 262, 6034, 286, 1115, 1688, 13391, 11, 2426, 284, 477, 262, 37088, 47155, 13, 679, 3568, 6196, 1498, 284, 12377, 262, 9332, 290, 12354, 13391, 286, 2839, 1910, 3341, 981, 635, 39038, 511, 749, 9208, 15536, 13, 679, 635, 3568, 284, 2897, 262, 6034, 286, 21843, 1231, 6992, 10538, 11, 18244, 379, 262, 1575, 286, 19855, 1981, 1143, 14521, 286, 4290, 447, 247, 15387, 13, 843, 3443, 11, 339, 3568, 284, 2897, 262, 6034, 11, 832, 4542, 286, 262, 10007, 286, 5436, 447, 247, 82, 1919, 9490, 2163, 11, 286, 6079, 1588, 12, 9888, 3034, 4542, 739, 4050, 290, 7981, 10518, 1630, 11, 1231, 6078, 262, 13391, 286, 2839, 5939, 13, 1795, 198, 12211, 11, 772, 428, 15223, 3645, 5220, 326, 1366, 290, 1366, 11812, 2476, 743, 13238, 7634, 625, 1180, 5107, 286, 5436, 290, 3946, 1813, 284, 683, 784, 304, 13, 70, 1539, 24171, 1981, 9490, 11, 13045, 7097, 871, 11, 13720, 290, 47165, 28393, 422, 1910, 1176, 11, 24171, 290, 10068, 3081, 286, 1762, 1204, 11, 290, 11560, 17560, 25438, 290, 7325, 2499, 13, 10968, 11, 777, 1366, 2476, 743, 635, 13238, 7634, 422, 883, 2622, 284, 4331, 290, 18510, 26879, 5981, 4069, 329, 262, 4414, 286, 3753, 12, 3911, 444, 13, 2195, 27289, 777, 2476, 329, 2176, 12031, 11, 3249, 606, 1231, 18010, 4419, 284, 584, 3815, 11, 290, 4673, 703, 284, 19386, 2839, 1366, 546, 1981, 9490, 351, 1576, 7373, 284, 7139, 4050, 1919, 23989, 11, 481, 477, 307, 1593, 2267, 3006, 13, 198, 17699, 3006, 286, 2252, 12069, 5220, 416, 262, 5517, 2291, 262, 9871, 11701, 329, 11560, 11044, 290, 16389, 290, 11044, 287, 3592, 11, 287, 262, 4931, 286, 31350, 12971, 326, 460, 9257, 22636, 290, 27183, 379, 1551, 883, 11044, 11701, 326, 4745, 319, 10342, 27606, 1695, 1321, 26, 290, 262, 2695, 11, 4645, 11, 290, 1724, 286, 16215, 281, 19406, 1919, 9490, 2163, 13, 383, 4238, 12069, 656, 428, 6846, 1808, 5220, 326, 340, 1244, 307, 881, 1342, 2408, 621, 6768, 9672, 11, 475, 262, 1029, 21147, 2950, 1950, 11681, 428, 16915, 4238, 13367, 11200, 1146, 13, 7735, 4688, 10986, 656, 2785, 5107, 286, 1919, 9490, 2163, 326, 389, 7141, 1576, 284, 5698, 5436, 447, 247, 82, 5370, 475, 635, 4084, 290, 2391, 5772, 316, 380, 8863, 1576, 284, 1104, 11570, 10518, 2551, 12, 8601, 389, 286, 1029, 1988, 26, 355, 318, 3645, 286, 5559, 48517, 9829, 7767, 290, 6712, 284, 3189, 428, 11507, 12, 33990, 1429, 13, 198, 32, 3573, 3499, 1989, 329, 2252, 12069, 29579, 416, 262, 5517, 561, 307, 17247, 517, 3614, 40854, 286, 5436, 13, 1002, 262, 9815, 11, 3773, 12, 4421, 5436, 6693, 994, 318, 329, 617, 1738, 1167, 30412, 856, 393, 18010, 11, 1244, 17670, 351, 517, 3614, 8354, 2148, 867, 286, 262, 5150, 4034, 351, 1342, 1575, 11, 19911, 11, 393, 22007, 30, 5436, 447, 247, 82, 8354, 1244, 11, 329, 1672, 11, 307, 3614, 284, 23941, 625, 257, 7368, 5046, 11, 393, 284, 16020, 5174, 355, 17728, 2592, 1588, 7097, 871, 393, 25671, 284, 1910, 1176, 290, 5602, 12, 38515, 13, 317, 3573, 3499, 15304, 561, 4179, 5436, 447, 247, 82, 4934, 284, 3139, 5939, 11, 2035, 4045, 393, 26913, 351, 257, 5046, 11387, 13, 554, 428, 15304, 11, 564, 250, 39315, 5436, 447, 251, 561, 31935, 11, 393, 517, 1884, 2756, 12, 23032, 11, 3139, 284, 23941, 11, 13586, 393, 5361, 287, 10730, 351, 2839, 3139, 5939, 13, 9747, 5436, 561, 14572, 779, 262, 976, 9432, 2163, 355, 3773, 12, 4421, 5436, 13, 4619, 428, 2163, 561, 2074, 1111, 262, 2839, 290, 1171, 3048, 286, 13953, 4560, 11, 9747, 5436, 561, 407, 10582, 24340, 262, 4069, 286, 2035, 2839, 3139, 5939, 11, 393, 1613, 4040, 284, 31935, 3139, 287, 1627, 351, 1964, 12031, 13, 24982, 11, 262, 880, 1900, 44329, 286, 777, 1613, 4040, 561, 407, 6646, 4174, 11, 597, 517, 621, 262, 1468, 44329, 286, 9815, 4318, 5410, 561, 4174, 284, 3773, 12, 4421, 5436, 13, 367, 29503, 326, 9747, 5436, 1244, 307, 23498, 290, 39512, 1282, 422, 734, 3951, 286, 2370, 25, 717, 11, 262, 6287, 284, 543, 3139, 20157, 318, 1541, 16359, 11, 2884, 6376, 5153, 11, 7313, 4056, 11, 290, 584, 8385, 9383, 3341, 11, 543, 5644, 326, 262, 1487, 284, 5436, 1244, 6974, 2421, 22000, 262, 9432, 2163, 26, 290, 1218, 11, 262, 14955, 326, 1994, 2173, 287, 3139, 5939, 15866, 8904, 1910, 1176, 11, 355, 880, 355, 17895, 29275, 290, 3572, 3108, 5823, 13, 9747, 5436, 1244, 4145, 307, 1498, 284, 6431, 1877, 12, 71, 4924, 8234, 11, 5361, 287, 10730, 351, 290, 503, 12, 5589, 13629, 4683, 2839, 3139, 12, 439, 5040, 11701, 784, 4145, 11, 32532, 11, 2426, 278, 606, 284, 3220, 1910, 12883, 13, 3582, 276, 287, 428, 835, 11, 9747, 5436, 561, 407, 4031, 284, 35531, 5007, 3530, 11, 475, 6974, 284, 2426, 340, 284, 1103, 5449, 290, 4145, 787, 340, 670, 1365, 13, 198, 33234, 4035, 777, 2683, 329, 2252, 2267, 290, 262, 3917, 21147, 302, 12, 2001, 8789, 530, 13432, 925, 287, 262, 9793, 13, 632, 318, 6768, 4367, 326, 1588, 14614, 1487, 460, 3708, 43590, 26877, 1487, 11, 19911, 11, 290, 5358, 13, 887, 884, 2458, 460, 635, 1193, 5344, 290, 10114, 43936, 4888, 14895, 326, 42185, 262, 19444, 11, 6712, 11, 290, 1176, 8573, 286, 3592, 13, 554, 1948, 11, 777, 743, 4745, 319, 14895, 546, 644, 661, 460, 466, 284, 1123, 584, 326, 389, 3037, 12, 10698, 11, 475, 407, 8018, 355, 884, 1566, 262, 3037, 2458, 13, 770, 5517, 468, 7977, 890, 12, 17744, 992, 14895, 546, 262, 6573, 290, 21543, 3048, 286, 5939, 9051, 4318, 3034, 1630, 11, 475, 584, 8522, 321, 1389, 43936, 14895, 784, 287, 1948, 546, 262, 6287, 290, 1296, 286, 1176, 326, 617, 460, 5517, 625, 1854, 784, 743, 1986, 2092, 44365, 739, 1588, 12, 9888, 14614, 1487, 13, 198, 11518, 11, 287, 1948, 465, 23097, 709, 666, 1296, 11, 10969, 1115, 18203, 84, 871, 11, 543, 815, 307, 4030, 287, 2000, 618, 6402, 465, 2785, 10939, 13, 3274, 11, 318, 340, 27102, 284, 644, 4922, 23097, 709, 666, 5436, 561, 2380, 281, 29497, 4975, 393, 257, 12253, 13389, 13, 314, 2540, 262, 1628, 355, 281, 16464, 3257, 13367, 546, 14614, 1487, 290, 663, 10939, 13, 887, 12628, 803, 262, 8472, 871, 286, 15427, 23097, 709, 666, 5436, 925, 683, 6481, 804, 588, 257, 23498, 11, 772, 29497, 4975, 25, 281, 15068, 284, 2987, 257, 6209, 15098, 1080, 11, 8263, 319, 880, 4920, 2742, 11, 15855, 11, 290, 11553, 9889, 11, 543, 3568, 2407, 11670, 351, 257, 7270, 10518, 1181, 13, 770, 1624, 1276, 307, 10617, 11, 286, 1781, 11, 780, 7822, 3307, 481, 2300, 9257, 25, 617, 17670, 286, 5436, 561, 4084, 307, 523, 4334, 12, 13638, 287, 511, 38091, 286, 4318, 1630, 355, 284, 307, 27294, 351, 4096, 22008, 13, 632, 1244, 307, 257, 1402, 2239, 11, 2562, 284, 35174, 625, 11, 422, 1262, 2756, 751, 364, 284, 3376, 1598, 7097, 871, 290, 5602, 12, 38515, 11, 284, 4375, 16538, 329, 1307, 12455, 11, 28790, 1854, 11, 39974, 11, 2205, 879, 11, 44296, 11, 393, 28777, 284, 1459, 1964, 4773, 13, 4377, 13052, 326, 5436, 1244, 307, 257, 12949, 29497, 1487, 284, 262, 10959, 286, 12129, 1276, 43851, 351, 777, 7476, 784, 290, 635, 351, 262, 4427, 11, 6693, 2174, 11, 286, 4917, 257, 23498, 290, 1729, 12, 24498, 6801, 21182, 326, 5983, 422, 994, 284, 5436, 13, 198, 32, 1218, 33985, 4786, 262, 19406, 46219, 12660, 286, 5436, 25, 561, 339, 319, 5236, 307, 922, 393, 2089, 329, 1692, 9490, 30, 314, 2540, 262, 5517, 556, 43758, 319, 428, 966, 11, 290, 4251, 262, 31035, 14619, 7664, 326, 340, 714, 467, 2035, 835, 11, 6906, 319, 1486, 290, 7822, 3307, 326, 281, 12069, 379, 428, 1029, 1241, 286, 1152, 1483, 2314, 10568, 13, 6430, 428, 1998, 635, 3350, 656, 7786, 8259, 262, 4202, 286, 46219, 1293, 669, 326, 43828, 584, 19336, 319, 428, 1808, 11, 290, 703, 13770, 290, 37891, 777, 1293, 669, 1085, 3264, 284, 262, 13242, 13, 770, 13432, 8991, 8603, 319, 1111, 5389, 286, 262, 4384, 25, 319, 262, 530, 1021, 11, 284, 262, 3957, 1271, 286, 43005, 3597, 319, 9552, 4318, 5410, 11, 508, 760, 784, 351, 1310, 9110, 286, 5559, 7822, 3307, 393, 38356, 3403, 784, 326, 340, 561, 307, 922, 26, 290, 319, 262, 584, 1021, 11, 284, 262, 23226, 2274, 14268, 396, 287, 262, 36619, 11, 508, 4206, 351, 2092, 3161, 6628, 326, 340, 561, 307, 2089, 13, 6659, 632, 3568, 1598, 326, 2252, 10986, 286, 428, 2071, 815, 2792, 511, 46219, 21837, 284, 7952, 290, 2176, 14895, 546, 703, 4318, 5410, 318, 9177, 290, 644, 9889, 340, 14293, 319, 11, 287, 644, 4732, 784, 772, 379, 262, 1575, 286, 39127, 1342, 1598, 290, 1342, 20039, 7429, 13, 198, 32, 2368, 33985, 4786, 703, 284, 34404, 5436, 447, 247, 82, 1693, 11, 287, 1948, 355, 13957, 644, 339, 318, 13586, 13, 4900, 262, 3599, 4031, 373, 329, 5436, 284, 6330, 564, 250, 1169, 1910, 11, 447, 251, 1762, 832, 262, 3307, 2957, 284, 257, 9871, 1296, 286, 5436, 11, 23097, 709, 666, 5436, 11, 508, 8781, 262, 1910, 8076, 788, 8991, 18118, 16586, 16895, 284, 262, 43440, 4536, 13, 4619, 12755, 7097, 871, 290, 1910, 1176, 389, 40091, 1181, 5499, 11, 428, 1838, 5436, 804, 517, 588, 257, 9815, 24161, 784, 257, 1181, 8674, 784, 621, 257, 1910, 12, 2339, 35449, 9030, 13, 10968, 11, 379, 1123, 966, 287, 262, 4578, 810, 314, 5150, 11581, 5436, 447, 247, 82, 1308, 1177, 284, 2291, 3224, 5499, 11, 777, 635, 3114, 517, 588, 1181, 621, 1910, 5499, 784, 393, 3737, 5499, 286, 262, 1729, 12, 31353, 21803, 6567, 13, 6430, 5436, 318, 407, 784, 290, 2192, 2314, 290, 815, 407, 307, 784, 477, 286, 262, 1181, 13, 383, 1181, 857, 517, 621, 16697, 11, 290, 772, 663, 11344, 5499, 389, 407, 3614, 284, 3034, 8945, 13, 383, 1181, 12, 10728, 18645, 318, 1541, 34669, 290, 24163, 11, 257, 966, 329, 543, 1762, 832, 5436, 2810, 257, 7613, 15438, 13, 887, 16118, 5436, 2299, 16856, 11, 11476, 6249, 9010, 11, 290, 6100, 428, 1181, 12, 10728, 18645, 13, 198, 818, 9605, 11, 314, 1441, 284, 262, 1808, 286, 5436, 447, 247, 82, 48923, 2247, 11, 290, 284, 262, 749, 14851, 2071, 4376, 416, 262, 5517, 13, 314, 4752, 2029, 326, 5436, 8318, 617, 11387, 1332, 286, 48923, 2247, 11, 475, 19756, 857, 407, 1612, 1884, 13, 3412, 257, 517, 1844, 290, 40116, 13646, 326, 257, 3938, 9177, 5436, 561, 5298, 645, 5340, 3403, 561, 407, 6646, 20135, 257, 23498, 393, 10909, 6801, 3108, 284, 651, 422, 994, 284, 612, 13, 317, 14614, 24127, 286, 5436, 447, 247, 82, 5046, 290, 13357, 857, 407, 15058, 37512, 11, 475, 1276, 307, 19189, 290, 4166, 416, 10544, 508, 460, 43494, 262, 2622, 357, 45781, 8627, 8, 5046, 286, 13572, 11, 4133, 11, 290, 4934, 13, 5436, 447, 247, 82, 1103, 12, 6894, 40460, 481, 4145, 4745, 319, 1111, 2622, 14614, 9889, 290, 17070, 1919, 290, 1964, 3403, 13, 554, 428, 2754, 11, 262, 1109, 326, 5436, 12, 2339, 9889, 11, 393, 1588, 3354, 15370, 11, 389, 1541, 1944, 393, 287, 2478, 784, 351, 262, 8780, 3580, 326, 777, 13312, 389, 287, 2839, 2832, 290, 4031, 284, 5963, 2839, 393, 2665, 282, 5353, 11, 407, 3154, 1171, 3392, 784, 6630, 1111, 2842, 11, 1111, 329, 5436, 447, 247, 82, 40460, 290, 329, 262, 6034, 286, 9552, 6079, 3154, 1692, 14901, 13, 4930, 24281, 278, 10939, 1061, 13, 198, 464, 717, 4786, 262, 2526, 286, 2626, 6443, 13, 1318, 743, 880, 307, 13285, 329, 3095, 12, 4354, 9552, 13312, 326, 714, 2222, 11982, 14901, 287, 1692, 9490, 11, 1771, 832, 1223, 588, 5436, 393, 832, 584, 5479, 287, 1535, 11, 2858, 11, 3707, 11, 393, 1230, 13, 887, 611, 262, 2176, 6276, 5359, 284, 6537, 884, 3154, 4034, 13238, 9257, 422, 883, 852, 19189, 416, 2839, 10544, 11, 788, 1474, 12, 4354, 31475, 5, 35, 5370, 11, 5556, 3108, 12, 45841, 1387, 11, 743, 1674, 19836, 262, 6034, 329, 884, 43590, 2003, 4034, 13, 383, 19440, 286, 428, 2526, 8338, 319, 262, 2493, 1799, 290, 6068, 1799, 286, 9889, 784, 703, 14704, 883, 4166, 329, 2839, 393, 8976, 4959, 460, 307, 16573, 284, 4691, 1171, 393, 10112, 3392, 784, 543, 318, 7744, 8627, 13, 198, 464, 1218, 26863, 4786, 262, 7090, 12, 4354, 10939, 286, 3767, 18648, 286, 2839, 10544, 290, 5353, 287, 26727, 2478, 286, 6481, 3665, 9552, 9889, 13, 22168, 7118, 286, 9889, 714, 1716, 2116, 12, 260, 259, 18766, 784, 407, 287, 262, 28639, 12, 22930, 1335, 2565, 286, 9552, 3341, 2405, 3957, 22619, 6302, 306, 3665, 832, 45115, 2116, 12, 49453, 434, 11, 475, 287, 262, 2565, 286, 9889, 6856, 416, 1692, 10544, 664, 1834, 2280, 24175, 262, 10368, 286, 1919, 11, 3034, 11, 290, 1964, 1176, 287, 883, 10544, 447, 247, 2832, 13, 8673, 772, 4785, 621, 262, 2994, 286, 2785, 1692, 12, 33203, 803, 9889, 11, 884, 11257, 714, 1085, 284, 27377, 49483, 25650, 11, 1771, 777, 1282, 546, 351, 257, 20188, 357, 24498, 46907, 8, 393, 257, 29923, 525, 357, 24988, 37098, 2994, 286, 1692, 9490, 11, 4086, 11, 290, 2911, 737, 6469, 198, 4711, 19958, 12779, 1950, 262, 1988, 286, 1588, 1903, 11115, 287, 2478, 286, 9552, 290, 3519, 9889, 326, 389, 11777, 7977, 379, 9815, 1171, 4034, 13, 770, 743, 2128, 3489, 11, 475, 340, 743, 287, 1109, 307, 262, 749, 7702, 13052, 287, 262, 3348, 11, 780, 884, 4040, 1244, 407, 655, 13238, 9257, 422, 1944, 17092, 12, 15808, 13312, 11, 475, 635, 422, 1944, 1402, 564, 250, 20185, 329, 922, 447, 251, 4040, 11, 287, 379, 1551, 734, 19410, 13, 3274, 11, 262, 2622, 2478, 4040, 561, 407, 27383, 3481, 7048, 326, 3034, 2478, 4034, 284, 262, 42569, 12934, 784, 262, 3349, 290, 7606, 1943, 286, 23941, 5140, 612, 11, 393, 262, 4388, 7261, 12, 23213, 563, 1693, 13127, 286, 2444, 8776, 612, 784, 389, 10411, 284, 262, 19406, 1171, 4414, 13, 5498, 11, 484, 561, 407, 36059, 326, 262, 14614, 9889, 4166, 329, 2839, 5068, 4621, 481, 307, 14704, 290, 1231, 4179, 302, 12, 2934, 1420, 540, 287, 14748, 286, 1729, 12, 36313, 1171, 4959, 13, 770, 481, 307, 262, 1339, 284, 617, 4922, 11, 286, 1781, 11, 290, 257, 2478, 1430, 6095, 1171, 4414, 815, 407, 761, 8613, 302, 12, 259, 1151, 13666, 326, 460, 8603, 880, 307, 6589, 319, 1171, 290, 2839, 5672, 784, 475, 703, 881, 11, 287, 644, 45596, 11, 290, 329, 703, 890, 428, 481, 307, 262, 1339, 318, 7744, 8627, 11, 290, 340, 561, 307, 41492, 329, 257, 7271, 13338, 2478, 3626, 284, 7048, 9815, 11, 3767, 16829, 6806, 1022, 777, 13, 314, 7564, 326, 262, 10939, 286, 428, 7664, 329, 8271, 5359, 389, 1588, 784, 290, 379, 10402, 351, 1944, 11257, 287, 1171, 12, 19734, 7297, 286, 4133, 290, 4773, 784, 475, 262, 2526, 286, 3767, 11, 4591, 36487, 24126, 319, 262, 9672, 16829, 6806, 286, 8514, 284, 5963, 7606, 393, 8976, 5353, 290, 284, 4691, 3154, 1171, 3392, 11, 3568, 1165, 1588, 284, 8856, 13, 464, 1281, 5436, 784, 317, 27522, 29544, 25, 10347, 9552, 5660, 262, 18493, 11625, 17924, 30251, 30, 717, 4120, 319, 9552, 25062, 13], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(encoding.tokens()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFngOlySNd5x",
        "outputId": "66afc1d9-1ab6-45a3-8794-9a54e05679ec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3h0UWoLep_O"
      },
      "source": [
        "directory = 'data/elon-musk/tweets-2010-2021/'\n",
        "musk_tweets = pd.read_csv(f'{directory}' + '2010.csv')\n",
        "\n",
        "list_of_years = ['2011', '2012', '2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021']\n",
        "\n",
        "for year in list_of_years:\n",
        "    temp_df = pd.read_csv(f'{directory}' + year + '.csv')\n",
        "    musk_tweets = musk_tweets.append(temp_df, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "TyskPpaMrtgA",
        "outputId": "9bfae0b2-3d18-436f-e5ba-84898e6d1f92"
      },
      "source": [
        "musk_tweets.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>conversation_id</th>\n",
              "      <th>created_at</th>\n",
              "      <th>date</th>\n",
              "      <th>timezone</th>\n",
              "      <th>place</th>\n",
              "      <th>tweet</th>\n",
              "      <th>language</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>cashtags</th>\n",
              "      <th>user_id</th>\n",
              "      <th>user_id_str</th>\n",
              "      <th>username</th>\n",
              "      <th>name</th>\n",
              "      <th>day</th>\n",
              "      <th>hour</th>\n",
              "      <th>link</th>\n",
              "      <th>urls</th>\n",
              "      <th>photos</th>\n",
              "      <th>video</th>\n",
              "      <th>thumbnail</th>\n",
              "      <th>retweet</th>\n",
              "      <th>nlikes</th>\n",
              "      <th>nreplies</th>\n",
              "      <th>nretweets</th>\n",
              "      <th>quote_url</th>\n",
              "      <th>search</th>\n",
              "      <th>near</th>\n",
              "      <th>geo</th>\n",
              "      <th>source</th>\n",
              "      <th>user_rt_id</th>\n",
              "      <th>user_rt</th>\n",
              "      <th>retweet_id</th>\n",
              "      <th>reply_to</th>\n",
              "      <th>retweet_date</th>\n",
              "      <th>translate</th>\n",
              "      <th>trans_src</th>\n",
              "      <th>trans_dest</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>15434727182</td>\n",
              "      <td>15434727182</td>\n",
              "      <td>1.275676e+12</td>\n",
              "      <td>2010-06-04 18:31:57</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>44196397</td>\n",
              "      <td>44196397</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>Elon Musk</td>\n",
              "      <td>5</td>\n",
              "      <td>18</td>\n",
              "      <td>https://twitter.com/elonmusk/status/15434727182</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>4652</td>\n",
              "      <td>391</td>\n",
              "      <td>348</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>152153637639028736</td>\n",
              "      <td>152151847614943233</td>\n",
              "      <td>1.325111e+12</td>\n",
              "      <td>2011-12-28 22:27:08</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>@TheOnion So true :)</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>44196397</td>\n",
              "      <td>44196397</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>Elon Musk</td>\n",
              "      <td>3</td>\n",
              "      <td>22</td>\n",
              "      <td>https://twitter.com/elonmusk/status/152153637639028736</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>12</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>151809315026636800</td>\n",
              "      <td>151809315026636800</td>\n",
              "      <td>1.325029e+12</td>\n",
              "      <td>2011-12-27 23:38:55</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>If you ever wanted to know the *real* truth about the moon landings ...(best Onion article ever)  http://t.co/pgNEJsjI</td>\n",
              "      <td>en</td>\n",
              "      <td>[]</td>\n",
              "      <td>[]</td>\n",
              "      <td>44196397</td>\n",
              "      <td>44196397</td>\n",
              "      <td>elonmusk</td>\n",
              "      <td>Elon Musk</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>https://twitter.com/elonmusk/status/151809315026636800</td>\n",
              "      <td>['http://j.mp/vLhhov']</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>39</td>\n",
              "      <td>13</td>\n",
              "      <td>34</td>\n",
              "      <td>NaN</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>[]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                  id  ...  trans_src  trans_dest\n",
              "0           0         15434727182  ...        NaN         NaN\n",
              "1           0  152153637639028736  ...        NaN         NaN\n",
              "2           1  151809315026636800  ...        NaN         NaN\n",
              "\n",
              "[3 rows x 39 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H79ADkFnsrcP",
        "outputId": "e216f4ff-e90b-4786-c4f2-452538391ce2"
      },
      "source": [
        "musk_tweets.rename(columns={'tweet': 'text'}, inplace=True)\n",
        "musk_tweets = musk_tweets['text']\n",
        "musk_tweets.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.\n",
              "1                                                                            @TheOnion So true :)\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7veNZgJPs0RL"
      },
      "source": [
        "musk_tweets.replace(to_replace=\"@[A-Za-z0-9]+\", value=\"\", regex=True, inplace=True)\n",
        "musk_tweets.replace(to_replace=r'http\\S+', value=\"\", regex=True, inplace=True)\n",
        "musk_tweets.replace(to_replace=r'#[A-Za-z0-9]+', value=\"\", regex=True, inplace=True)\n",
        "musk_tweets = musk_tweets[musk_tweets.str.len()>=20]\n",
        "# musk_tweets = \"<endoftext>\" + musk_tweets + \"<endoftext>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1osSu0xot9eY",
        "outputId": "e231087c-5172-4e4b-9e47-d19ecf4d0a7c"
      },
      "source": [
        "musk_tweets.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                 Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.\n",
              "2                                           If you ever wanted to know the *real* truth about the moon landings ...(best Onion article ever)  \n",
              "3                                                                Walked around a neighborhood recently rebuilt with help from APJ and others  \n",
              "4                                            It was Xmas, so we brought presents for the kids at the orphanage. They don't usually get much.  \n",
              "5                  Met with UNICEF, Doctors Without Borders and Artists for Peace & Justice. I support them and would recommend others do too.\n",
              "6                          Just returned from a trip to Haiti. Covered a lot of ground and saw many tough situations. They need a lot of help.\n",
              "7                                                                        Single character Tweets are the ulitmate extension of the Twitmeme...\n",
              "9                     The Russians are having some challenges with their rockets. Too many of the engineers that designed them have retired:  \n",
              "10         We had a long and interesting conversation on many subjects. He has exciting ideas for extending his creative talents beyond music.\n",
              "11                                                                                         Kanye stopped by the SpaceX rocket factory today.  \n",
              "12                                        Model S options are out! Performance in red and black for me.  I will deliver my car in June/July.  \n",
              "13                                        Hi, I'm Art Garfunkel. Have you heard the sound of silence? Because, you know, it makes a sound...  \n",
              "14                                   Raul Campos invited me to do a guest DJ gig on KCRW.  Hear my random holiday season music selections at  \n",
              "15                                                                                                  Yum! Even better than deep fried butter:  \n",
              "16         Yeah, this really is me, as my Mom  will attest. Not sure I can handle just doing 140 char missives. Will put longer thoughts on G+\n",
              "17    Got called randomly by Kanye West today and received a download of his thoughts, ranging from shoes to Moses. He was polite, but opaque.\n",
              "18                                                    His singing and acting talent will be sorely missed:    South Park sequel coming soon...\n",
              "19                                Why does the crowd cry over the glorious leader Kim Il Sung's death?  Fear of being shot may play a role:   \n",
              "20                                                  Sam Harris also wrote a nice piece on the awesomeness of Hitchens:   May the good man RIP.\n",
              "21                               Read \"Lying\", the new book by my friend Sam Harris.  Excellent cover art and lots of good reasons not to lie!\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2CsHkXf1Z8b",
        "outputId": "e5f4801e-99c5-456a-8bef-486e41924126"
      },
      "source": [
        "len(musk_tweets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33935"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyw9hWomAV1H",
        "outputId": "2b14cb5a-97e8-47ed-db39-7bb892705726"
      },
      "source": [
        "musk_tweets.dropna(inplace=True)\n",
        "len(musk_tweets)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33935"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60UIFTAY1N2l"
      },
      "source": [
        "## Training Splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T54RFVmS1NRv"
      },
      "source": [
        "train, val = train_test_split(musk_tweets, test_size=0.2)\n",
        "test, val = train_test_split(val, test_size=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oy37CKxfmK7",
        "outputId": "efcf5d88-1c83-4c26-f5e9-2ae996e41874"
      },
      "source": [
        "print(\"Number of Train examples: \" + str(len(train)))\n",
        "print(\"Number of Val examples: \" + str(len(val)))\n",
        "print(\"Number of Test examples: \" + str(len(test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Train examples: 27148\n",
            "Number of Val examples: 3394\n",
            "Number of Test examples: 3393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCoTxXg81quI"
      },
      "source": [
        "train_path = f'{directory}' + 'train.csv'\n",
        "val_path = f'{directory}' + 'val.csv'\n",
        "test_path = f'{directory}' + 'test.csv'\n",
        "\n",
        "train.to_csv(train_path, index=False)\n",
        "val.to_csv(val_path, index=False)\n",
        "test.to_csv(test_path, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9ZvyvZyerDT"
      },
      "source": [
        "# Fine-Tuning GPT-2\n",
        "\n",
        "If we're looking to fine-tune models which are found on the HuggingFace model hub, then it becomes much easier to fine-tune the models since HuggingFace provides us with scripts.\n",
        "\n",
        "From the `transformers` repo:\n",
        "\n",
        "> There are two sets of scripts provided. The first set leverages the Trainer API. The second set with no_trainer in the suffix uses a custom training loop and leverages the 🤗 Accelerate library. Both sets use the 🤗 Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.\n",
        "\n",
        "You can learn more about it here: https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling\n",
        "\n",
        "We will be using the script that leveraged the Trainer API. We can download the script by running:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty4g9WUhfMz0"
      },
      "source": [
        "if os.path.exists('/gpt-2/run_clm.py'):\n",
        "    !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/run_clm.py -P gpt-2/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXvm0wpQxS10"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOtpCx8KfVYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e1542db-53bb-4b8b-c10c-638490044244"
      },
      "source": [
        "!python gpt-2/run_clm.py \\\n",
        "    --model_name_or_path gpt2 \\\n",
        "    --train_file data/elon-musk/tweets-2010-2021/train.csv \\\n",
        "    --validation_file data/elon-musk/tweets-2010-2021/val.csv \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --per_device_eval_batch_size=2 \\\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    --output_dir gpt-2/tmp/elon-test-clm \\\n",
        "    --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11/30/2021 00:14:04 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "11/30/2021 00:14:04 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=gpt-2/tmp/elon-test-clm/runs/Nov30_00-14-04_551d802533ba,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=gpt-2/tmp/elon-test-clm,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=2,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=gpt-2/tmp/elon-test-clm,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "11/30/2021 00:14:04 - WARNING - datasets.builder - Using custom data configuration default-c3208190fec074ad\n",
            "11/30/2021 00:14:04 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-c3208190fec074ad/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-c3208190fec074ad/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a...\n",
            "100% 2/2 [00:00<00:00, 2439.97it/s]\n",
            "11/30/2021 00:14:04 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\n",
            "11/30/2021 00:14:04 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\n",
            "100% 2/2 [00:00<00:00, 69.03it/s]\n",
            "11/30/2021 00:14:04 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "11/30/2021 00:14:04 - INFO - datasets.builder - Generating split train\n",
            "11/30/2021 00:14:05 - INFO - datasets.builder - Generating split validation\n",
            "11/30/2021 00:14:05 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-c3208190fec074ad/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 902.49it/s]\n",
            "[INFO|file_utils.py:1773] 2021-11-30 00:14:05,345 >> https://huggingface.co/gpt2/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpyu1fuisu\n",
            "Downloading: 100% 665/665 [00:00<00:00, 562kB/s]\n",
            "[INFO|file_utils.py:1777] 2021-11-30 00:14:05,444 >> storing https://huggingface.co/gpt2/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|file_utils.py:1785] 2021-11-30 00:14:05,444 >> creating metadata file for /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:602] 2021-11-30 00:14:05,444 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:639] 2021-11-30 00:14:05,445 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:344] 2021-11-30 00:14:05,543 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:602] 2021-11-30 00:14:05,759 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:639] 2021-11-30 00:14:05,760 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1773] 2021-11-30 00:14:05,973 >> https://huggingface.co/gpt2/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4k3asp0i\n",
            "Downloading: 100% 0.99M/0.99M [00:00<00:00, 8.87MB/s]\n",
            "[INFO|file_utils.py:1777] 2021-11-30 00:14:06,199 >> storing https://huggingface.co/gpt2/resolve/main/vocab.json in cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1785] 2021-11-30 00:14:06,199 >> creating metadata file for /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|file_utils.py:1773] 2021-11-30 00:14:06,304 >> https://huggingface.co/gpt2/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpjr7y9sg7\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 4.58MB/s]\n",
            "[INFO|file_utils.py:1777] 2021-11-30 00:14:06,508 >> storing https://huggingface.co/gpt2/resolve/main/merges.txt in cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1785] 2021-11-30 00:14:06,508 >> creating metadata file for /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|file_utils.py:1773] 2021-11-30 00:14:06,610 >> https://huggingface.co/gpt2/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp7p4j0ofa\n",
            "Downloading: 100% 1.29M/1.29M [00:00<00:00, 10.6MB/s]\n",
            "[INFO|file_utils.py:1777] 2021-11-30 00:14:06,850 >> storing https://huggingface.co/gpt2/resolve/main/tokenizer.json in cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|file_utils.py:1785] 2021-11-30 00:14:06,850 >> creating metadata file for /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-30 00:14:07,150 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-30 00:14:07,150 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-30 00:14:07,150 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-30 00:14:07,150 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-30 00:14:07,150 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1742] 2021-11-30 00:14:07,150 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:602] 2021-11-30 00:14:07,368 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:639] 2021-11-30 00:14:07,369 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.13.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1773] 2021-11-30 00:14:07,569 >> https://huggingface.co/gpt2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpnvpnwd0a\n",
            "Downloading: 100% 523M/523M [00:14<00:00, 38.4MB/s]\n",
            "[INFO|file_utils.py:1777] 2021-11-30 00:14:21,963 >> storing https://huggingface.co/gpt2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|file_utils.py:1785] 2021-11-30 00:14:21,963 >> creating metadata file for /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1352] 2021-11-30 00:14:21,963 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1619] 2021-11-30 00:14:24,113 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:1628] 2021-11-30 00:14:24,113 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "Running tokenizer on dataset:   0% 0/28 [00:00<?, ?ba/s]11/30/2021 00:14:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-c3208190fec074ad/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-fefea0224f9e1bd3.arrow\n",
            "Running tokenizer on dataset: 100% 28/28 [00:01<00:00, 22.59ba/s]\n",
            "Running tokenizer on dataset:   0% 0/4 [00:00<?, ?ba/s]11/30/2021 00:14:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-c3208190fec074ad/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-14f394ef7a1bd2bf.arrow\n",
            "Running tokenizer on dataset: 100% 4/4 [00:00<00:00, 30.65ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/28 [00:00<?, ?ba/s]11/30/2021 00:14:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-c3208190fec074ad/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-0e98e27fef2ce3c8.arrow\n",
            "Grouping texts in chunks of 1024: 100% 28/28 [00:00<00:00, 42.12ba/s]\n",
            "Grouping texts in chunks of 1024:   0% 0/4 [00:00<?, ?ba/s]11/30/2021 00:14:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-c3208190fec074ad/0.0.0/bf68a4c4aefa545d0712b2fcbb1b327f905bbe2f6425fbc5e8c25234acb9e14a/cache-1127eebd5c59c92c.arrow\n",
            "Grouping texts in chunks of 1024: 100% 4/4 [00:00<00:00, 47.17ba/s]\n",
            "[INFO|trainer.py:1196] 2021-11-30 00:14:39,025 >> ***** Running training *****\n",
            "[INFO|trainer.py:1197] 2021-11-30 00:14:39,026 >>   Num examples = 564\n",
            "[INFO|trainer.py:1198] 2021-11-30 00:14:39,026 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1199] 2021-11-30 00:14:39,026 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1200] 2021-11-30 00:14:39,026 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1201] 2021-11-30 00:14:39,026 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1202] 2021-11-30 00:14:39,026 >>   Total optimization steps = 846\n",
            "{'loss': 3.9963, 'learning_rate': 2.0449172576832152e-05, 'epoch': 1.77}\n",
            " 59% 500/846 [03:26<02:23,  2.41it/s][INFO|trainer.py:2003] 2021-11-30 00:18:06,543 >> Saving model checkpoint to gpt-2/tmp/elon-test-clm/checkpoint-500\n",
            "[INFO|configuration_utils.py:423] 2021-11-30 00:18:06,720 >> Configuration saved in gpt-2/tmp/elon-test-clm/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1070] 2021-11-30 00:18:16,537 >> Model weights saved in gpt-2/tmp/elon-test-clm/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2043] 2021-11-30 00:18:17,175 >> tokenizer config file saved in gpt-2/tmp/elon-test-clm/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2049] 2021-11-30 00:18:17,368 >> Special tokens file saved in gpt-2/tmp/elon-test-clm/checkpoint-500/special_tokens_map.json\n",
            "100% 846/846 [06:16<00:00,  2.42it/s][INFO|trainer.py:1417] 2021-11-30 00:20:55,729 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 376.7065, 'train_samples_per_second': 4.492, 'train_steps_per_second': 2.246, 'train_loss': 3.816978652990174, 'epoch': 3.0}\n",
            "100% 846/846 [06:16<00:00,  2.25it/s]\n",
            "[INFO|trainer.py:2003] 2021-11-30 00:20:55,738 >> Saving model checkpoint to gpt-2/tmp/elon-test-clm\n",
            "[INFO|configuration_utils.py:423] 2021-11-30 00:20:55,873 >> Configuration saved in gpt-2/tmp/elon-test-clm/config.json\n",
            "[INFO|modeling_utils.py:1070] 2021-11-30 00:21:05,537 >> Model weights saved in gpt-2/tmp/elon-test-clm/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2043] 2021-11-30 00:21:05,780 >> tokenizer config file saved in gpt-2/tmp/elon-test-clm/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2049] 2021-11-30 00:21:05,935 >> Special tokens file saved in gpt-2/tmp/elon-test-clm/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =      3.817\n",
            "  train_runtime            = 0:06:16.70\n",
            "  train_samples            =        564\n",
            "  train_samples_per_second =      4.492\n",
            "  train_steps_per_second   =      2.246\n",
            "11/30/2021 00:21:07 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:2248] 2021-11-30 00:21:07,787 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2250] 2021-11-30 00:21:07,788 >>   Num examples = 71\n",
            "[INFO|trainer.py:2253] 2021-11-30 00:21:07,788 >>   Batch size = 2\n",
            "100% 36/36 [00:04<00:00,  7.40it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_loss               =     3.5285\n",
            "  eval_runtime            = 0:00:04.87\n",
            "  eval_samples            =         71\n",
            "  eval_samples_per_second =     14.552\n",
            "  eval_steps_per_second   =      7.379\n",
            "  perplexity              =    34.0726\n",
            "[INFO|modelcard.py:449] 2021-11-30 00:21:13,072 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7q0Q066UWo-"
      },
      "source": [
        "# Let's use the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdIAGG1a27xX",
        "outputId": "f73c6a29-f0c9-4519-ccc5-f6057408b3dd"
      },
      "source": [
        "OUTPUT_DIR = \"gpt-2/tmp/elon-test-clm\"\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(OUTPUT_DIR)\n",
        "model = GPT2LMHeadModel.from_pretrained(OUTPUT_DIR)\n",
        "model = model.to(device)\n",
        "                                        \n",
        "def generate(input_str, length=250, n=5):\n",
        "  cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i in range(length):\n",
        "      outputs = model(cur_ids[:, -1024:], labels=cur_ids[:, -1024:])\n",
        "      loss, logits = outputs[:2]\n",
        "      softmax_logits = torch.softmax(logits[0,-1], dim=0)\n",
        "      next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n)\n",
        "      cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim=1)\n",
        "    output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "    output_text = tokenizer.decode(output_list)\n",
        "    return output_text\n",
        "\n",
        "def choose_from_top(probs, n=5):\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id)\n",
        "\n",
        "generated_text = generate(\"Just dropping some\")\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Just dropping some of my old stuff in the trunk. It’s been a while since I last used a car.      _AA_Carmack   Yeah, I think we should do something about it.  We have a long way to go. Will be interesting to see what happens to those who don’t support this.   I think it will be great. It will be a lot more than a mere cameo. _AA_Carmack  Yes, it’s a great game. I think we should do something about it.   _Station _Ryan _AA_Carmack       _Station _AA_Carmack  I’m not a big fan of the Tesla Autopilot software, but I do like the idea of having a car capable of recognizing pedestrians and cyclists. It’s a great idea, especially with the high speed autotracing. _Station I love you         _Padival         _Station  Yes, it will have a lot of new features coming to the Tesla Model S, including the ability to drive from the garage to the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF05N8SmVIIr",
        "outputId": "ae797da8-79c9-43ef-aa77-feca1aec9452"
      },
      "source": [
        "generated_text = generate(\"Just dropping some\")\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Just dropping some of our own resources into the ocean                   Yeah, we have to make sure we have a good product. That will be a priority.  We will make a new version of Falcon Heavy for free.  _AA_Carmack      _Sword _Sword _Sword    _AA_Carmack _Sword          We’ll try to get that done, but I think we’ll be better off with a more advanced, reusable, reusable rocket booster.        Yeah, I love it :)       I love the idea of having a Tesla in the car. It’s awesome.  _Ryan    Yeah, that's what we should do   Yeah, we will make the Model S a lot faster, but we have a lot of work to do to get it right, as we did in the beginning. We’ve had a lot of setbacks._Gardi  _Ryan Yeah, that's exactly right _AA_Carmack I’m just trying to be as polite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kzOlqi8eMys"
      },
      "source": [
        "# Compressing the Model\n",
        "\n",
        "Let's save the model as a `tar.gz` file so that we can save it in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOmuQ4tUVei9"
      },
      "source": [
        "!tar -czf gpt-2-elon-tweets.tar.gz gpt-2/tuned-models/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
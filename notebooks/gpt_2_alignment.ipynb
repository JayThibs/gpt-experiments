{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt-2-alignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/gpt-experiments/blob/main/notebooks/gpt_2_alignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lq-qHwDRba4"
      },
      "source": [
        "# Fine-Tuning GPT-2 on Alignment Texts Dataset\n",
        "\n",
        "This notebook is meant for initial experimentation of fine-tuning on the alignment text dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjSP3oGNHyJd",
        "outputId": "75d90fdd-4f6d-4226-8233-93a6973988f4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Jul  1 14:52:52 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap-rQ1P1Y058"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr14u8fcYR4y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b593396-aa96-4c36-dd75-903c0532a8b3"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers pytorch-lightning beautifulsoup4 datasets jsonlines ftfy lm_dataformat --quiet"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 585 kB 7.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 362 kB 23.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 11.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 57.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 54.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 55.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 419 kB 54.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 50.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 72.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 68.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 41.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 2.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 52.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 54.1 MB/s \n",
            "\u001b[?25h  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84TRkXXhY2U5"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvxQKSqQY3Fa"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "import random\n",
        "import jsonlines\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import seed_everything\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2Tokenizer, GPT2TokenizerFast, AutoTokenizer, TrainingArguments, Trainer, GPT2LMHeadModel\n",
        "import ftfy\n",
        "from lm_dataformat import Reader\n",
        "pd.set_option('display.max_colwidth', None)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5tY6KPsgUFQ"
      },
      "source": [
        "# Mounting Google Drive\n",
        "\n",
        "Here we will mount our Google Drive so that we can grab data and save the HuggingFace scripts, and save the model once we've fine-tuned it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNeXrm1qgWRp",
        "outputId": "b4de94dd-d9f2-4296-f84b-10769757a0ce"
      },
      "source": [
        "# For saving the data locally\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIMELLTfg76e",
        "outputId": "f6caaa93-07f7-4a13-d018-014dccd3c95d"
      },
      "source": [
        "%cd drive/MyDrive/data/ai-alignment-dataset/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/data/ai-alignment-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/JayThibs/gpt-experiments"
      ],
      "metadata": {
        "id": "UK0_tMPMwPOg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-AVhqkVeluk"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clearning and Chunking Functions\n",
        "\n",
        "Functions for preparing the data into chunks that can fit into GPT."
      ],
      "metadata": {
        "id": "IWi2WcqKH_cF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python create_finetune_csv.py \"alignment_texts.jsonl\" \"gpt\" --normalize-with-ftfy --min-unique-tokens=10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2oCbGImXtEr",
        "outputId": "a114e1eb-c09f-44d4-d1e8-f6fbf2f24426"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reading/tokenizing files: 100% 34827/34827 [08:41<00:00, 66.83it/s]\n",
            "enforce_min_unique_tokens: 100% 87765/87765 [00:03<00:00, 28657.78it/s]\n",
            "87606\n",
            "1000\n",
            "dropped 389 tokens of trailing data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "eFp7LJUKJ4Uh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import csv\n",
        "\n",
        "# i = 0\n",
        "# texts = []\n",
        "# with jsonlines.open(\"alignment_texts.jsonl\") as reader:\n",
        "#     for line in reader:\n",
        "#         text = line[\"text\"]\n",
        "#         texts.append(text)\n",
        "#         if i > 3:\n",
        "#             break\n",
        "#         # try:\n",
        "#         if text != \"\":\n",
        "#             print(text)\n",
        "#             print(len(text.split()))\n",
        "#             encoding = tokenizer(text)\n",
        "#             total_len = len(encoding.tokens())\n",
        "#             tokens = encoding.tokens()\n",
        "#             # print(tokens)\n",
        "#             print(tokenizer.decode(encoding.input_ids))\n",
        "#         # if total_len > 1024:\n",
        "#         #     break\n",
        "#         i += 1\n",
        "#         # except:\n",
        "#         #     pass"
      ],
      "metadata": {
        "id": "XYyFaGPBJTIm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60UIFTAY1N2l"
      },
      "source": [
        "## Training Splits"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "alignment_texts = pd.read_csv(\"alignment_texts_87606.csv\")"
      ],
      "metadata": {
        "id": "KAKD-owCehsb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alignment_texts = list(alignment_texts)\n",
        "alignment_texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "PGRhi7-Ee0YE",
        "outputId": "58d89aa1-9ec4-45a8-d728-b339293c8673"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<|endoftext|> Braudel is probably the most impressive historian I have read. His quantitative estimates of premodern populations and crop yields are exactly the sort of foundation you\\'d think any understanding of history would be based upon. Yet reading his magnum opus, it became steadily clearer as the books progressed that Braudel was missing some fairly fundamental economic concepts. I couldn\\'t quite put my finger on *what* was missing until a section early in book 3:\\n>... these deliberately simple tautologies make more sense to my mind than the so-called \\'irrefutable\\' pseudo-theorem of David Ricardo (1817), whose terms are well known: that the relations between two given countries depend on the \"comparative costs\" obtaining in them at the point of production\\nBraudel, apparently, is not convinced by the principle of comparative advantage. What is his objection?\\n> The division of labor on a world scale (or on world-economy-scale) cannot be described as a concerted agreement made between equal parties and always open to review... Unequal exchange, the origin of the inequality in the world, and, by the same token, the inequality of the world, the invariable generator of trade, are longstanding realities. In the economic poker game, some people have always held better cards than others...\\nIt seems Braudel is under the impression that comparative advantage is only relevant in the context of \"equal\" exchange or \"free\" trade or something along those lines.\\nIf an otherwise impressive economic historian is that deeply confused about comparative advantage, then I expect other people are similarly confused. This post is intended to clarify.\\nThe principle of comparative advantage does not require that trade be \"free\" or \"equal\" or anything of the sort. When the Portugese or the British seized monopolies on trade with India in the early modern era, those trades were certainly not free or equal. Yet the monopolists would not have made any profit whatsoever unless there were some underlying comparative advantage.\\nFor example, consider an oversimplified model of the salt trade. People historically needed lots of salt to preserve food, yet many inland areas lack local sources, so salt imports were necessary for survival. Transport by ship was historically orders of magnitude more efficient than overland, so a government in control of a major river could grab a monopoly on the salt trade. Since the people living inland could not live without it, the salt monopolist could charge quite high prices—a \"trade\" arguably not so different from threatening inland farmers with death if they did not pay up. (An exaggeration, since there were other ways to store food and overland smuggling became viable at high enough prices, but I did say it\\'s an oversimplified example.)\\nNotice that, in this example, there is a clear underlying comparative advantage: the inland farmers have a comparative disadvantage in producing salt, while the ultimate salt supplier (a salt mine or salt pan) has a comparative advantage in salt production. If the farmer could produce salt with the same opportunity cost as the salt mine/\\u200bpan, then the monopolist would have no buyers. If the salt mine/\\u200bpan had the same opportunity cost for obtaining salt as the farmers, then the monopolist would have no supplier. Absent some underlying comparative advantage between two places, the trade monopolist cannot make any profit.\\nAnother example: suppose I\\'m a transatlantic slave trader, kidnapping people in Africa and shipping them to slave markets in the Americas. It\\'s easy to see how the kidnapping part might be profitable, but why was it profitable to move people across the Atlantic? Why not save the transportation costs, and work the same slaves on plantations *in Africa* rather than plantations in the Americas? Or why not use native American slaves entirely, rather than importing Africans? Ultimately, the profits were because the Americas had a lot lower population density—there was more land, and fewer people to work it. Thus, labor was worth more in the Americas (and that same comparative advantage drove not just the slave trade, but also immigration and automation). Without a comparative advantage, enslaving people might still have been profitable, but there would be no reason to ship them across the Atlantic.\\nLet\\'s take it a step further. This argument need not involve any trade at all.\\nSuppose I\\'m the dictator of some small archipelago. I have total ownership and control over the country\\'s main industries (bananas and construction), and there\\'s an international embargo against trade with my little country, so there\\'s no trade to worry about either internally or externally. Let\\'s say I just want to maximize construction output—although I will still need to order *some* banana-growing in order to keep my construction workers fed.\\nThe question is: who and where do I order to grow bananas, and who and where do I order to build things? To maximize construction, I will want to order people with the <|endoftext|>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T54RFVmS1NRv"
      },
      "source": [
        "train, val = train_test_split(musk_tweets, test_size=0.2)\n",
        "test, val = train_test_split(val, test_size=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_oy37CKxfmK7",
        "outputId": "efcf5d88-1c83-4c26-f5e9-2ae996e41874"
      },
      "source": [
        "print(\"Number of Train examples: \" + str(len(train)))\n",
        "print(\"Number of Val examples: \" + str(len(val)))\n",
        "print(\"Number of Test examples: \" + str(len(test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Train examples: 27148\n",
            "Number of Val examples: 3394\n",
            "Number of Test examples: 3393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCoTxXg81quI"
      },
      "source": [
        "train_path = f'{directory}' + 'train.csv'\n",
        "val_path = f'{directory}' + 'val.csv'\n",
        "test_path = f'{directory}' + 'test.csv'\n",
        "\n",
        "train.to_csv(train_path, index=False)\n",
        "val.to_csv(val_path, index=False)\n",
        "test.to_csv(test_path, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9ZvyvZyerDT"
      },
      "source": [
        "# Fine-Tuning GPT-2\n",
        "\n",
        "If we're looking to fine-tune models which are found on the HuggingFace model hub, then it becomes much easier to fine-tune the models since HuggingFace provides us with scripts.\n",
        "\n",
        "From the `transformers` repo:\n",
        "\n",
        "> There are two sets of scripts provided. The first set leverages the Trainer API. The second set with no_trainer in the suffix uses a custom training loop and leverages the 🤗 Accelerate library. Both sets use the 🤗 Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.\n",
        "\n",
        "You can learn more about it here: https://github.com/huggingface/transformers/tree/master/examples/pytorch/language-modeling\n",
        "\n",
        "We will be using the script that leveraged the Trainer API. We can download the script by running:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty4g9WUhfMz0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2757d3cf-7cf5-4971-c226-ff6151a1949c"
      },
      "source": [
        "if not os.path.exists('/gpt-2/run_clm.py'):\n",
        "    !wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/run_clm.py -P gpt-2/"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-01 16:02:39--  https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/language-modeling/run_clm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 25025 (24K) [text/plain]\n",
            "Saving to: ‘gpt-2/run_clm.py’\n",
            "\n",
            "run_clm.py          100%[===================>]  24.44K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2022-07-01 16:02:39 (6.13 MB/s) - ‘gpt-2/run_clm.py’ saved [25025/25025]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXvm0wpQxS10"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOtpCx8KfVYB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae0f166e-6c3e-4b37-c23b-b3c642dacfda"
      },
      "source": [
        "!python gpt-2/run_clm.py \\\n",
        "    --model_name_or_path gpt2 \\\n",
        "    --train_file alignment_texts_87606.csv \\\n",
        "    --do_train \\\n",
        "    --fp16 \\\n",
        "    --overwrite_cache \\\n",
        "    --per_device_train_batch_size=2 \\\n",
        "    --output_dir gpt-2/tmp/alignment-texts-clm \\\n",
        "    --overwrite_output_dir"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "07/01/2022 16:02:47 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "07/01/2022 16:02:47 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=gpt-2/tmp/alignment-texts-clm/runs/Jul01_16-02-47_5604b7cbe387,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_hf,\n",
            "output_dir=gpt-2/tmp/alignment-texts-clm,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=gpt-2/tmp/alignment-texts-clm,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "07/01/2022 16:02:47 - WARNING - datasets.builder - Using custom data configuration default-d1a8e0ee8d09b7e2\n",
            "07/01/2022 16:02:47 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-d1a8e0ee8d09b7e2/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-d1a8e0ee8d09b7e2/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 2981.03it/s]\n",
            "07/01/2022 16:02:47 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "07/01/2022 16:02:49 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 24.29it/s]\n",
            "07/01/2022 16:02:49 - INFO - datasets.utils.info_utils - Unable to verify checksums.\n",
            "07/01/2022 16:02:49 - INFO - datasets.builder - Generating train split\n",
            "07/01/2022 16:02:54 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-d1a8e0ee8d09b7e2/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00,  7.72it/s]\n",
            "07/01/2022 16:02:55 - WARNING - datasets.builder - Using custom data configuration default-d1a8e0ee8d09b7e2\n",
            "07/01/2022 16:02:55 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "07/01/2022 16:02:55 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-d1a8e0ee8d09b7e2/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "07/01/2022 16:02:55 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-d1a8e0ee8d09b7e2/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
            "07/01/2022 16:02:55 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-d1a8e0ee8d09b7e2/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "07/01/2022 16:02:55 - WARNING - datasets.builder - Using custom data configuration default-d1a8e0ee8d09b7e2\n",
            "07/01/2022 16:02:55 - INFO - datasets.builder - Overwrite dataset info from restored data version.\n",
            "07/01/2022 16:02:55 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-d1a8e0ee8d09b7e2/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "07/01/2022 16:02:55 - WARNING - datasets.builder - Reusing dataset csv (/root/.cache/huggingface/datasets/csv/default-d1a8e0ee8d09b7e2/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
            "07/01/2022 16:02:55 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-d1a8e0ee8d09b7e2/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58\n",
            "[INFO|configuration_utils.py:659] 2022-07-01 16:02:55,840 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:708] 2022-07-01 16:02:55,841 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.21.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_auto.py:396] 2022-07-01 16:02:56,113 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
            "[INFO|configuration_utils.py:659] 2022-07-01 16:02:56,385 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:708] 2022-07-01 16:02:56,386 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.21.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-01 16:02:58,278 >> loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-01 16:02:58,278 >> loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-01 16:02:58,278 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-01 16:02:58,278 >> loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-01 16:02:58,278 >> loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1801] 2022-07-01 16:02:58,278 >> loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
            "[INFO|configuration_utils.py:659] 2022-07-01 16:02:58,546 >> loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
            "[INFO|configuration_utils.py:708] 2022-07-01 16:02:58,547 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.21.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|hub.py:592] 2022-07-01 16:02:58,893 >> https://huggingface.co/gpt2/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpory3cvq5\n",
            "Downloading: 100% 523M/523M [00:10<00:00, 50.4MB/s]\n",
            "[INFO|hub.py:596] 2022-07-01 16:03:09,862 >> storing https://huggingface.co/gpt2/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|hub.py:604] 2022-07-01 16:03:09,862 >> creating metadata file for /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:1999] 2022-07-01 16:03:09,862 >> loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
            "[INFO|modeling_utils.py:2389] 2022-07-01 16:03:12,133 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:2398] 2022-07-01 16:03:12,133 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "07/01/2022 16:03:12 - WARNING - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7f87876bef80> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "Running tokenizer on dataset:   0% 0/84 [00:00<?, ?ba/s]07/01/2022 16:03:13 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-d1a8e0ee8d09b7e2/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-1c80317fa3b1799d.arrow\n",
            "Running tokenizer on dataset: 100% 84/84 [01:43<00:00,  1.23s/ba]\n",
            "07/01/2022 16:04:55 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.tokenize_function at 0x7f87876beef0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Running tokenizer on dataset:   0% 0/5 [00:00<?, ?ba/s]07/01/2022 16:04:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-d1a8e0ee8d09b7e2/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-bdd640fb06671ad1.arrow\n",
            "Running tokenizer on dataset: 100% 5/5 [00:05<00:00,  1.10s/ba]\n",
            "07/01/2022 16:05:01 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.group_texts at 0x7f879e88a7a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Grouping texts in chunks of 1024:   0% 0/84 [00:00<?, ?ba/s]07/01/2022 16:05:02 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-d1a8e0ee8d09b7e2/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-3eb13b9046685257.arrow\n",
            "Grouping texts in chunks of 1024: 100% 84/84 [01:29<00:00,  1.07s/ba]\n",
            "07/01/2022 16:06:30 - INFO - datasets.fingerprint - Parameter 'function'=<function main.<locals>.group_texts at 0x7f879e88a7a0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead.\n",
            "Grouping texts in chunks of 1024:   0% 0/5 [00:00<?, ?ba/s]07/01/2022 16:06:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-d1a8e0ee8d09b7e2/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58/cache-23b8c1e9392456de.arrow\n",
            "Grouping texts in chunks of 1024: 100% 5/5 [00:04<00:00,  1.05ba/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "[INFO|trainer.py:1520] 2022-07-01 16:06:50,663 >> ***** Running training *****\n",
            "[INFO|trainer.py:1521] 2022-07-01 16:06:50,663 >>   Num examples = 81521\n",
            "[INFO|trainer.py:1522] 2022-07-01 16:06:50,663 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1523] 2022-07-01 16:06:50,663 >>   Instantaneous batch size per device = 2\n",
            "[INFO|trainer.py:1524] 2022-07-01 16:06:50,663 >>   Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "[INFO|trainer.py:1525] 2022-07-01 16:06:50,663 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1526] 2022-07-01 16:06:50,664 >>   Total optimization steps = 122283\n",
            "{'loss': 3.1258, 'learning_rate': 4.9795556209775686e-05, 'epoch': 0.01}\n",
            "  0% 500/122283 [03:16<13:18:52,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 16:10:07,426 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 16:10:07,433 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 16:10:09,083 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 16:10:09,088 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 16:10:09,093 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 3.0232, 'learning_rate': 4.959111241955137e-05, 'epoch': 0.02}\n",
            "  1% 1000/122283 [06:40<13:16:04,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 16:13:31,178 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-1000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 16:13:31,184 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 16:13:32,874 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 16:13:32,879 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 16:13:32,883 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 2.9705, 'learning_rate': 4.9386668629327054e-05, 'epoch': 0.04}\n",
            "  1% 1500/122283 [10:03<13:15:27,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 16:16:54,119 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-1500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 16:16:54,125 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 16:16:55,931 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 16:16:55,937 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 16:16:55,941 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 2.9835, 'learning_rate': 4.9182224839102744e-05, 'epoch': 0.05}\n",
            "  2% 2000/122283 [13:26<13:08:49,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 16:20:16,753 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-2000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 16:20:16,760 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 16:20:18,465 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 16:20:18,470 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 16:20:18,474 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 2.947, 'learning_rate': 4.897778104887842e-05, 'epoch': 0.06}\n",
            "  2% 2500/122283 [16:52<13:11:32,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 16:23:42,834 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-2500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 16:23:42,841 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 16:23:44,766 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 16:23:44,772 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 16:23:45,887 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 2.9124, 'learning_rate': 4.8773337258654105e-05, 'epoch': 0.07}\n",
            "  2% 3000/122283 [20:19<13:05:52,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 16:27:10,254 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-3000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 16:27:10,261 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 16:27:11,987 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-3000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 16:27:11,992 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 16:27:11,996 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 2.9215, 'learning_rate': 4.8568893468429795e-05, 'epoch': 0.09}\n",
            "  3% 3500/122283 [23:44<13:11:56,  2.50it/s][INFO|trainer.py:2506] 2022-07-01 16:30:35,576 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-3500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 16:30:35,582 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 16:30:37,238 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-3500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 16:30:37,243 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 16:30:37,247 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 2.8711, 'learning_rate': 4.836444967820548e-05, 'epoch': 0.1}\n",
            "  3% 4000/122283 [27:11<13:00:58,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 16:34:01,806 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-4000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 16:34:01,813 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 16:34:03,597 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-4000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 16:34:03,602 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 16:34:03,607 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 2.9139, 'learning_rate': 4.816000588798116e-05, 'epoch': 0.11}\n",
            "  4% 4500/122283 [30:37<12:56:59,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 16:37:28,634 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-4500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 16:37:28,640 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 16:37:30,572 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-4500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 16:37:30,578 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 16:37:30,582 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-4500/special_tokens_map.json\n",
            "{'loss': 2.909, 'learning_rate': 4.7955562097756846e-05, 'epoch': 0.12}\n",
            "  4% 5000/122283 [34:03<12:53:13,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 16:40:54,055 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-5000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 16:40:54,061 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-5000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 16:40:55,839 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-5000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 16:40:55,844 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-5000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 16:40:55,848 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-5000/special_tokens_map.json\n",
            "{'loss': 2.8681, 'learning_rate': 4.775111830753253e-05, 'epoch': 0.13}\n",
            "  4% 5500/122283 [37:29<12:50:27,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 16:44:19,825 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-5500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 16:44:19,832 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-5500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 16:44:21,560 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-5500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 16:44:21,568 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-5500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 16:44:21,573 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-5500/special_tokens_map.json\n",
            "{'loss': 2.8382, 'learning_rate': 4.7546674517308214e-05, 'epoch': 0.15}\n",
            "  5% 6000/122283 [40:53<12:50:44,  2.51it/s][INFO|trainer.py:2506] 2022-07-01 16:47:43,806 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-6000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 16:47:43,813 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-6000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 16:47:46,176 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-6000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 16:47:46,183 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-6000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 16:47:46,188 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-6000/special_tokens_map.json\n",
            "{'loss': 2.8723, 'learning_rate': 4.73422307270839e-05, 'epoch': 0.16}\n",
            "  5% 6500/122283 [44:19<12:40:58,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 16:51:09,848 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-6500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 16:51:09,855 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-6500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 16:51:11,666 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-6500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 16:51:11,671 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-6500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 16:51:11,678 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-6500/special_tokens_map.json\n",
            "{'loss': 2.8112, 'learning_rate': 4.713778693685959e-05, 'epoch': 0.17}\n",
            "  6% 7000/122283 [47:43<12:38:32,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 16:54:34,448 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-7000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 16:54:34,454 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-7000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 16:54:36,399 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-7000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 16:54:36,405 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-7000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 16:54:36,411 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-7000/special_tokens_map.json\n",
            "{'loss': 2.8817, 'learning_rate': 4.6933343146635265e-05, 'epoch': 0.18}\n",
            "  6% 7500/122283 [51:09<12:37:47,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 16:57:59,950 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-7500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 16:57:59,957 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-7500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 16:58:01,708 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-7500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 16:58:01,715 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-7500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 16:58:01,721 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-7500/special_tokens_map.json\n",
            "{'loss': 2.8813, 'learning_rate': 4.672889935641095e-05, 'epoch': 0.2}\n",
            "  7% 8000/122283 [54:34<12:36:21,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 17:01:24,862 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-8000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:01:24,869 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-8000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:01:26,779 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-8000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:01:26,784 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-8000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:01:26,787 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-8000/special_tokens_map.json\n",
            "{'loss': 2.8099, 'learning_rate': 4.652445556618664e-05, 'epoch': 0.21}\n",
            "  7% 8500/122283 [58:00<12:27:28,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 17:04:51,386 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-8500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:04:51,393 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-8500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:04:53,149 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-8500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:04:53,154 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-8500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:04:53,158 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-8500/special_tokens_map.json\n",
            "{'loss': 2.8693, 'learning_rate': 4.6320011775962316e-05, 'epoch': 0.22}\n",
            "  7% 9000/122283 [1:01:33<12:28:37,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 17:08:23,809 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-9000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:08:23,816 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-9000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:08:25,625 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-9000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:08:25,629 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-9000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:08:25,634 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-9000/special_tokens_map.json\n",
            "{'loss': 2.8338, 'learning_rate': 4.6115567985738e-05, 'epoch': 0.23}\n",
            "  8% 9500/122283 [1:04:58<12:26:58,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 17:11:49,140 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-9500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:11:49,147 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-9500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:11:51,064 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-9500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:11:51,069 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-9500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:11:51,087 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-9500/special_tokens_map.json\n",
            "{'loss': 2.8302, 'learning_rate': 4.591112419551369e-05, 'epoch': 0.25}\n",
            "  8% 10000/122283 [1:08:24<12:25:09,  2.51it/s][INFO|trainer.py:2506] 2022-07-01 17:15:14,730 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-10000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:15:14,737 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-10000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:15:16,512 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-10000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:15:16,517 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-10000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:15:16,520 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-10000/special_tokens_map.json\n",
            "{'loss': 2.8224, 'learning_rate': 4.570668040528937e-05, 'epoch': 0.26}\n",
            "  9% 10500/122283 [1:11:48<12:16:19,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 17:18:38,825 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-10500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:18:38,831 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-10500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:18:40,540 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-10500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:18:40,545 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-10500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:18:40,549 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-10500/special_tokens_map.json\n",
            "{'loss': 2.8091, 'learning_rate': 4.550223661506506e-05, 'epoch': 0.27}\n",
            "  9% 11000/122283 [1:15:14<12:17:50,  2.51it/s][INFO|trainer.py:2506] 2022-07-01 17:22:04,749 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-11000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:22:04,756 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-11000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:22:06,573 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-11000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:22:06,578 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-11000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:22:06,583 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-11000/special_tokens_map.json\n",
            "{'loss': 2.8394, 'learning_rate': 4.529779282484074e-05, 'epoch': 0.28}\n",
            "  9% 11500/122283 [1:18:39<12:08:20,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 17:25:29,929 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-11500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:25:29,934 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-11500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:25:31,615 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-11500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:25:31,620 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-11500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:25:31,624 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-11500/special_tokens_map.json\n",
            "{'loss': 2.8372, 'learning_rate': 4.5093349034616425e-05, 'epoch': 0.29}\n",
            " 10% 12000/122283 [1:22:04<12:08:15,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 17:28:55,307 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-12000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:28:55,313 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-12000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:28:56,939 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-12000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:28:56,944 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-12000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:28:57,846 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-12000/special_tokens_map.json\n",
            "{'loss': 2.8375, 'learning_rate': 4.488890524439211e-05, 'epoch': 0.31}\n",
            " 10% 12500/122283 [1:25:30<12:02:17,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 17:32:20,927 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-12500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:32:20,934 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-12500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:32:23,597 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-12500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:32:23,602 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-12500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:32:23,607 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-12500/special_tokens_map.json\n",
            "{'loss': 2.8002, 'learning_rate': 4.468446145416779e-05, 'epoch': 0.32}\n",
            " 11% 13000/122283 [1:28:55<12:00:20,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 17:35:46,432 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-13000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:35:46,437 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-13000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:35:48,158 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-13000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:35:48,163 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-13000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:35:48,167 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-13000/special_tokens_map.json\n",
            "{'loss': 2.8388, 'learning_rate': 4.4480017663943477e-05, 'epoch': 0.33}\n",
            " 11% 13500/122283 [1:32:21<11:52:57,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 17:39:12,036 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-13500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:39:12,042 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-13500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:39:13,783 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-13500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:39:13,788 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-13500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:39:13,792 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-13500/special_tokens_map.json\n",
            "{'loss': 2.7791, 'learning_rate': 4.427557387371916e-05, 'epoch': 0.34}\n",
            " 11% 14000/122283 [1:35:45<11:58:04,  2.51it/s][INFO|trainer.py:2506] 2022-07-01 17:42:36,422 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-14000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:42:36,428 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-14000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:42:38,009 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-14000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:42:38,015 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-14000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:42:38,019 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-14000/special_tokens_map.json\n",
            "{'loss': 2.8106, 'learning_rate': 4.4071130083494844e-05, 'epoch': 0.36}\n",
            " 12% 14500/122283 [1:39:10<11:52:52,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 17:46:01,394 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-14500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:46:01,399 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-14500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:46:03,048 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-14500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:46:03,053 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-14500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:46:03,058 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-14500/special_tokens_map.json\n",
            "{'loss': 2.8096, 'learning_rate': 4.3866686293270534e-05, 'epoch': 0.37}\n",
            " 12% 15000/122283 [1:42:34<11:48:13,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 17:49:25,580 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-15000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:49:25,586 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-15000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:49:28,428 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-15000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:49:28,441 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-15000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:49:28,445 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-15000/special_tokens_map.json\n",
            "{'loss': 2.8443, 'learning_rate': 4.366224250304621e-05, 'epoch': 0.38}\n",
            " 13% 15500/122283 [1:46:00<11:43:00,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 17:52:50,809 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-15500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:52:50,814 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-15500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:52:52,616 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-15500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:52:52,621 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-15500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:52:52,626 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-15500/special_tokens_map.json\n",
            "{'loss': 2.8253, 'learning_rate': 4.34577987128219e-05, 'epoch': 0.39}\n",
            " 13% 16000/122283 [1:49:25<11:42:03,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 17:56:15,841 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-16000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:56:15,848 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-16000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:56:17,541 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-16000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:56:17,545 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-16000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:56:17,549 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-16000/special_tokens_map.json\n",
            "{'loss': 2.7834, 'learning_rate': 4.3253354922597586e-05, 'epoch': 0.4}\n",
            " 13% 16500/122283 [1:52:50<11:39:29,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 17:59:41,578 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-16500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 17:59:41,584 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-16500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 17:59:43,344 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-16500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 17:59:43,348 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-16500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 17:59:43,352 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-16500/special_tokens_map.json\n",
            "{'loss': 2.81, 'learning_rate': 4.304891113237326e-05, 'epoch': 0.42}\n",
            " 14% 17000/122283 [1:56:16<11:30:19,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 18:03:07,027 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-17000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:03:07,032 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-17000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:03:08,759 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-17000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:03:08,769 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-17000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:03:08,773 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-17000/special_tokens_map.json\n",
            "{'loss': 2.7776, 'learning_rate': 4.284446734214895e-05, 'epoch': 0.43}\n",
            " 14% 17500/122283 [1:59:40<11:53:10,  2.45it/s][INFO|trainer.py:2506] 2022-07-01 18:06:31,181 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-17500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:06:31,187 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-17500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:06:33,084 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-17500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:06:33,089 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-17500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:06:33,094 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-17500/special_tokens_map.json\n",
            "{'loss': 2.8037, 'learning_rate': 4.264002355192464e-05, 'epoch': 0.44}\n",
            " 15% 18000/122283 [2:03:06<11:30:14,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 18:09:57,585 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-18000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:09:57,592 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-18000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:09:59,406 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-18000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:09:59,412 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-18000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:10:00,205 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-18000/special_tokens_map.json\n",
            "{'loss': 2.7756, 'learning_rate': 4.243557976170032e-05, 'epoch': 0.45}\n",
            " 15% 18500/122283 [2:06:33<11:24:09,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 18:13:24,102 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-18500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:13:24,120 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-18500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:13:25,824 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-18500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:13:25,830 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-18500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:13:25,840 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-18500/special_tokens_map.json\n",
            "{'loss': 2.8066, 'learning_rate': 4.2231135971476004e-05, 'epoch': 0.47}\n",
            " 16% 19000/122283 [2:09:57<11:20:31,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 18:16:48,462 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-19000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:16:48,468 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-19000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:16:50,337 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-19000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:16:50,344 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-19000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:16:50,348 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-19000/special_tokens_map.json\n",
            "{'loss': 2.7938, 'learning_rate': 4.202669218125169e-05, 'epoch': 0.48}\n",
            " 16% 19500/122283 [2:13:23<11:21:04,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 18:20:13,708 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-19500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:20:13,714 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-19500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:20:15,624 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-19500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:20:15,629 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-19500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:20:15,634 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-19500/special_tokens_map.json\n",
            "{'loss': 2.8033, 'learning_rate': 4.182224839102737e-05, 'epoch': 0.49}\n",
            " 16% 20000/122283 [2:16:48<11:13:46,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 18:23:39,402 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-20000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:23:39,408 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-20000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:23:41,289 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-20000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:23:41,294 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-20000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:23:41,299 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-20000/special_tokens_map.json\n",
            "{'loss': 2.7574, 'learning_rate': 4.1617804600803055e-05, 'epoch': 0.5}\n",
            " 17% 20500/122283 [2:20:14<11:10:06,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 18:27:05,528 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-20500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:27:05,534 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-20500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:27:07,470 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-20500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:27:07,475 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-20500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:27:07,479 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-20500/special_tokens_map.json\n",
            "{'loss': 2.7742, 'learning_rate': 4.1413360810578746e-05, 'epoch': 0.52}\n",
            " 17% 21000/122283 [2:23:40<11:06:47,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 18:30:31,173 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-21000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:30:31,180 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-21000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:30:33,010 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-21000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:30:33,738 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-21000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:30:33,977 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-21000/special_tokens_map.json\n",
            "{'loss': 2.7744, 'learning_rate': 4.120891702035442e-05, 'epoch': 0.53}\n",
            " 18% 21500/122283 [2:27:07<11:12:38,  2.50it/s][INFO|trainer.py:2506] 2022-07-01 18:33:57,995 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-21500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:33:58,000 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-21500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:33:59,964 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-21500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:33:59,968 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-21500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:34:00,923 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-21500/special_tokens_map.json\n",
            "{'loss': 2.7569, 'learning_rate': 4.100447323013011e-05, 'epoch': 0.54}\n",
            " 18% 22000/122283 [2:30:34<11:11:24,  2.49it/s][INFO|trainer.py:2506] 2022-07-01 18:37:25,380 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-22000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:37:25,386 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-22000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:37:27,443 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-22000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:37:27,451 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-22000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:37:27,456 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-22000/special_tokens_map.json\n",
            "{'loss': 2.7752, 'learning_rate': 4.08000294399058e-05, 'epoch': 0.55}\n",
            " 18% 22500/122283 [2:34:01<10:56:21,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 18:40:52,085 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-22500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:40:52,090 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-22500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:40:53,964 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-22500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:40:53,969 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-22500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:40:53,973 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-22500/special_tokens_map.json\n",
            "{'loss': 2.8026, 'learning_rate': 4.059558564968148e-05, 'epoch': 0.56}\n",
            " 19% 23000/122283 [2:37:28<10:53:24,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 18:44:18,707 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-23000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:44:18,713 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-23000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:44:20,478 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-23000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:44:20,484 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-23000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:44:20,488 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-23000/special_tokens_map.json\n",
            "{'loss': 2.8085, 'learning_rate': 4.0391141859457165e-05, 'epoch': 0.58}\n",
            " 19% 23500/122283 [2:40:54<10:51:37,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 18:47:44,777 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-23500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:47:44,783 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-23500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:47:47,505 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-23500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:47:47,510 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-23500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:47:47,527 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-23500/special_tokens_map.json\n",
            "{'loss': 2.7673, 'learning_rate': 4.018669806923285e-05, 'epoch': 0.59}\n",
            " 20% 24000/122283 [2:44:21<10:50:37,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 18:51:11,935 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-24000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:51:11,942 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-24000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:51:13,831 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-24000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:51:13,836 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-24000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:51:13,840 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-24000/special_tokens_map.json\n",
            "{'loss': 2.7526, 'learning_rate': 3.998225427900853e-05, 'epoch': 0.6}\n",
            " 20% 24500/122283 [2:47:46<10:46:27,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 18:54:36,798 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-24500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:54:36,805 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-24500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:54:39,441 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-24500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:54:39,447 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-24500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:54:39,451 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-24500/special_tokens_map.json\n",
            "{'loss': 2.7653, 'learning_rate': 3.9777810488784216e-05, 'epoch': 0.61}\n",
            " 20% 25000/122283 [2:51:12<10:40:05,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 18:58:03,172 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-25000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 18:58:03,179 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-25000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 18:58:05,082 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-25000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 18:58:05,088 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-25000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 18:58:05,093 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-25000/special_tokens_map.json\n",
            "{'loss': 2.7815, 'learning_rate': 3.95733666985599e-05, 'epoch': 0.63}\n",
            " 21% 25500/122283 [2:54:37<10:37:58,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 19:01:27,940 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-25500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:01:27,946 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-25500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:01:29,786 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-25500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:01:29,792 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-25500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:01:29,797 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-25500/special_tokens_map.json\n",
            "{'loss': 2.7691, 'learning_rate': 3.936892290833558e-05, 'epoch': 0.64}\n",
            " 21% 26000/122283 [2:58:03<10:34:21,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 19:04:53,889 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-26000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:04:53,895 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-26000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:04:55,793 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-26000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:04:55,798 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-26000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:04:55,803 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-26000/special_tokens_map.json\n",
            "{'loss': 2.735, 'learning_rate': 3.916447911811127e-05, 'epoch': 0.65}\n",
            " 22% 26500/122283 [3:01:29<10:47:12,  2.47it/s][INFO|trainer.py:2506] 2022-07-01 19:08:19,863 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-26500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:08:19,870 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-26500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:08:21,938 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-26500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:08:21,943 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-26500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:08:21,947 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-26500/special_tokens_map.json\n",
            "{'loss': 2.7577, 'learning_rate': 3.896003532788695e-05, 'epoch': 0.66}\n",
            " 22% 27000/122283 [3:04:56<10:25:54,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 19:11:46,706 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-27000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:11:46,713 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-27000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:11:48,611 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-27000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:11:48,617 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-27000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:11:48,622 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-27000/special_tokens_map.json\n",
            "{'loss': 2.7543, 'learning_rate': 3.875559153766264e-05, 'epoch': 0.67}\n",
            " 22% 27500/122283 [3:08:22<10:24:11,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 19:15:12,988 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-27500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:15:12,995 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-27500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:15:14,893 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-27500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:15:14,899 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-27500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:15:14,903 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-27500/special_tokens_map.json\n",
            "{'loss': 2.7977, 'learning_rate': 3.855114774743832e-05, 'epoch': 0.69}\n",
            " 23% 28000/122283 [3:11:48<10:28:13,  2.50it/s][INFO|trainer.py:2506] 2022-07-01 19:18:39,415 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-28000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:18:39,422 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-28000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:18:41,936 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-28000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:18:41,941 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-28000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:18:41,945 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-28000/special_tokens_map.json\n",
            "{'loss': 2.7599, 'learning_rate': 3.8346703957214e-05, 'epoch': 0.7}\n",
            " 23% 28500/122283 [3:15:15<10:20:07,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 19:22:06,372 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-28500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:22:06,379 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-28500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:22:08,748 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-28500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:22:08,756 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-28500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:22:08,761 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-28500/special_tokens_map.json\n",
            "{'loss': 2.7798, 'learning_rate': 3.814226016698969e-05, 'epoch': 0.71}\n",
            " 24% 29000/122283 [3:18:42<10:13:31,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 19:25:33,239 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-29000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:25:33,246 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-29000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:25:35,143 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-29000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:25:36,055 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-29000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:25:36,059 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-29000/special_tokens_map.json\n",
            "{'loss': 2.7206, 'learning_rate': 3.7937816376765376e-05, 'epoch': 0.72}\n",
            " 24% 29500/122283 [3:22:09<10:18:28,  2.50it/s][INFO|trainer.py:2506] 2022-07-01 19:28:59,846 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-29500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:28:59,851 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-29500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:29:01,683 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-29500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:29:01,689 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-29500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:29:01,693 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-29500/special_tokens_map.json\n",
            "{'loss': 2.7335, 'learning_rate': 3.773337258654106e-05, 'epoch': 0.74}\n",
            " 25% 30000/122283 [3:25:34<10:08:29,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 19:32:25,400 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-30000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:32:25,406 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-30000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:32:27,311 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-30000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:32:27,316 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-30000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:32:27,320 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-30000/special_tokens_map.json\n",
            "{'loss': 2.7901, 'learning_rate': 3.7528928796316744e-05, 'epoch': 0.75}\n",
            " 25% 30500/122283 [3:28:59<10:03:04,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 19:35:50,055 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-30500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:35:50,061 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-30500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:35:51,971 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-30500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:35:51,977 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-30500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:35:51,981 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-30500/special_tokens_map.json\n",
            "{'loss': 2.7389, 'learning_rate': 3.732448500609243e-05, 'epoch': 0.76}\n",
            " 25% 31000/122283 [3:32:25<10:02:51,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 19:39:15,794 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-31000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:39:15,800 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-31000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:39:17,644 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-31000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:39:17,650 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-31000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:39:17,654 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-31000/special_tokens_map.json\n",
            "{'loss': 2.7732, 'learning_rate': 3.712004121586811e-05, 'epoch': 0.77}\n",
            " 26% 31500/122283 [3:35:51<9:58:39,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 19:42:42,038 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-31500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:42:42,044 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-31500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:42:43,897 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-31500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:42:43,903 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-31500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:42:43,908 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-31500/special_tokens_map.json\n",
            "{'loss': 2.7321, 'learning_rate': 3.6915597425643795e-05, 'epoch': 0.79}\n",
            " 26% 32000/122283 [3:39:17<10:05:54,  2.48it/s][INFO|trainer.py:2506] 2022-07-01 19:46:08,545 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-32000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:46:08,551 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-32000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:46:10,597 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-32000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:46:10,602 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-32000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:46:10,607 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-32000/special_tokens_map.json\n",
            "{'loss': 2.7505, 'learning_rate': 3.6711153635419485e-05, 'epoch': 0.8}\n",
            " 27% 32500/122283 [3:42:43<9:48:46,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 19:49:34,180 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-32500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:49:34,186 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-32500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:49:36,011 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-32500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:49:37,006 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-32500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:49:37,011 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-32500/special_tokens_map.json\n",
            "{'loss': 2.7808, 'learning_rate': 3.650670984519516e-05, 'epoch': 0.81}\n",
            " 27% 33000/122283 [3:46:09<9:46:50,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 19:53:00,448 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-33000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:53:00,455 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-33000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:53:02,334 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-33000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:53:02,340 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-33000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:53:02,344 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-33000/special_tokens_map.json\n",
            "{'loss': 2.7443, 'learning_rate': 3.6302266054970846e-05, 'epoch': 0.82}\n",
            " 27% 33500/122283 [3:49:33<9:42:17,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 19:56:24,377 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-33500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:56:24,383 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-33500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:56:26,178 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-33500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:56:26,183 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-33500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:56:26,187 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-33500/special_tokens_map.json\n",
            "{'loss': 2.698, 'learning_rate': 3.6097822264746536e-05, 'epoch': 0.83}\n",
            " 28% 34000/122283 [3:52:58<9:41:27,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 19:59:49,496 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-34000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 19:59:49,502 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-34000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 19:59:51,357 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-34000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 19:59:51,362 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-34000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 19:59:51,366 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-34000/special_tokens_map.json\n",
            "{'loss': 2.7316, 'learning_rate': 3.589337847452221e-05, 'epoch': 0.85}\n",
            " 28% 34500/122283 [3:56:24<9:39:31,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 20:03:14,840 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-34500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:03:14,845 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-34500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:03:16,719 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-34500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:03:16,724 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-34500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:03:16,729 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-34500/special_tokens_map.json\n",
            "{'loss': 2.7478, 'learning_rate': 3.5688934684297904e-05, 'epoch': 0.86}\n",
            " 29% 35000/122283 [3:59:49<9:33:26,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 20:06:40,579 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-35000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:06:40,585 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-35000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:06:42,498 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-35000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:06:42,504 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-35000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:06:42,508 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-35000/special_tokens_map.json\n",
            "{'loss': 2.7405, 'learning_rate': 3.548449089407359e-05, 'epoch': 0.87}\n",
            " 29% 35500/122283 [4:03:14<9:30:19,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 20:10:05,508 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-35500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:10:05,514 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-35500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:10:07,266 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-35500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:10:08,265 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-35500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:10:08,269 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-35500/special_tokens_map.json\n",
            "{'loss': 2.7482, 'learning_rate': 3.5280047103849265e-05, 'epoch': 0.88}\n",
            " 29% 36000/122283 [4:06:39<9:28:09,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 20:13:30,676 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-36000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:13:30,684 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-36000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:13:32,558 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-36000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:13:33,518 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-36000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:13:33,524 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-36000/special_tokens_map.json\n",
            "{'loss': 2.7397, 'learning_rate': 3.5075603313624955e-05, 'epoch': 0.9}\n",
            " 30% 36500/122283 [4:10:05<9:30:49,  2.50it/s][INFO|trainer.py:2506] 2022-07-01 20:16:56,270 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-36500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:16:56,277 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-36500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:16:58,381 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-36500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:16:58,388 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-36500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:16:58,394 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-36500/special_tokens_map.json\n",
            "{'loss': 2.7444, 'learning_rate': 3.487115952340064e-05, 'epoch': 0.91}\n",
            " 30% 37000/122283 [4:13:29<9:20:04,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 20:20:20,549 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-37000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:20:20,556 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-37000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:20:22,366 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-37000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:20:22,371 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-37000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:20:22,375 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-37000/special_tokens_map.json\n",
            "{'loss': 2.7208, 'learning_rate': 3.466671573317632e-05, 'epoch': 0.92}\n",
            " 31% 37500/122283 [4:16:54<9:22:18,  2.51it/s][INFO|trainer.py:2506] 2022-07-01 20:23:45,413 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-37500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:23:45,419 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-37500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:23:47,174 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-37500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:23:47,180 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-37500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:23:47,184 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-37500/special_tokens_map.json\n",
            "{'loss': 2.7956, 'learning_rate': 3.4462271942952006e-05, 'epoch': 0.93}\n",
            " 31% 38000/122283 [4:20:19<9:13:50,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 20:27:10,662 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-38000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:27:10,667 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-38000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:27:12,506 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-38000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:27:13,514 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-38000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:27:13,518 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-38000/special_tokens_map.json\n",
            "{'loss': 2.7561, 'learning_rate': 3.425782815272769e-05, 'epoch': 0.94}\n",
            " 31% 38500/122283 [4:23:44<9:09:27,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 20:30:35,233 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-38500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:30:35,251 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-38500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:30:37,072 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-38500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:30:37,077 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-38500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:30:37,081 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-38500/special_tokens_map.json\n",
            "{'loss': 2.7191, 'learning_rate': 3.4053384362503374e-05, 'epoch': 0.96}\n",
            " 32% 39000/122283 [4:27:08<9:05:51,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 20:33:58,786 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-39000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:33:58,795 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-39000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:34:00,587 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-39000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:34:00,599 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-39000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:34:00,603 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-39000/special_tokens_map.json\n",
            "{'loss': 2.7915, 'learning_rate': 3.384894057227906e-05, 'epoch': 0.97}\n",
            " 32% 39500/122283 [4:30:32<9:02:30,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 20:37:23,047 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-39500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:37:23,053 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-39500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:37:24,853 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-39500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:37:24,859 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-39500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:37:24,862 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-39500/special_tokens_map.json\n",
            "{'loss': 2.7058, 'learning_rate': 3.364449678205474e-05, 'epoch': 0.98}\n",
            " 33% 40000/122283 [4:33:56<8:59:55,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 20:40:47,092 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-40000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:40:47,098 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-40000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:40:48,880 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-40000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:40:48,885 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-40000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:40:48,890 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-40000/special_tokens_map.json\n",
            "{'loss': 2.7388, 'learning_rate': 3.344005299183043e-05, 'epoch': 0.99}\n",
            " 33% 40500/122283 [4:37:21<8:56:17,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 20:44:11,877 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-40500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:44:11,883 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-40500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:44:13,658 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-40500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:44:13,663 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-40500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:44:13,667 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-40500/special_tokens_map.json\n",
            "{'loss': 2.7113, 'learning_rate': 3.323560920160611e-05, 'epoch': 1.01}\n",
            " 34% 41000/122283 [4:40:44<8:54:01,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 20:47:35,558 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-41000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:47:35,563 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-41000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:47:38,181 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-41000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:47:38,186 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-41000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:47:38,191 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-41000/special_tokens_map.json\n",
            "{'loss': 2.6542, 'learning_rate': 3.30311654113818e-05, 'epoch': 1.02}\n",
            " 34% 41500/122283 [4:44:10<8:50:09,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 20:51:01,549 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-41500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:51:01,570 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-41500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:51:03,229 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-41500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:51:03,235 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-41500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:51:03,238 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-41500/special_tokens_map.json\n",
            "{'loss': 2.6839, 'learning_rate': 3.282672162115748e-05, 'epoch': 1.03}\n",
            " 34% 42000/122283 [4:47:34<8:50:47,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 20:54:24,807 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-42000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:54:24,813 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-42000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:54:26,630 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-42000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:54:26,636 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-42000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:54:26,640 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-42000/special_tokens_map.json\n",
            "{'loss': 2.6702, 'learning_rate': 3.262227783093316e-05, 'epoch': 1.04}\n",
            " 35% 42500/122283 [4:50:58<8:45:10,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 20:57:49,418 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-42500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 20:57:49,424 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-42500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 20:57:51,234 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-42500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 20:57:51,240 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-42500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 20:57:51,244 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-42500/special_tokens_map.json\n",
            "{'loss': 2.6787, 'learning_rate': 3.241783404070885e-05, 'epoch': 1.05}\n",
            " 35% 43000/122283 [4:54:22<8:54:09,  2.47it/s][INFO|trainer.py:2506] 2022-07-01 21:01:13,157 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-43000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:01:13,164 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-43000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:01:15,127 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-43000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:01:15,133 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-43000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:01:15,139 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-43000/special_tokens_map.json\n",
            "{'loss': 2.6672, 'learning_rate': 3.2213390250484534e-05, 'epoch': 1.07}\n",
            " 36% 43500/122283 [4:57:47<8:36:16,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 21:04:38,325 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-43500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:04:38,330 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-43500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:04:39,968 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-43500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:04:39,974 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-43500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:04:39,978 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-43500/special_tokens_map.json\n",
            "{'loss': 2.6725, 'learning_rate': 3.200894646026022e-05, 'epoch': 1.08}\n",
            " 36% 44000/122283 [5:01:12<8:33:41,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 21:08:03,460 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-44000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:08:03,466 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-44000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:08:05,074 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-44000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:08:05,959 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-44000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:08:05,965 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-44000/special_tokens_map.json\n",
            "{'loss': 2.6813, 'learning_rate': 3.18045026700359e-05, 'epoch': 1.09}\n",
            " 36% 44500/122283 [5:04:37<8:29:21,  2.55it/s][INFO|trainer.py:2506] 2022-07-01 21:11:27,991 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-44500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:11:27,997 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-44500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:11:29,788 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-44500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:11:29,794 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-44500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:11:29,798 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-44500/special_tokens_map.json\n",
            "{'loss': 2.6458, 'learning_rate': 3.1600058879811585e-05, 'epoch': 1.1}\n",
            " 37% 45000/122283 [5:08:02<8:26:31,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 21:14:53,218 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-45000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:14:53,224 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-45000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:14:55,029 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-45000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:14:56,040 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-45000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:14:56,044 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-45000/special_tokens_map.json\n",
            "{'loss': 2.6892, 'learning_rate': 3.139561508958727e-05, 'epoch': 1.12}\n",
            " 37% 45500/122283 [5:11:26<8:25:30,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 21:18:17,682 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-45500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:18:17,689 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-45500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:18:19,447 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-45500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:18:19,453 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-45500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:18:19,458 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-45500/special_tokens_map.json\n",
            "{'loss': 2.6626, 'learning_rate': 3.119117129936295e-05, 'epoch': 1.13}\n",
            " 38% 46000/122283 [5:14:51<8:22:35,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 21:21:42,642 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-46000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:21:42,648 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-46000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:21:44,375 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-46000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:21:44,380 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-46000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:21:44,385 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-46000/special_tokens_map.json\n",
            "{'loss': 2.6522, 'learning_rate': 3.098672750913864e-05, 'epoch': 1.14}\n",
            " 38% 46500/122283 [5:18:16<8:17:19,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 21:25:06,724 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-46500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:25:06,729 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-46500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:25:08,550 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-46500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:25:08,557 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-46500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:25:08,561 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-46500/special_tokens_map.json\n",
            "{'loss': 2.6632, 'learning_rate': 3.078228371891432e-05, 'epoch': 1.15}\n",
            " 38% 47000/122283 [5:21:40<8:18:00,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 21:28:31,058 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-47000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:28:31,064 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-47000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:28:32,875 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-47000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:28:32,881 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-47000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:28:32,885 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-47000/special_tokens_map.json\n",
            "{'loss': 2.693, 'learning_rate': 3.0577839928690004e-05, 'epoch': 1.17}\n",
            " 39% 47500/122283 [5:25:04<8:11:05,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 21:31:54,928 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-47500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:31:54,935 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-47500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:31:56,761 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-47500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:31:56,767 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-47500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:31:56,773 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-47500/special_tokens_map.json\n",
            "{'loss': 2.6678, 'learning_rate': 3.0373396138465694e-05, 'epoch': 1.18}\n",
            " 39% 48000/122283 [5:28:28<8:08:43,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 21:35:19,138 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-48000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:35:19,145 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-48000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:35:21,001 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-48000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:35:21,006 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-48000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:35:21,011 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-48000/special_tokens_map.json\n",
            "{'loss': 2.6471, 'learning_rate': 3.0168952348241375e-05, 'epoch': 1.19}\n",
            " 40% 48500/122283 [5:31:52<8:03:47,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 21:38:43,530 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-48500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:38:43,536 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-48500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:38:46,213 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-48500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:38:46,219 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-48500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:38:46,224 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-48500/special_tokens_map.json\n",
            "{'loss': 2.6438, 'learning_rate': 2.9964508558017062e-05, 'epoch': 1.2}\n",
            " 40% 49000/122283 [5:35:17<8:00:12,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 21:42:08,139 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-49000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:42:08,146 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-49000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:42:09,866 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-49000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:42:09,872 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-49000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:42:09,875 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-49000/special_tokens_map.json\n",
            "{'loss': 2.7106, 'learning_rate': 2.9760064767792745e-05, 'epoch': 1.21}\n",
            " 40% 49500/122283 [5:38:41<7:58:50,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 21:45:32,431 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-49500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:45:32,438 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-49500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:45:35,133 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-49500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:45:35,138 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-49500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:45:35,143 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-49500/special_tokens_map.json\n",
            "{'loss': 2.7013, 'learning_rate': 2.9555620977568426e-05, 'epoch': 1.23}\n",
            " 41% 50000/122283 [5:42:06<7:55:12,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 21:48:57,297 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-50000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:48:57,304 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-50000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:48:59,169 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-50000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:48:59,174 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-50000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:48:59,179 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-50000/special_tokens_map.json\n",
            "{'loss': 2.6783, 'learning_rate': 2.9351177187344113e-05, 'epoch': 1.24}\n",
            " 41% 50500/122283 [5:45:29<7:52:32,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 21:52:20,339 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-50500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:52:20,345 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-50500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:52:22,329 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-50500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:52:22,335 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-50500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:52:22,340 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-50500/special_tokens_map.json\n",
            "{'loss': 2.6423, 'learning_rate': 2.9146733397119797e-05, 'epoch': 1.25}\n",
            " 42% 51000/122283 [5:48:53<7:47:30,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 21:55:44,601 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-51000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:55:44,607 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-51000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:55:46,519 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-51000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:55:46,525 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-51000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:55:46,531 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-51000/special_tokens_map.json\n",
            "{'loss': 2.6941, 'learning_rate': 2.8942289606895484e-05, 'epoch': 1.26}\n",
            " 42% 51500/122283 [5:52:17<7:46:52,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 21:59:08,200 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-51500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 21:59:08,207 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-51500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 21:59:10,854 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-51500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 21:59:10,861 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-51500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 21:59:10,866 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-51500/special_tokens_map.json\n",
            "{'loss': 2.6988, 'learning_rate': 2.8737845816671167e-05, 'epoch': 1.28}\n",
            " 43% 52000/122283 [5:55:41<7:41:32,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 22:02:32,018 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-52000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:02:32,026 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-52000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:02:33,895 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-52000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:02:33,900 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-52000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:02:33,904 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-52000/special_tokens_map.json\n",
            "{'loss': 2.6599, 'learning_rate': 2.8533402026446848e-05, 'epoch': 1.29}\n",
            " 43% 52500/122283 [5:59:05<7:37:59,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 22:05:55,923 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-52500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:05:55,929 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-52500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:05:57,926 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-52500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:05:57,932 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-52500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:05:57,936 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-52500/special_tokens_map.json\n",
            "{'loss': 2.6324, 'learning_rate': 2.8328958236222535e-05, 'epoch': 1.3}\n",
            " 43% 53000/122283 [6:02:30<7:36:22,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 22:09:20,960 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-53000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:09:20,967 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-53000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:09:22,912 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-53000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:09:22,917 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-53000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:09:22,921 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-53000/special_tokens_map.json\n",
            "{'loss': 2.6791, 'learning_rate': 2.812451444599822e-05, 'epoch': 1.31}\n",
            " 44% 53500/122283 [6:05:54<7:30:25,  2.55it/s][INFO|trainer.py:2506] 2022-07-01 22:12:45,059 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-53500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:12:45,065 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-53500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:12:47,257 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-53500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:12:47,265 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-53500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:12:48,009 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-53500/special_tokens_map.json\n",
            "{'loss': 2.7561, 'learning_rate': 2.7920070655773906e-05, 'epoch': 1.32}\n",
            " 44% 54000/122283 [6:09:18<7:29:03,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 22:16:09,654 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-54000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:16:09,660 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-54000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:16:11,479 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-54000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:16:11,485 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-54000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:16:12,444 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-54000/special_tokens_map.json\n",
            "{'loss': 2.6486, 'learning_rate': 2.7715626865549586e-05, 'epoch': 1.34}\n",
            " 45% 54500/122283 [6:12:43<7:23:56,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 22:19:34,330 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-54500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:19:34,337 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-54500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:19:37,221 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-54500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:19:37,242 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-54500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:19:37,246 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-54500/special_tokens_map.json\n",
            "{'loss': 2.6847, 'learning_rate': 2.751118307532527e-05, 'epoch': 1.35}\n",
            " 45% 55000/122283 [6:16:09<7:30:33,  2.49it/s][INFO|trainer.py:2506] 2022-07-01 22:23:00,112 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-55000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:23:00,124 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-55000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:23:02,279 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-55000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:23:03,054 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-55000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:23:03,059 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-55000/special_tokens_map.json\n",
            "{'loss': 2.6603, 'learning_rate': 2.7306739285100957e-05, 'epoch': 1.36}\n",
            " 45% 55500/122283 [6:19:33<7:19:16,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 22:26:24,657 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-55500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:26:24,662 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-55500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:26:26,491 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-55500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:26:26,497 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-55500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:26:26,501 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-55500/special_tokens_map.json\n",
            "{'loss': 2.6897, 'learning_rate': 2.710229549487664e-05, 'epoch': 1.37}\n",
            " 46% 56000/122283 [6:22:58<7:20:20,  2.51it/s][INFO|trainer.py:2506] 2022-07-01 22:29:49,118 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-56000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:29:49,124 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-56000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:29:51,066 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-56000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:29:51,873 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-56000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:29:51,877 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-56000/special_tokens_map.json\n",
            "{'loss': 2.6704, 'learning_rate': 2.689785170465232e-05, 'epoch': 1.39}\n",
            " 46% 56500/122283 [6:26:22<7:20:29,  2.49it/s][INFO|trainer.py:2506] 2022-07-01 22:33:13,223 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-56500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:33:13,230 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-56500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:33:15,444 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-56500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:33:16,101 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-56500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:33:16,107 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-56500/special_tokens_map.json\n",
            "{'loss': 2.7124, 'learning_rate': 2.6693407914428008e-05, 'epoch': 1.4}\n",
            " 47% 57000/122283 [6:29:47<7:09:11,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 22:36:38,175 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-57000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:36:38,180 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-57000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:36:40,922 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-57000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:36:40,929 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-57000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:36:40,935 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-57000/special_tokens_map.json\n",
            "{'loss': 2.6518, 'learning_rate': 2.6488964124203692e-05, 'epoch': 1.41}\n",
            " 47% 57500/122283 [6:33:13<7:09:26,  2.51it/s][INFO|trainer.py:2506] 2022-07-01 22:40:03,771 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-57500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:40:03,778 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-57500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:40:05,759 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-57500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:40:05,765 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-57500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:40:05,770 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-57500/special_tokens_map.json\n",
            "{'loss': 2.6727, 'learning_rate': 2.628452033397938e-05, 'epoch': 1.42}\n",
            " 47% 58000/122283 [6:36:38<7:14:16,  2.47it/s][INFO|trainer.py:2506] 2022-07-01 22:43:29,241 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-58000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:43:29,247 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-58000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:43:31,276 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-58000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:43:31,284 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-58000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:43:31,288 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-58000/special_tokens_map.json\n",
            "{'loss': 2.6313, 'learning_rate': 2.608007654375506e-05, 'epoch': 1.44}\n",
            " 48% 58500/122283 [6:40:02<6:58:07,  2.54it/s][INFO|trainer.py:2506] 2022-07-01 22:46:53,695 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-58500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:46:53,709 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-58500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:46:55,598 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-58500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:46:55,603 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-58500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:46:55,608 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-58500/special_tokens_map.json\n",
            "{'loss': 2.6504, 'learning_rate': 2.5875632753530743e-05, 'epoch': 1.45}\n",
            " 48% 59000/122283 [6:43:28<6:58:24,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 22:50:19,285 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-59000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:50:19,291 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-59000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:50:21,234 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-59000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:50:21,240 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-59000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:50:21,245 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-59000/special_tokens_map.json\n",
            "{'loss': 2.7183, 'learning_rate': 2.567118896330643e-05, 'epoch': 1.46}\n",
            " 49% 59500/122283 [6:46:53<6:54:07,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 22:53:44,211 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-59500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:53:44,217 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-59500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:53:46,839 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-59500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:53:46,846 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-59500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:53:46,852 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-59500/special_tokens_map.json\n",
            "{'loss': 2.6543, 'learning_rate': 2.5466745173082114e-05, 'epoch': 1.47}\n",
            " 49% 60000/122283 [6:50:18<6:49:30,  2.53it/s][INFO|trainer.py:2506] 2022-07-01 22:57:09,595 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-60000\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 22:57:09,601 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-60000/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 22:57:11,496 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-60000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 22:57:11,501 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-60000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 22:57:11,506 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-60000/special_tokens_map.json\n",
            "{'loss': 2.6635, 'learning_rate': 2.52623013828578e-05, 'epoch': 1.48}\n",
            " 49% 60500/122283 [6:53:43<6:47:52,  2.52it/s][INFO|trainer.py:2506] 2022-07-01 23:00:34,339 >> Saving model checkpoint to gpt-2/tmp/alignment-texts-clm/checkpoint-60500\n",
            "[INFO|configuration_utils.py:446] 2022-07-01 23:00:34,344 >> Configuration saved in gpt-2/tmp/alignment-texts-clm/checkpoint-60500/config.json\n",
            "[INFO|modeling_utils.py:1550] 2022-07-01 23:00:36,532 >> Model weights saved in gpt-2/tmp/alignment-texts-clm/checkpoint-60500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2143] 2022-07-01 23:00:36,537 >> tokenizer config file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-60500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2150] 2022-07-01 23:00:36,542 >> Special tokens file saved in gpt-2/tmp/alignment-texts-clm/checkpoint-60500/special_tokens_map.json\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 380, in save\n",
            "    _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 604, in _save\n",
            "    zip_file.write_record(name, storage.data_ptr(), num_bytes)\n",
            "OSError: [Errno 28] No space left on device\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"gpt-2/run_clm.py\", line 579, in <module>\n",
            "    main()\n",
            "  File \"gpt-2/run_clm.py\", line 527, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1417, in train\n",
            "    ignore_keys_for_eval=ignore_keys_for_eval,\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1732, in _inner_training_loop\n",
            "    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 1920, in _maybe_log_save_evaluate\n",
            "    self._save_checkpoint(model, trial, metrics=metrics)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\", line 2026, in _save_checkpoint\n",
            "    torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 381, in save\n",
            "    return\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 260, in __exit__\n",
            "    self.file_like.write_end_of_file()\n",
            "RuntimeError: [enforce fail at inline_container.cc:300] . unexpected pos 920086784 vs 920086672\n",
            "terminate called after throwing an instance of 'c10::Error'\n",
            "  what():  [enforce fail at inline_container.cc:300] . unexpected pos 920086784 vs 920086672\n",
            "frame #0: c10::ThrowEnforceNotMet(char const*, int, char const*, std::string const&, void const*) + 0x47 (0x7f8836f8d557 in /usr/local/lib/python3.7/dist-packages/torch/lib/libc10.so)\n",
            "frame #1: <unknown function> + 0x27bff40 (0x7f88725f7f40 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #2: mz_zip_writer_add_mem_ex_v2 + 0x6bf (0x7f88725f333f in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #3: caffe2::serialize::PyTorchStreamWriter::writeRecord(std::string const&, void const*, unsigned long, bool) + 0xb5 (0x7f88725fb185 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #4: caffe2::serialize::PyTorchStreamWriter::writeEndOfFile() + 0x2c3 (0x7f88725fb693 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #5: caffe2::serialize::PyTorchStreamWriter::~PyTorchStreamWriter() + 0x125 (0x7f88725fb945 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_cpu.so)\n",
            "frame #6: <unknown function> + 0x534053 (0x7f88ec5d9053 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #7: <unknown function> + 0x1f57a0 (0x7f88ec29a7a0 in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #8: <unknown function> + 0x1f69ee (0x7f88ec29b9ee in /usr/local/lib/python3.7/dist-packages/torch/lib/libtorch_python.so)\n",
            "frame #9: python3() [0x5a9afc]\n",
            "frame #10: python3() [0x4fa7d8]\n",
            "frame #11: python3() [0x4fa7ec]\n",
            "frame #12: python3() [0x4fa7ec]\n",
            "frame #13: python3() [0x4fa7ec]\n",
            "frame #14: python3() [0x4fa7ec]\n",
            "frame #15: python3() [0x4fa7ec]\n",
            "frame #16: python3() [0x4fa7ec]\n",
            "frame #17: python3() [0x561a71]\n",
            "<omitting python frames>\n",
            "frame #21: python3() [0x64f939]\n",
            "frame #23: __libc_start_main + 0xe7 (0x7f8908dc5c87 in /lib/x86_64-linux-gnu/libc.so.6)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7q0Q066UWo-"
      },
      "source": [
        "# Let's use the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdIAGG1a27xX",
        "outputId": "f73c6a29-f0c9-4519-ccc5-f6057408b3dd"
      },
      "source": [
        "OUTPUT_DIR = \"gpt-2/tmp/elon-test-clm\"\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(OUTPUT_DIR)\n",
        "model = GPT2LMHeadModel.from_pretrained(OUTPUT_DIR)\n",
        "model = model.to(device)\n",
        "                                        \n",
        "def generate(input_str, length=250, n=5):\n",
        "  cur_ids = torch.tensor(tokenizer.encode(input_str)).unsqueeze(0).long().to(device)\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for i in range(length):\n",
        "      outputs = model(cur_ids[:, -1024:], labels=cur_ids[:, -1024:])\n",
        "      loss, logits = outputs[:2]\n",
        "      softmax_logits = torch.softmax(logits[0,-1], dim=0)\n",
        "      next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n)\n",
        "      cur_ids = torch.cat([cur_ids, torch.ones((1,1)).long().to(device) * next_token_id], dim=1)\n",
        "    output_list = list(cur_ids.squeeze().to('cpu').numpy())\n",
        "    output_text = tokenizer.decode(output_list)\n",
        "    return output_text\n",
        "\n",
        "def choose_from_top(probs, n=5):\n",
        "    ind = np.argpartition(probs, -n)[-n:]\n",
        "    top_prob = probs[ind]\n",
        "    top_prob = top_prob / np.sum(top_prob) # Normalize\n",
        "    choice = np.random.choice(n, 1, p = top_prob)\n",
        "    token_id = ind[choice][0]\n",
        "    return int(token_id)\n",
        "\n",
        "generated_text = generate(\"Just dropping some\")\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Just dropping some of my old stuff in the trunk. It’s been a while since I last used a car.      _AA_Carmack   Yeah, I think we should do something about it.  We have a long way to go. Will be interesting to see what happens to those who don’t support this.   I think it will be great. It will be a lot more than a mere cameo. _AA_Carmack  Yes, it’s a great game. I think we should do something about it.   _Station _Ryan _AA_Carmack       _Station _AA_Carmack  I’m not a big fan of the Tesla Autopilot software, but I do like the idea of having a car capable of recognizing pedestrians and cyclists. It’s a great idea, especially with the high speed autotracing. _Station I love you         _Padival         _Station  Yes, it will have a lot of new features coming to the Tesla Model S, including the ability to drive from the garage to the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF05N8SmVIIr",
        "outputId": "ae797da8-79c9-43ef-aa77-feca1aec9452"
      },
      "source": [
        "generated_text = generate(\"Just dropping some\")\n",
        "print(generated_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Just dropping some of our own resources into the ocean                   Yeah, we have to make sure we have a good product. That will be a priority.  We will make a new version of Falcon Heavy for free.  _AA_Carmack      _Sword _Sword _Sword    _AA_Carmack _Sword          We’ll try to get that done, but I think we’ll be better off with a more advanced, reusable, reusable rocket booster.        Yeah, I love it :)       I love the idea of having a Tesla in the car. It’s awesome.  _Ryan    Yeah, that's what we should do   Yeah, we will make the Model S a lot faster, but we have a lot of work to do to get it right, as we did in the beginning. We’ve had a lot of setbacks._Gardi  _Ryan Yeah, that's exactly right _AA_Carmack I’m just trying to be as polite\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kzOlqi8eMys"
      },
      "source": [
        "# Compressing the Model\n",
        "\n",
        "Let's save the model as a `tar.gz` file so that we can save it in Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOmuQ4tUVei9"
      },
      "source": [
        "!tar -czf gpt-2-elon-tweets.tar.gz gpt-2/tuned-models/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minGPT-from-scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOv3LjxaWJYjNB7uGqsRf0E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayThibs/gpt-experiments/blob/main/notebooks/minGPT_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End-to-End minGPT\n",
        "\n",
        "This is an implementation of minGPT in order to quickly get a feel for the end-to-end training of a GPT model."
      ],
      "metadata": {
        "id": "UkI5QrUhQCGP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "wnqQm_HHQWB3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZpKxu0fyPZBY"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model configuration\n",
        "\n",
        "We are now going to create a class where we can initialize all the parameters of the model. This is where we include all the hyperparameters for the model. Since we are doing an implementation of minGPT, we don't have the same model as GPT-2 or GPT-3. However, to get those models, we can simple add more layers, increase maximum sequence length, and embedding dimension.\n",
        "\n",
        "Those bigger models have some additional tricks for training, but the general idea is just a bigger model and more data."
      ],
      "metadata": {
        "id": "bg-M2ufVQX8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTConfig:\n",
        "    attn_dropout = 0.1\n",
        "    embed_dropout = 0.1\n",
        "    ff_dropout = 0.1\n",
        "\n",
        "    def __init__(self, vocab_size, max_len, **kwargs):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "class GPT1Config(GPTConfig):\n",
        "    num_heads = 12\n",
        "    num_blocks = 12\n",
        "    embed_dim = 768"
      ],
      "metadata": {
        "id": "J30QUiJVRd9V"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        embed_dim = config.embed_dim\n",
        "        self.max_len = config.max_len\n",
        "        self.tok_embed = nn.Embedding(\n",
        "            config.vocab_size, embed_dim\n",
        "        )\n",
        "        self.pos_embed = nn.Parameter(\n",
        "            torch.zeros(1, self.max_len, embed_dim)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(config.embed_dropout)\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block(config) for _ in range(config.num_blocks)]\n",
        "        )\n",
        "        self.ln = nn.LayerNorm(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, config.vocab_size)\n",
        "\n",
        "    def forward(self, x, target=None):\n",
        "        # batch_size = x.size(0) # (batch_size, sequence_length, embedding_dimension)\n",
        "        seq_len = x.size(1)\n",
        "        assert seq_len <= self.max_len, \"sequence longer than model capacity\"\n",
        "\n",
        "        tok_embedding = self.tok_embed(x)\n",
        "        # tok_embedding.shape == (batch_size, seq_len, embed_dim)\n",
        "        pos_embedding = self.pos_embed[:, :seq_len, :] # cuts pos_embed shorter based on seq_len passed\n",
        "        # pos_embedding.shape == (1, seq_len, embed_dim)\n",
        "        x = self.dropout(tok_embedding + pos_embedding)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln(x)\n",
        "        x = self.fc(x) # logits\n",
        "        # x.shape == (batch_size, seq_len, vocab_size)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RmwzhOmJn7wO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "eV62TQ5H8eH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "        "
      ],
      "metadata": {
        "id": "M0SoClqD8_AE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}